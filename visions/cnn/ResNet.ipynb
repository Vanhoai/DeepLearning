{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8707e1bf",
   "metadata": {},
   "source": [
    "##### Import require dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "224cc729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cd2e4a",
   "metadata": {},
   "source": [
    "##### Load CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "758468c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def data_loader(root, batch_size, random_seed=5, valid_size=0.1, shuffle=True, test=False):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2023, 0.1994, 0.2010],\n",
    "    )\n",
    "\n",
    "    # transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "    if test:\n",
    "        dataset = datasets.CIFAR10(\n",
    "            root=root,\n",
    "            train=False,\n",
    "            download=True,\n",
    "            transform=transform,\n",
    "        )\n",
    "\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=shuffle\n",
    "        )\n",
    "\n",
    "        return data_loader\n",
    "    \n",
    "    # load the dataset\n",
    "    train_dataset = datasets.CIFAR10(\n",
    "        root=root, train=True,\n",
    "        download=True,\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "    valid_dataset = datasets.CIFAR10(\n",
    "        root=root,\n",
    "        train=True,\n",
    "        download=True, \n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "    num_train = len(train_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "\n",
    "    return (train_loader, valid_loader)\n",
    "\n",
    "train_loader, valid_loader = data_loader(\n",
    "    root='./datasets',\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "test_loader = data_loader(\n",
    "    root='./datasets',\n",
    "    batch_size=64,\n",
    "    test=True\n",
    ")\n",
    "\n",
    "train_loader.dataset.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ae9de5",
   "metadata": {},
   "source": [
    "##### Build ResidualBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48a98676",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride = 1, down_sample = None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = stride, padding = 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "        self.down_sample = down_sample\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.down_sample:\n",
    "            residual = self.down_sample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c4c7dc",
   "metadata": {},
   "source": [
    "##### Build Model ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdcf6b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes = 10):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        self.in_planes = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size = 7, stride = 2, padding = 3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], stride = 1)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride = 2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride = 2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride = 2)\n",
    "        self.avg_pool = nn.AvgPool2d(7, stride=1)\n",
    "\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        down_sample = None\n",
    "        if stride != 1 or self.in_planes != planes:\n",
    "            down_sample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_planes, planes, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(planes),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_planes, planes, stride, down_sample))\n",
    "        self.in_planes = planes\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.in_planes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cd27b2",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1337f352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [1/704], Loss: 2.4780\n",
      "Epoch [1/20], Step [2/704], Loss: 2.3366\n",
      "Epoch [1/20], Step [3/704], Loss: 2.5150\n",
      "Epoch [1/20], Step [4/704], Loss: 2.4239\n",
      "Epoch [1/20], Step [5/704], Loss: 2.5685\n",
      "Epoch [1/20], Step [6/704], Loss: 2.4030\n",
      "Epoch [1/20], Step [7/704], Loss: 2.2277\n",
      "Epoch [1/20], Step [8/704], Loss: 2.4448\n",
      "Epoch [1/20], Step [9/704], Loss: 2.4341\n",
      "Epoch [1/20], Step [10/704], Loss: 2.0827\n",
      "Epoch [1/20], Step [11/704], Loss: 2.3374\n",
      "Epoch [1/20], Step [12/704], Loss: 2.4648\n",
      "Epoch [1/20], Step [13/704], Loss: 2.2556\n",
      "Epoch [1/20], Step [14/704], Loss: 2.3061\n",
      "Epoch [1/20], Step [15/704], Loss: 2.6321\n",
      "Epoch [1/20], Step [16/704], Loss: 2.4192\n",
      "Epoch [1/20], Step [17/704], Loss: 2.3231\n",
      "Epoch [1/20], Step [18/704], Loss: 2.2205\n",
      "Epoch [1/20], Step [19/704], Loss: 2.3871\n",
      "Epoch [1/20], Step [20/704], Loss: 2.4605\n",
      "Epoch [1/20], Step [21/704], Loss: 2.5355\n",
      "Epoch [1/20], Step [22/704], Loss: 2.4179\n",
      "Epoch [1/20], Step [23/704], Loss: 2.3324\n",
      "Epoch [1/20], Step [24/704], Loss: 2.0531\n",
      "Epoch [1/20], Step [25/704], Loss: 2.1898\n",
      "Epoch [1/20], Step [26/704], Loss: 2.2291\n",
      "Epoch [1/20], Step [27/704], Loss: 2.0614\n",
      "Epoch [1/20], Step [28/704], Loss: 2.0849\n",
      "Epoch [1/20], Step [29/704], Loss: 1.8328\n",
      "Epoch [1/20], Step [30/704], Loss: 1.9131\n",
      "Epoch [1/20], Step [31/704], Loss: 2.1723\n",
      "Epoch [1/20], Step [32/704], Loss: 1.9747\n",
      "Epoch [1/20], Step [33/704], Loss: 2.1896\n",
      "Epoch [1/20], Step [34/704], Loss: 1.8907\n",
      "Epoch [1/20], Step [35/704], Loss: 1.7851\n",
      "Epoch [1/20], Step [36/704], Loss: 2.0469\n",
      "Epoch [1/20], Step [37/704], Loss: 2.0492\n",
      "Epoch [1/20], Step [38/704], Loss: 1.8875\n",
      "Epoch [1/20], Step [39/704], Loss: 1.9639\n",
      "Epoch [1/20], Step [40/704], Loss: 1.8944\n",
      "Epoch [1/20], Step [41/704], Loss: 2.1685\n",
      "Epoch [1/20], Step [42/704], Loss: 1.9743\n",
      "Epoch [1/20], Step [43/704], Loss: 2.0889\n",
      "Epoch [1/20], Step [44/704], Loss: 1.7499\n",
      "Epoch [1/20], Step [45/704], Loss: 2.1453\n",
      "Epoch [1/20], Step [46/704], Loss: 1.9077\n",
      "Epoch [1/20], Step [47/704], Loss: 2.0167\n",
      "Epoch [1/20], Step [48/704], Loss: 1.9144\n",
      "Epoch [1/20], Step [49/704], Loss: 2.0862\n",
      "Epoch [1/20], Step [50/704], Loss: 1.9324\n",
      "Epoch [1/20], Step [51/704], Loss: 2.0529\n",
      "Epoch [1/20], Step [52/704], Loss: 2.0817\n",
      "Epoch [1/20], Step [53/704], Loss: 1.6132\n",
      "Epoch [1/20], Step [54/704], Loss: 2.0707\n",
      "Epoch [1/20], Step [55/704], Loss: 2.3111\n",
      "Epoch [1/20], Step [56/704], Loss: 2.0548\n",
      "Epoch [1/20], Step [57/704], Loss: 1.7381\n",
      "Epoch [1/20], Step [58/704], Loss: 1.8350\n",
      "Epoch [1/20], Step [59/704], Loss: 2.0250\n",
      "Epoch [1/20], Step [60/704], Loss: 1.9099\n",
      "Epoch [1/20], Step [61/704], Loss: 1.9767\n",
      "Epoch [1/20], Step [62/704], Loss: 1.7956\n",
      "Epoch [1/20], Step [63/704], Loss: 2.0361\n",
      "Epoch [1/20], Step [64/704], Loss: 1.9494\n",
      "Epoch [1/20], Step [65/704], Loss: 1.9662\n",
      "Epoch [1/20], Step [66/704], Loss: 2.3689\n",
      "Epoch [1/20], Step [67/704], Loss: 1.8840\n",
      "Epoch [1/20], Step [68/704], Loss: 2.2250\n",
      "Epoch [1/20], Step [69/704], Loss: 1.9973\n",
      "Epoch [1/20], Step [70/704], Loss: 1.9992\n",
      "Epoch [1/20], Step [71/704], Loss: 2.0547\n",
      "Epoch [1/20], Step [72/704], Loss: 1.9663\n",
      "Epoch [1/20], Step [73/704], Loss: 1.8356\n",
      "Epoch [1/20], Step [74/704], Loss: 1.9570\n",
      "Epoch [1/20], Step [75/704], Loss: 2.0220\n",
      "Epoch [1/20], Step [76/704], Loss: 1.8696\n",
      "Epoch [1/20], Step [77/704], Loss: 1.7575\n",
      "Epoch [1/20], Step [78/704], Loss: 1.9770\n",
      "Epoch [1/20], Step [79/704], Loss: 1.9546\n",
      "Epoch [1/20], Step [80/704], Loss: 1.9318\n",
      "Epoch [1/20], Step [81/704], Loss: 2.0082\n",
      "Epoch [1/20], Step [82/704], Loss: 1.8053\n",
      "Epoch [1/20], Step [83/704], Loss: 1.8955\n",
      "Epoch [1/20], Step [84/704], Loss: 1.9470\n",
      "Epoch [1/20], Step [85/704], Loss: 1.8694\n",
      "Epoch [1/20], Step [86/704], Loss: 1.9775\n",
      "Epoch [1/20], Step [87/704], Loss: 1.8838\n",
      "Epoch [1/20], Step [88/704], Loss: 2.0288\n",
      "Epoch [1/20], Step [89/704], Loss: 1.7589\n",
      "Epoch [1/20], Step [90/704], Loss: 1.9060\n",
      "Epoch [1/20], Step [91/704], Loss: 1.8473\n",
      "Epoch [1/20], Step [92/704], Loss: 1.5828\n",
      "Epoch [1/20], Step [93/704], Loss: 1.9412\n",
      "Epoch [1/20], Step [94/704], Loss: 1.7708\n",
      "Epoch [1/20], Step [95/704], Loss: 1.7269\n",
      "Epoch [1/20], Step [96/704], Loss: 2.1136\n",
      "Epoch [1/20], Step [97/704], Loss: 1.8602\n",
      "Epoch [1/20], Step [98/704], Loss: 1.8487\n",
      "Epoch [1/20], Step [99/704], Loss: 1.9533\n",
      "Epoch [1/20], Step [100/704], Loss: 1.9401\n",
      "Epoch [1/20], Step [101/704], Loss: 1.7162\n",
      "Epoch [1/20], Step [102/704], Loss: 1.8565\n",
      "Epoch [1/20], Step [103/704], Loss: 1.5813\n",
      "Epoch [1/20], Step [104/704], Loss: 1.8377\n",
      "Epoch [1/20], Step [105/704], Loss: 1.9010\n",
      "Epoch [1/20], Step [106/704], Loss: 1.6611\n",
      "Epoch [1/20], Step [107/704], Loss: 1.7448\n",
      "Epoch [1/20], Step [108/704], Loss: 1.9729\n",
      "Epoch [1/20], Step [109/704], Loss: 1.8712\n",
      "Epoch [1/20], Step [110/704], Loss: 1.9182\n",
      "Epoch [1/20], Step [111/704], Loss: 1.9341\n",
      "Epoch [1/20], Step [112/704], Loss: 1.9854\n",
      "Epoch [1/20], Step [113/704], Loss: 1.8303\n",
      "Epoch [1/20], Step [114/704], Loss: 1.7395\n",
      "Epoch [1/20], Step [115/704], Loss: 1.6900\n",
      "Epoch [1/20], Step [116/704], Loss: 1.8872\n",
      "Epoch [1/20], Step [117/704], Loss: 1.6182\n",
      "Epoch [1/20], Step [118/704], Loss: 1.6481\n",
      "Epoch [1/20], Step [119/704], Loss: 1.8732\n",
      "Epoch [1/20], Step [120/704], Loss: 1.8140\n",
      "Epoch [1/20], Step [121/704], Loss: 2.0462\n",
      "Epoch [1/20], Step [122/704], Loss: 1.6361\n",
      "Epoch [1/20], Step [123/704], Loss: 1.6932\n",
      "Epoch [1/20], Step [124/704], Loss: 1.6953\n",
      "Epoch [1/20], Step [125/704], Loss: 1.9969\n",
      "Epoch [1/20], Step [126/704], Loss: 1.7958\n",
      "Epoch [1/20], Step [127/704], Loss: 2.0418\n",
      "Epoch [1/20], Step [128/704], Loss: 1.9448\n",
      "Epoch [1/20], Step [129/704], Loss: 1.5239\n",
      "Epoch [1/20], Step [130/704], Loss: 1.6189\n",
      "Epoch [1/20], Step [131/704], Loss: 1.8316\n",
      "Epoch [1/20], Step [132/704], Loss: 2.0358\n",
      "Epoch [1/20], Step [133/704], Loss: 1.9004\n",
      "Epoch [1/20], Step [134/704], Loss: 1.7762\n",
      "Epoch [1/20], Step [135/704], Loss: 1.7553\n",
      "Epoch [1/20], Step [136/704], Loss: 1.6632\n",
      "Epoch [1/20], Step [137/704], Loss: 1.5943\n",
      "Epoch [1/20], Step [138/704], Loss: 1.7951\n",
      "Epoch [1/20], Step [139/704], Loss: 1.7720\n",
      "Epoch [1/20], Step [140/704], Loss: 1.8835\n",
      "Epoch [1/20], Step [141/704], Loss: 1.6894\n",
      "Epoch [1/20], Step [142/704], Loss: 1.7043\n",
      "Epoch [1/20], Step [143/704], Loss: 1.6921\n",
      "Epoch [1/20], Step [144/704], Loss: 1.6469\n",
      "Epoch [1/20], Step [145/704], Loss: 1.5455\n",
      "Epoch [1/20], Step [146/704], Loss: 1.6240\n",
      "Epoch [1/20], Step [147/704], Loss: 1.5562\n",
      "Epoch [1/20], Step [148/704], Loss: 1.9001\n",
      "Epoch [1/20], Step [149/704], Loss: 1.7663\n",
      "Epoch [1/20], Step [150/704], Loss: 1.7572\n",
      "Epoch [1/20], Step [151/704], Loss: 1.7674\n",
      "Epoch [1/20], Step [152/704], Loss: 1.6739\n",
      "Epoch [1/20], Step [153/704], Loss: 1.7851\n",
      "Epoch [1/20], Step [154/704], Loss: 1.8790\n",
      "Epoch [1/20], Step [155/704], Loss: 1.6871\n",
      "Epoch [1/20], Step [156/704], Loss: 2.0445\n",
      "Epoch [1/20], Step [157/704], Loss: 1.9416\n",
      "Epoch [1/20], Step [158/704], Loss: 1.6665\n",
      "Epoch [1/20], Step [159/704], Loss: 1.6020\n",
      "Epoch [1/20], Step [160/704], Loss: 1.8133\n",
      "Epoch [1/20], Step [161/704], Loss: 1.4651\n",
      "Epoch [1/20], Step [162/704], Loss: 1.5529\n",
      "Epoch [1/20], Step [163/704], Loss: 1.7672\n",
      "Epoch [1/20], Step [164/704], Loss: 1.6039\n",
      "Epoch [1/20], Step [165/704], Loss: 1.6310\n",
      "Epoch [1/20], Step [166/704], Loss: 1.7429\n",
      "Epoch [1/20], Step [167/704], Loss: 1.6608\n",
      "Epoch [1/20], Step [168/704], Loss: 1.5547\n",
      "Epoch [1/20], Step [169/704], Loss: 1.8751\n",
      "Epoch [1/20], Step [170/704], Loss: 1.7003\n",
      "Epoch [1/20], Step [171/704], Loss: 1.6768\n",
      "Epoch [1/20], Step [172/704], Loss: 1.8151\n",
      "Epoch [1/20], Step [173/704], Loss: 1.4732\n",
      "Epoch [1/20], Step [174/704], Loss: 1.8436\n",
      "Epoch [1/20], Step [175/704], Loss: 1.9612\n",
      "Epoch [1/20], Step [176/704], Loss: 1.8751\n",
      "Epoch [1/20], Step [177/704], Loss: 1.7578\n",
      "Epoch [1/20], Step [178/704], Loss: 1.6929\n",
      "Epoch [1/20], Step [179/704], Loss: 1.7112\n",
      "Epoch [1/20], Step [180/704], Loss: 1.5879\n",
      "Epoch [1/20], Step [181/704], Loss: 1.7488\n",
      "Epoch [1/20], Step [182/704], Loss: 1.9749\n",
      "Epoch [1/20], Step [183/704], Loss: 1.7007\n",
      "Epoch [1/20], Step [184/704], Loss: 1.9590\n",
      "Epoch [1/20], Step [185/704], Loss: 1.4709\n",
      "Epoch [1/20], Step [186/704], Loss: 1.6579\n",
      "Epoch [1/20], Step [187/704], Loss: 1.5542\n",
      "Epoch [1/20], Step [188/704], Loss: 1.4428\n",
      "Epoch [1/20], Step [189/704], Loss: 1.5914\n",
      "Epoch [1/20], Step [190/704], Loss: 1.5971\n",
      "Epoch [1/20], Step [191/704], Loss: 1.7984\n",
      "Epoch [1/20], Step [192/704], Loss: 1.5892\n",
      "Epoch [1/20], Step [193/704], Loss: 1.7319\n",
      "Epoch [1/20], Step [194/704], Loss: 1.5706\n",
      "Epoch [1/20], Step [195/704], Loss: 1.5119\n",
      "Epoch [1/20], Step [196/704], Loss: 1.6980\n",
      "Epoch [1/20], Step [197/704], Loss: 1.5404\n",
      "Epoch [1/20], Step [198/704], Loss: 1.6682\n",
      "Epoch [1/20], Step [199/704], Loss: 1.5829\n",
      "Epoch [1/20], Step [200/704], Loss: 1.6659\n",
      "Epoch [1/20], Step [201/704], Loss: 1.3679\n",
      "Epoch [1/20], Step [202/704], Loss: 1.6610\n",
      "Epoch [1/20], Step [203/704], Loss: 1.8740\n",
      "Epoch [1/20], Step [204/704], Loss: 1.8411\n",
      "Epoch [1/20], Step [205/704], Loss: 1.4137\n",
      "Epoch [1/20], Step [206/704], Loss: 1.9727\n",
      "Epoch [1/20], Step [207/704], Loss: 1.7860\n",
      "Epoch [1/20], Step [208/704], Loss: 2.1793\n",
      "Epoch [1/20], Step [209/704], Loss: 1.8341\n",
      "Epoch [1/20], Step [210/704], Loss: 1.7760\n",
      "Epoch [1/20], Step [211/704], Loss: 1.5712\n",
      "Epoch [1/20], Step [212/704], Loss: 1.5470\n",
      "Epoch [1/20], Step [213/704], Loss: 1.7738\n",
      "Epoch [1/20], Step [214/704], Loss: 1.7254\n",
      "Epoch [1/20], Step [215/704], Loss: 1.8254\n",
      "Epoch [1/20], Step [216/704], Loss: 1.5935\n",
      "Epoch [1/20], Step [217/704], Loss: 1.5854\n",
      "Epoch [1/20], Step [218/704], Loss: 1.6392\n",
      "Epoch [1/20], Step [219/704], Loss: 1.6998\n",
      "Epoch [1/20], Step [220/704], Loss: 1.6464\n",
      "Epoch [1/20], Step [221/704], Loss: 1.8216\n",
      "Epoch [1/20], Step [222/704], Loss: 1.7144\n",
      "Epoch [1/20], Step [223/704], Loss: 1.5851\n",
      "Epoch [1/20], Step [224/704], Loss: 1.4996\n",
      "Epoch [1/20], Step [225/704], Loss: 1.7353\n",
      "Epoch [1/20], Step [226/704], Loss: 1.6533\n",
      "Epoch [1/20], Step [227/704], Loss: 1.7919\n",
      "Epoch [1/20], Step [228/704], Loss: 1.6984\n",
      "Epoch [1/20], Step [229/704], Loss: 1.6588\n",
      "Epoch [1/20], Step [230/704], Loss: 1.4661\n",
      "Epoch [1/20], Step [231/704], Loss: 1.5719\n",
      "Epoch [1/20], Step [232/704], Loss: 1.6986\n",
      "Epoch [1/20], Step [233/704], Loss: 1.6975\n",
      "Epoch [1/20], Step [234/704], Loss: 1.9077\n",
      "Epoch [1/20], Step [235/704], Loss: 1.8348\n",
      "Epoch [1/20], Step [236/704], Loss: 1.7638\n",
      "Epoch [1/20], Step [237/704], Loss: 1.3286\n",
      "Epoch [1/20], Step [238/704], Loss: 1.6623\n",
      "Epoch [1/20], Step [239/704], Loss: 1.5549\n",
      "Epoch [1/20], Step [240/704], Loss: 1.3966\n",
      "Epoch [1/20], Step [241/704], Loss: 1.5478\n",
      "Epoch [1/20], Step [242/704], Loss: 1.5208\n",
      "Epoch [1/20], Step [243/704], Loss: 1.6082\n",
      "Epoch [1/20], Step [244/704], Loss: 1.6815\n",
      "Epoch [1/20], Step [245/704], Loss: 1.5971\n",
      "Epoch [1/20], Step [246/704], Loss: 1.5238\n",
      "Epoch [1/20], Step [247/704], Loss: 1.5834\n",
      "Epoch [1/20], Step [248/704], Loss: 1.5010\n",
      "Epoch [1/20], Step [249/704], Loss: 1.8344\n",
      "Epoch [1/20], Step [250/704], Loss: 1.7334\n",
      "Epoch [1/20], Step [251/704], Loss: 1.5210\n",
      "Epoch [1/20], Step [252/704], Loss: 1.5187\n",
      "Epoch [1/20], Step [253/704], Loss: 1.5037\n",
      "Epoch [1/20], Step [254/704], Loss: 1.4473\n",
      "Epoch [1/20], Step [255/704], Loss: 1.5567\n",
      "Epoch [1/20], Step [256/704], Loss: 1.3864\n",
      "Epoch [1/20], Step [257/704], Loss: 1.8732\n",
      "Epoch [1/20], Step [258/704], Loss: 1.7302\n",
      "Epoch [1/20], Step [259/704], Loss: 1.5726\n",
      "Epoch [1/20], Step [260/704], Loss: 1.5956\n",
      "Epoch [1/20], Step [261/704], Loss: 1.6327\n",
      "Epoch [1/20], Step [262/704], Loss: 1.7473\n",
      "Epoch [1/20], Step [263/704], Loss: 1.6077\n",
      "Epoch [1/20], Step [264/704], Loss: 1.9289\n",
      "Epoch [1/20], Step [265/704], Loss: 1.6157\n",
      "Epoch [1/20], Step [266/704], Loss: 1.5272\n",
      "Epoch [1/20], Step [267/704], Loss: 1.3517\n",
      "Epoch [1/20], Step [268/704], Loss: 1.5708\n",
      "Epoch [1/20], Step [269/704], Loss: 1.5287\n",
      "Epoch [1/20], Step [270/704], Loss: 1.8420\n",
      "Epoch [1/20], Step [271/704], Loss: 1.7366\n",
      "Epoch [1/20], Step [272/704], Loss: 1.5410\n",
      "Epoch [1/20], Step [273/704], Loss: 1.5122\n",
      "Epoch [1/20], Step [274/704], Loss: 1.4624\n",
      "Epoch [1/20], Step [275/704], Loss: 1.5978\n",
      "Epoch [1/20], Step [276/704], Loss: 1.5380\n",
      "Epoch [1/20], Step [277/704], Loss: 1.6174\n",
      "Epoch [1/20], Step [278/704], Loss: 1.3568\n",
      "Epoch [1/20], Step [279/704], Loss: 1.7755\n",
      "Epoch [1/20], Step [280/704], Loss: 1.8780\n",
      "Epoch [1/20], Step [281/704], Loss: 1.5276\n",
      "Epoch [1/20], Step [282/704], Loss: 1.7050\n",
      "Epoch [1/20], Step [283/704], Loss: 1.4033\n",
      "Epoch [1/20], Step [284/704], Loss: 1.4085\n",
      "Epoch [1/20], Step [285/704], Loss: 1.7736\n",
      "Epoch [1/20], Step [286/704], Loss: 1.6753\n",
      "Epoch [1/20], Step [287/704], Loss: 1.5709\n",
      "Epoch [1/20], Step [288/704], Loss: 1.5067\n",
      "Epoch [1/20], Step [289/704], Loss: 1.3366\n",
      "Epoch [1/20], Step [290/704], Loss: 1.6404\n",
      "Epoch [1/20], Step [291/704], Loss: 1.5233\n",
      "Epoch [1/20], Step [292/704], Loss: 1.5238\n",
      "Epoch [1/20], Step [293/704], Loss: 1.5616\n",
      "Epoch [1/20], Step [294/704], Loss: 1.4664\n",
      "Epoch [1/20], Step [295/704], Loss: 1.4815\n",
      "Epoch [1/20], Step [296/704], Loss: 1.5503\n",
      "Epoch [1/20], Step [297/704], Loss: 1.5116\n",
      "Epoch [1/20], Step [298/704], Loss: 1.3723\n",
      "Epoch [1/20], Step [299/704], Loss: 1.6966\n",
      "Epoch [1/20], Step [300/704], Loss: 1.4346\n",
      "Epoch [1/20], Step [301/704], Loss: 1.2715\n",
      "Epoch [1/20], Step [302/704], Loss: 1.9523\n",
      "Epoch [1/20], Step [303/704], Loss: 1.7367\n",
      "Epoch [1/20], Step [304/704], Loss: 1.2445\n",
      "Epoch [1/20], Step [305/704], Loss: 1.5326\n",
      "Epoch [1/20], Step [306/704], Loss: 1.4941\n",
      "Epoch [1/20], Step [307/704], Loss: 1.5314\n",
      "Epoch [1/20], Step [308/704], Loss: 1.2718\n",
      "Epoch [1/20], Step [309/704], Loss: 1.5585\n",
      "Epoch [1/20], Step [310/704], Loss: 1.6186\n",
      "Epoch [1/20], Step [311/704], Loss: 1.3879\n",
      "Epoch [1/20], Step [312/704], Loss: 1.2869\n",
      "Epoch [1/20], Step [313/704], Loss: 1.5197\n",
      "Epoch [1/20], Step [314/704], Loss: 1.7060\n",
      "Epoch [1/20], Step [315/704], Loss: 1.5899\n",
      "Epoch [1/20], Step [316/704], Loss: 1.5391\n",
      "Epoch [1/20], Step [317/704], Loss: 1.4964\n",
      "Epoch [1/20], Step [318/704], Loss: 1.6177\n",
      "Epoch [1/20], Step [319/704], Loss: 1.4600\n",
      "Epoch [1/20], Step [320/704], Loss: 1.4047\n",
      "Epoch [1/20], Step [321/704], Loss: 1.5294\n",
      "Epoch [1/20], Step [322/704], Loss: 1.5292\n",
      "Epoch [1/20], Step [323/704], Loss: 1.6052\n",
      "Epoch [1/20], Step [324/704], Loss: 1.3933\n",
      "Epoch [1/20], Step [325/704], Loss: 1.5905\n",
      "Epoch [1/20], Step [326/704], Loss: 1.4588\n",
      "Epoch [1/20], Step [327/704], Loss: 1.4240\n",
      "Epoch [1/20], Step [328/704], Loss: 1.6584\n",
      "Epoch [1/20], Step [329/704], Loss: 1.5951\n",
      "Epoch [1/20], Step [330/704], Loss: 1.6050\n",
      "Epoch [1/20], Step [331/704], Loss: 1.4480\n",
      "Epoch [1/20], Step [332/704], Loss: 1.4437\n",
      "Epoch [1/20], Step [333/704], Loss: 1.2640\n",
      "Epoch [1/20], Step [334/704], Loss: 1.5604\n",
      "Epoch [1/20], Step [335/704], Loss: 1.4060\n",
      "Epoch [1/20], Step [336/704], Loss: 1.6576\n",
      "Epoch [1/20], Step [337/704], Loss: 1.4925\n",
      "Epoch [1/20], Step [338/704], Loss: 1.6364\n",
      "Epoch [1/20], Step [339/704], Loss: 1.5174\n",
      "Epoch [1/20], Step [340/704], Loss: 1.4450\n",
      "Epoch [1/20], Step [341/704], Loss: 1.5744\n",
      "Epoch [1/20], Step [342/704], Loss: 1.2315\n",
      "Epoch [1/20], Step [343/704], Loss: 1.5344\n",
      "Epoch [1/20], Step [344/704], Loss: 1.4441\n",
      "Epoch [1/20], Step [345/704], Loss: 1.6907\n",
      "Epoch [1/20], Step [346/704], Loss: 1.7301\n",
      "Epoch [1/20], Step [347/704], Loss: 1.6625\n",
      "Epoch [1/20], Step [348/704], Loss: 1.3306\n",
      "Epoch [1/20], Step [349/704], Loss: 1.3824\n",
      "Epoch [1/20], Step [350/704], Loss: 1.6898\n",
      "Epoch [1/20], Step [351/704], Loss: 1.5036\n",
      "Epoch [1/20], Step [352/704], Loss: 1.4965\n",
      "Epoch [1/20], Step [353/704], Loss: 1.5370\n",
      "Epoch [1/20], Step [354/704], Loss: 1.4759\n",
      "Epoch [1/20], Step [355/704], Loss: 1.2795\n",
      "Epoch [1/20], Step [356/704], Loss: 1.4643\n",
      "Epoch [1/20], Step [357/704], Loss: 1.3408\n",
      "Epoch [1/20], Step [358/704], Loss: 1.3698\n",
      "Epoch [1/20], Step [359/704], Loss: 1.4424\n",
      "Epoch [1/20], Step [360/704], Loss: 1.3623\n",
      "Epoch [1/20], Step [361/704], Loss: 1.4313\n",
      "Epoch [1/20], Step [362/704], Loss: 1.4252\n",
      "Epoch [1/20], Step [363/704], Loss: 1.4329\n",
      "Epoch [1/20], Step [364/704], Loss: 1.5508\n",
      "Epoch [1/20], Step [365/704], Loss: 1.3482\n",
      "Epoch [1/20], Step [366/704], Loss: 1.6438\n",
      "Epoch [1/20], Step [367/704], Loss: 1.4282\n",
      "Epoch [1/20], Step [368/704], Loss: 1.5848\n",
      "Epoch [1/20], Step [369/704], Loss: 1.5954\n",
      "Epoch [1/20], Step [370/704], Loss: 1.3660\n",
      "Epoch [1/20], Step [371/704], Loss: 1.3154\n",
      "Epoch [1/20], Step [372/704], Loss: 1.5664\n",
      "Epoch [1/20], Step [373/704], Loss: 1.3290\n",
      "Epoch [1/20], Step [374/704], Loss: 1.4788\n",
      "Epoch [1/20], Step [375/704], Loss: 1.4995\n",
      "Epoch [1/20], Step [376/704], Loss: 1.3367\n",
      "Epoch [1/20], Step [377/704], Loss: 1.5709\n",
      "Epoch [1/20], Step [378/704], Loss: 1.4435\n",
      "Epoch [1/20], Step [379/704], Loss: 1.5278\n",
      "Epoch [1/20], Step [380/704], Loss: 1.6385\n",
      "Epoch [1/20], Step [381/704], Loss: 1.4584\n",
      "Epoch [1/20], Step [382/704], Loss: 1.4913\n",
      "Epoch [1/20], Step [383/704], Loss: 1.5004\n",
      "Epoch [1/20], Step [384/704], Loss: 1.3307\n",
      "Epoch [1/20], Step [385/704], Loss: 1.4877\n",
      "Epoch [1/20], Step [386/704], Loss: 1.6056\n",
      "Epoch [1/20], Step [387/704], Loss: 1.3775\n",
      "Epoch [1/20], Step [388/704], Loss: 1.5821\n",
      "Epoch [1/20], Step [389/704], Loss: 1.3788\n",
      "Epoch [1/20], Step [390/704], Loss: 1.5572\n",
      "Epoch [1/20], Step [391/704], Loss: 1.3405\n",
      "Epoch [1/20], Step [392/704], Loss: 1.6410\n",
      "Epoch [1/20], Step [393/704], Loss: 1.4769\n",
      "Epoch [1/20], Step [394/704], Loss: 1.5605\n",
      "Epoch [1/20], Step [395/704], Loss: 1.3806\n",
      "Epoch [1/20], Step [396/704], Loss: 1.8495\n",
      "Epoch [1/20], Step [397/704], Loss: 1.6289\n",
      "Epoch [1/20], Step [398/704], Loss: 1.6391\n",
      "Epoch [1/20], Step [399/704], Loss: 1.6931\n",
      "Epoch [1/20], Step [400/704], Loss: 1.7619\n",
      "Epoch [1/20], Step [401/704], Loss: 1.4561\n",
      "Epoch [1/20], Step [402/704], Loss: 1.4281\n",
      "Epoch [1/20], Step [403/704], Loss: 1.3815\n",
      "Epoch [1/20], Step [404/704], Loss: 1.4915\n",
      "Epoch [1/20], Step [405/704], Loss: 1.3481\n",
      "Epoch [1/20], Step [406/704], Loss: 1.4229\n",
      "Epoch [1/20], Step [407/704], Loss: 1.6427\n",
      "Epoch [1/20], Step [408/704], Loss: 1.6952\n",
      "Epoch [1/20], Step [409/704], Loss: 1.6832\n",
      "Epoch [1/20], Step [410/704], Loss: 1.6093\n",
      "Epoch [1/20], Step [411/704], Loss: 1.5000\n",
      "Epoch [1/20], Step [412/704], Loss: 1.5811\n",
      "Epoch [1/20], Step [413/704], Loss: 1.4869\n",
      "Epoch [1/20], Step [414/704], Loss: 1.4309\n",
      "Epoch [1/20], Step [415/704], Loss: 1.3405\n",
      "Epoch [1/20], Step [416/704], Loss: 1.4253\n",
      "Epoch [1/20], Step [417/704], Loss: 1.5469\n",
      "Epoch [1/20], Step [418/704], Loss: 1.5551\n",
      "Epoch [1/20], Step [419/704], Loss: 1.5683\n",
      "Epoch [1/20], Step [420/704], Loss: 1.4846\n",
      "Epoch [1/20], Step [421/704], Loss: 1.4733\n",
      "Epoch [1/20], Step [422/704], Loss: 1.5934\n",
      "Epoch [1/20], Step [423/704], Loss: 1.3715\n",
      "Epoch [1/20], Step [424/704], Loss: 1.4129\n",
      "Epoch [1/20], Step [425/704], Loss: 1.5041\n",
      "Epoch [1/20], Step [426/704], Loss: 1.5700\n",
      "Epoch [1/20], Step [427/704], Loss: 1.4151\n",
      "Epoch [1/20], Step [428/704], Loss: 1.2885\n",
      "Epoch [1/20], Step [429/704], Loss: 1.4610\n",
      "Epoch [1/20], Step [430/704], Loss: 1.4013\n",
      "Epoch [1/20], Step [431/704], Loss: 1.3849\n",
      "Epoch [1/20], Step [432/704], Loss: 1.3409\n",
      "Epoch [1/20], Step [433/704], Loss: 1.4952\n",
      "Epoch [1/20], Step [434/704], Loss: 1.3897\n",
      "Epoch [1/20], Step [435/704], Loss: 1.6529\n",
      "Epoch [1/20], Step [436/704], Loss: 1.7128\n",
      "Epoch [1/20], Step [437/704], Loss: 1.4032\n",
      "Epoch [1/20], Step [438/704], Loss: 1.6839\n",
      "Epoch [1/20], Step [439/704], Loss: 1.2756\n",
      "Epoch [1/20], Step [440/704], Loss: 1.3089\n",
      "Epoch [1/20], Step [441/704], Loss: 1.7678\n",
      "Epoch [1/20], Step [442/704], Loss: 1.3900\n",
      "Epoch [1/20], Step [443/704], Loss: 1.4896\n",
      "Epoch [1/20], Step [444/704], Loss: 1.1751\n",
      "Epoch [1/20], Step [445/704], Loss: 1.4466\n",
      "Epoch [1/20], Step [446/704], Loss: 1.4985\n",
      "Epoch [1/20], Step [447/704], Loss: 1.3697\n",
      "Epoch [1/20], Step [448/704], Loss: 1.3709\n",
      "Epoch [1/20], Step [449/704], Loss: 1.2354\n",
      "Epoch [1/20], Step [450/704], Loss: 1.3910\n",
      "Epoch [1/20], Step [451/704], Loss: 1.3479\n",
      "Epoch [1/20], Step [452/704], Loss: 1.4528\n",
      "Epoch [1/20], Step [453/704], Loss: 1.3930\n",
      "Epoch [1/20], Step [454/704], Loss: 1.2685\n",
      "Epoch [1/20], Step [455/704], Loss: 1.2140\n",
      "Epoch [1/20], Step [456/704], Loss: 1.3412\n",
      "Epoch [1/20], Step [457/704], Loss: 1.5113\n",
      "Epoch [1/20], Step [458/704], Loss: 1.2706\n",
      "Epoch [1/20], Step [459/704], Loss: 1.3452\n",
      "Epoch [1/20], Step [460/704], Loss: 1.3171\n",
      "Epoch [1/20], Step [461/704], Loss: 1.2863\n",
      "Epoch [1/20], Step [462/704], Loss: 1.1486\n",
      "Epoch [1/20], Step [463/704], Loss: 1.4406\n",
      "Epoch [1/20], Step [464/704], Loss: 1.4331\n",
      "Epoch [1/20], Step [465/704], Loss: 1.3282\n",
      "Epoch [1/20], Step [466/704], Loss: 1.7184\n",
      "Epoch [1/20], Step [467/704], Loss: 1.1708\n",
      "Epoch [1/20], Step [468/704], Loss: 1.2555\n",
      "Epoch [1/20], Step [469/704], Loss: 1.1833\n",
      "Epoch [1/20], Step [470/704], Loss: 1.4398\n",
      "Epoch [1/20], Step [471/704], Loss: 1.3215\n",
      "Epoch [1/20], Step [472/704], Loss: 1.4724\n",
      "Epoch [1/20], Step [473/704], Loss: 1.1006\n",
      "Epoch [1/20], Step [474/704], Loss: 1.6794\n",
      "Epoch [1/20], Step [475/704], Loss: 1.2321\n",
      "Epoch [1/20], Step [476/704], Loss: 1.3765\n",
      "Epoch [1/20], Step [477/704], Loss: 1.2993\n",
      "Epoch [1/20], Step [478/704], Loss: 1.5503\n",
      "Epoch [1/20], Step [479/704], Loss: 1.3115\n",
      "Epoch [1/20], Step [480/704], Loss: 1.3040\n",
      "Epoch [1/20], Step [481/704], Loss: 1.5447\n",
      "Epoch [1/20], Step [482/704], Loss: 1.3573\n",
      "Epoch [1/20], Step [483/704], Loss: 1.2155\n",
      "Epoch [1/20], Step [484/704], Loss: 1.3896\n",
      "Epoch [1/20], Step [485/704], Loss: 1.3478\n",
      "Epoch [1/20], Step [486/704], Loss: 1.3033\n",
      "Epoch [1/20], Step [487/704], Loss: 1.5134\n",
      "Epoch [1/20], Step [488/704], Loss: 1.4222\n",
      "Epoch [1/20], Step [489/704], Loss: 1.5169\n",
      "Epoch [1/20], Step [490/704], Loss: 1.2977\n",
      "Epoch [1/20], Step [491/704], Loss: 1.3183\n",
      "Epoch [1/20], Step [492/704], Loss: 1.3887\n",
      "Epoch [1/20], Step [493/704], Loss: 1.2756\n",
      "Epoch [1/20], Step [494/704], Loss: 1.3865\n",
      "Epoch [1/20], Step [495/704], Loss: 1.4960\n",
      "Epoch [1/20], Step [496/704], Loss: 1.3571\n",
      "Epoch [1/20], Step [497/704], Loss: 1.5158\n",
      "Epoch [1/20], Step [498/704], Loss: 1.7281\n",
      "Epoch [1/20], Step [499/704], Loss: 1.4771\n",
      "Epoch [1/20], Step [500/704], Loss: 1.2378\n",
      "Epoch [1/20], Step [501/704], Loss: 1.2472\n",
      "Epoch [1/20], Step [502/704], Loss: 1.2730\n",
      "Epoch [1/20], Step [503/704], Loss: 1.7029\n",
      "Epoch [1/20], Step [504/704], Loss: 1.5381\n",
      "Epoch [1/20], Step [505/704], Loss: 1.3436\n",
      "Epoch [1/20], Step [506/704], Loss: 1.3228\n",
      "Epoch [1/20], Step [507/704], Loss: 1.3983\n",
      "Epoch [1/20], Step [508/704], Loss: 1.1729\n",
      "Epoch [1/20], Step [509/704], Loss: 1.5669\n",
      "Epoch [1/20], Step [510/704], Loss: 1.2236\n",
      "Epoch [1/20], Step [511/704], Loss: 1.3959\n",
      "Epoch [1/20], Step [512/704], Loss: 1.4597\n",
      "Epoch [1/20], Step [513/704], Loss: 1.3799\n",
      "Epoch [1/20], Step [514/704], Loss: 1.3867\n",
      "Epoch [1/20], Step [515/704], Loss: 1.4233\n",
      "Epoch [1/20], Step [516/704], Loss: 1.4260\n",
      "Epoch [1/20], Step [517/704], Loss: 1.3667\n",
      "Epoch [1/20], Step [518/704], Loss: 1.2632\n",
      "Epoch [1/20], Step [519/704], Loss: 1.3311\n",
      "Epoch [1/20], Step [520/704], Loss: 1.2396\n",
      "Epoch [1/20], Step [521/704], Loss: 1.2638\n",
      "Epoch [1/20], Step [522/704], Loss: 1.3161\n",
      "Epoch [1/20], Step [523/704], Loss: 1.4060\n",
      "Epoch [1/20], Step [524/704], Loss: 1.2845\n",
      "Epoch [1/20], Step [525/704], Loss: 1.5189\n",
      "Epoch [1/20], Step [526/704], Loss: 1.5708\n",
      "Epoch [1/20], Step [527/704], Loss: 1.6025\n",
      "Epoch [1/20], Step [528/704], Loss: 1.2502\n",
      "Epoch [1/20], Step [529/704], Loss: 1.2911\n",
      "Epoch [1/20], Step [530/704], Loss: 1.3636\n",
      "Epoch [1/20], Step [531/704], Loss: 1.3847\n",
      "Epoch [1/20], Step [532/704], Loss: 1.5173\n",
      "Epoch [1/20], Step [533/704], Loss: 1.6331\n",
      "Epoch [1/20], Step [534/704], Loss: 1.2456\n",
      "Epoch [1/20], Step [535/704], Loss: 1.3450\n",
      "Epoch [1/20], Step [536/704], Loss: 1.2248\n",
      "Epoch [1/20], Step [537/704], Loss: 1.3419\n",
      "Epoch [1/20], Step [538/704], Loss: 1.1906\n",
      "Epoch [1/20], Step [539/704], Loss: 1.2678\n",
      "Epoch [1/20], Step [540/704], Loss: 1.3453\n",
      "Epoch [1/20], Step [541/704], Loss: 1.1662\n",
      "Epoch [1/20], Step [542/704], Loss: 1.2880\n",
      "Epoch [1/20], Step [543/704], Loss: 1.1985\n",
      "Epoch [1/20], Step [544/704], Loss: 1.4707\n",
      "Epoch [1/20], Step [545/704], Loss: 1.1898\n",
      "Epoch [1/20], Step [546/704], Loss: 1.0075\n",
      "Epoch [1/20], Step [547/704], Loss: 1.2835\n",
      "Epoch [1/20], Step [548/704], Loss: 1.5115\n",
      "Epoch [1/20], Step [549/704], Loss: 1.1606\n",
      "Epoch [1/20], Step [550/704], Loss: 1.3928\n",
      "Epoch [1/20], Step [551/704], Loss: 1.2744\n",
      "Epoch [1/20], Step [552/704], Loss: 1.2371\n",
      "Epoch [1/20], Step [553/704], Loss: 1.5746\n",
      "Epoch [1/20], Step [554/704], Loss: 1.3606\n",
      "Epoch [1/20], Step [555/704], Loss: 1.3913\n",
      "Epoch [1/20], Step [556/704], Loss: 1.2329\n",
      "Epoch [1/20], Step [557/704], Loss: 1.4202\n",
      "Epoch [1/20], Step [558/704], Loss: 1.2404\n",
      "Epoch [1/20], Step [559/704], Loss: 1.6037\n",
      "Epoch [1/20], Step [560/704], Loss: 1.1757\n",
      "Epoch [1/20], Step [561/704], Loss: 1.3930\n",
      "Epoch [1/20], Step [562/704], Loss: 1.2075\n",
      "Epoch [1/20], Step [563/704], Loss: 1.2930\n",
      "Epoch [1/20], Step [564/704], Loss: 1.4983\n",
      "Epoch [1/20], Step [565/704], Loss: 0.9494\n",
      "Epoch [1/20], Step [566/704], Loss: 1.5425\n",
      "Epoch [1/20], Step [567/704], Loss: 1.5521\n",
      "Epoch [1/20], Step [568/704], Loss: 1.3379\n",
      "Epoch [1/20], Step [569/704], Loss: 1.2405\n",
      "Epoch [1/20], Step [570/704], Loss: 1.3410\n",
      "Epoch [1/20], Step [571/704], Loss: 1.2614\n",
      "Epoch [1/20], Step [572/704], Loss: 1.4174\n",
      "Epoch [1/20], Step [573/704], Loss: 1.3271\n",
      "Epoch [1/20], Step [574/704], Loss: 1.1108\n",
      "Epoch [1/20], Step [575/704], Loss: 1.1189\n",
      "Epoch [1/20], Step [576/704], Loss: 1.0056\n",
      "Epoch [1/20], Step [577/704], Loss: 1.4725\n",
      "Epoch [1/20], Step [578/704], Loss: 1.0252\n",
      "Epoch [1/20], Step [579/704], Loss: 1.4865\n",
      "Epoch [1/20], Step [580/704], Loss: 1.2437\n",
      "Epoch [1/20], Step [581/704], Loss: 1.1088\n",
      "Epoch [1/20], Step [582/704], Loss: 1.0536\n",
      "Epoch [1/20], Step [583/704], Loss: 1.2699\n",
      "Epoch [1/20], Step [584/704], Loss: 1.2619\n",
      "Epoch [1/20], Step [585/704], Loss: 1.0592\n",
      "Epoch [1/20], Step [586/704], Loss: 1.6246\n",
      "Epoch [1/20], Step [587/704], Loss: 1.0864\n",
      "Epoch [1/20], Step [588/704], Loss: 1.4021\n",
      "Epoch [1/20], Step [589/704], Loss: 1.2675\n",
      "Epoch [1/20], Step [590/704], Loss: 1.3420\n",
      "Epoch [1/20], Step [591/704], Loss: 1.3017\n",
      "Epoch [1/20], Step [592/704], Loss: 1.2729\n",
      "Epoch [1/20], Step [593/704], Loss: 1.2098\n",
      "Epoch [1/20], Step [594/704], Loss: 1.2430\n",
      "Epoch [1/20], Step [595/704], Loss: 1.3153\n",
      "Epoch [1/20], Step [596/704], Loss: 1.1760\n",
      "Epoch [1/20], Step [597/704], Loss: 1.2122\n",
      "Epoch [1/20], Step [598/704], Loss: 1.5176\n",
      "Epoch [1/20], Step [599/704], Loss: 1.2213\n",
      "Epoch [1/20], Step [600/704], Loss: 1.3784\n",
      "Epoch [1/20], Step [601/704], Loss: 1.1210\n",
      "Epoch [1/20], Step [602/704], Loss: 1.3389\n",
      "Epoch [1/20], Step [603/704], Loss: 1.1701\n",
      "Epoch [1/20], Step [604/704], Loss: 1.0201\n",
      "Epoch [1/20], Step [605/704], Loss: 1.2459\n",
      "Epoch [1/20], Step [606/704], Loss: 0.9975\n",
      "Epoch [1/20], Step [607/704], Loss: 1.2549\n",
      "Epoch [1/20], Step [608/704], Loss: 1.1545\n",
      "Epoch [1/20], Step [609/704], Loss: 1.2108\n",
      "Epoch [1/20], Step [610/704], Loss: 1.0919\n",
      "Epoch [1/20], Step [611/704], Loss: 1.1116\n",
      "Epoch [1/20], Step [612/704], Loss: 1.3373\n",
      "Epoch [1/20], Step [613/704], Loss: 1.1587\n",
      "Epoch [1/20], Step [614/704], Loss: 1.1818\n",
      "Epoch [1/20], Step [615/704], Loss: 1.0467\n",
      "Epoch [1/20], Step [616/704], Loss: 1.1631\n",
      "Epoch [1/20], Step [617/704], Loss: 1.3386\n",
      "Epoch [1/20], Step [618/704], Loss: 1.2380\n",
      "Epoch [1/20], Step [619/704], Loss: 1.2813\n",
      "Epoch [1/20], Step [620/704], Loss: 1.2735\n",
      "Epoch [1/20], Step [621/704], Loss: 0.9999\n",
      "Epoch [1/20], Step [622/704], Loss: 1.2272\n",
      "Epoch [1/20], Step [623/704], Loss: 1.2085\n",
      "Epoch [1/20], Step [624/704], Loss: 1.1302\n",
      "Epoch [1/20], Step [625/704], Loss: 1.3117\n",
      "Epoch [1/20], Step [626/704], Loss: 1.3240\n",
      "Epoch [1/20], Step [627/704], Loss: 1.2012\n",
      "Epoch [1/20], Step [628/704], Loss: 1.1852\n",
      "Epoch [1/20], Step [629/704], Loss: 1.3422\n",
      "Epoch [1/20], Step [630/704], Loss: 1.2300\n",
      "Epoch [1/20], Step [631/704], Loss: 1.4408\n",
      "Epoch [1/20], Step [632/704], Loss: 1.3275\n",
      "Epoch [1/20], Step [633/704], Loss: 1.1193\n",
      "Epoch [1/20], Step [634/704], Loss: 1.1974\n",
      "Epoch [1/20], Step [635/704], Loss: 1.0005\n",
      "Epoch [1/20], Step [636/704], Loss: 1.1232\n",
      "Epoch [1/20], Step [637/704], Loss: 1.2911\n",
      "Epoch [1/20], Step [638/704], Loss: 1.2630\n",
      "Epoch [1/20], Step [639/704], Loss: 1.2391\n",
      "Epoch [1/20], Step [640/704], Loss: 1.1075\n",
      "Epoch [1/20], Step [641/704], Loss: 1.0853\n",
      "Epoch [1/20], Step [642/704], Loss: 1.2551\n",
      "Epoch [1/20], Step [643/704], Loss: 1.4212\n",
      "Epoch [1/20], Step [644/704], Loss: 1.3099\n",
      "Epoch [1/20], Step [645/704], Loss: 1.3313\n",
      "Epoch [1/20], Step [646/704], Loss: 1.2246\n",
      "Epoch [1/20], Step [647/704], Loss: 1.0414\n",
      "Epoch [1/20], Step [648/704], Loss: 1.4441\n",
      "Epoch [1/20], Step [649/704], Loss: 1.4759\n",
      "Epoch [1/20], Step [650/704], Loss: 1.2626\n",
      "Epoch [1/20], Step [651/704], Loss: 1.1781\n",
      "Epoch [1/20], Step [652/704], Loss: 1.2222\n",
      "Epoch [1/20], Step [653/704], Loss: 1.5534\n",
      "Epoch [1/20], Step [654/704], Loss: 1.1852\n",
      "Epoch [1/20], Step [655/704], Loss: 1.3002\n",
      "Epoch [1/20], Step [656/704], Loss: 1.0987\n",
      "Epoch [1/20], Step [657/704], Loss: 1.0358\n",
      "Epoch [1/20], Step [658/704], Loss: 1.4509\n",
      "Epoch [1/20], Step [659/704], Loss: 1.4015\n",
      "Epoch [1/20], Step [660/704], Loss: 1.0920\n",
      "Epoch [1/20], Step [661/704], Loss: 1.4580\n",
      "Epoch [1/20], Step [662/704], Loss: 1.0342\n",
      "Epoch [1/20], Step [663/704], Loss: 1.1166\n",
      "Epoch [1/20], Step [664/704], Loss: 1.2534\n",
      "Epoch [1/20], Step [665/704], Loss: 1.1956\n",
      "Epoch [1/20], Step [666/704], Loss: 1.2003\n",
      "Epoch [1/20], Step [667/704], Loss: 1.3868\n",
      "Epoch [1/20], Step [668/704], Loss: 1.0793\n",
      "Epoch [1/20], Step [669/704], Loss: 1.6706\n",
      "Epoch [1/20], Step [670/704], Loss: 1.1649\n",
      "Epoch [1/20], Step [671/704], Loss: 0.9904\n",
      "Epoch [1/20], Step [672/704], Loss: 1.2980\n",
      "Epoch [1/20], Step [673/704], Loss: 1.1526\n",
      "Epoch [1/20], Step [674/704], Loss: 1.3805\n",
      "Epoch [1/20], Step [675/704], Loss: 1.1430\n",
      "Epoch [1/20], Step [676/704], Loss: 1.3680\n",
      "Epoch [1/20], Step [677/704], Loss: 1.1940\n",
      "Epoch [1/20], Step [678/704], Loss: 1.2334\n",
      "Epoch [1/20], Step [679/704], Loss: 1.1780\n",
      "Epoch [1/20], Step [680/704], Loss: 1.0855\n",
      "Epoch [1/20], Step [681/704], Loss: 1.0406\n",
      "Epoch [1/20], Step [682/704], Loss: 1.4528\n",
      "Epoch [1/20], Step [683/704], Loss: 1.2254\n",
      "Epoch [1/20], Step [684/704], Loss: 1.1500\n",
      "Epoch [1/20], Step [685/704], Loss: 1.2413\n",
      "Epoch [1/20], Step [686/704], Loss: 0.9468\n",
      "Epoch [1/20], Step [687/704], Loss: 1.1487\n",
      "Epoch [1/20], Step [688/704], Loss: 1.1347\n",
      "Epoch [1/20], Step [689/704], Loss: 1.1883\n",
      "Epoch [1/20], Step [690/704], Loss: 1.2824\n",
      "Epoch [1/20], Step [691/704], Loss: 1.1177\n",
      "Epoch [1/20], Step [692/704], Loss: 1.1424\n",
      "Epoch [1/20], Step [693/704], Loss: 1.1722\n",
      "Epoch [1/20], Step [694/704], Loss: 1.1327\n",
      "Epoch [1/20], Step [695/704], Loss: 1.0525\n",
      "Epoch [1/20], Step [696/704], Loss: 1.2432\n",
      "Epoch [1/20], Step [697/704], Loss: 1.0526\n",
      "Epoch [1/20], Step [698/704], Loss: 1.2858\n",
      "Epoch [1/20], Step [699/704], Loss: 1.3678\n",
      "Epoch [1/20], Step [700/704], Loss: 1.2393\n",
      "Epoch [1/20], Step [701/704], Loss: 1.1748\n",
      "Epoch [1/20], Step [702/704], Loss: 1.2100\n",
      "Epoch [1/20], Step [703/704], Loss: 1.0748\n",
      "Epoch [1/20], Step [704/704], Loss: 1.7139\n",
      "Epoch [1/20], Loss: 1.7139\n",
      "Epoch [2/20], Step [1/704], Loss: 1.2237\n",
      "Epoch [2/20], Step [2/704], Loss: 1.3518\n",
      "Epoch [2/20], Step [3/704], Loss: 1.1862\n",
      "Epoch [2/20], Step [4/704], Loss: 1.4207\n",
      "Epoch [2/20], Step [5/704], Loss: 1.2726\n",
      "Epoch [2/20], Step [6/704], Loss: 1.2300\n",
      "Epoch [2/20], Step [7/704], Loss: 1.1081\n",
      "Epoch [2/20], Step [8/704], Loss: 1.2178\n",
      "Epoch [2/20], Step [9/704], Loss: 1.4210\n",
      "Epoch [2/20], Step [10/704], Loss: 1.3190\n",
      "Epoch [2/20], Step [11/704], Loss: 1.3297\n",
      "Epoch [2/20], Step [12/704], Loss: 1.1649\n",
      "Epoch [2/20], Step [13/704], Loss: 1.2325\n",
      "Epoch [2/20], Step [14/704], Loss: 1.3154\n",
      "Epoch [2/20], Step [15/704], Loss: 1.0757\n",
      "Epoch [2/20], Step [16/704], Loss: 1.2258\n",
      "Epoch [2/20], Step [17/704], Loss: 1.4571\n",
      "Epoch [2/20], Step [18/704], Loss: 1.2036\n",
      "Epoch [2/20], Step [19/704], Loss: 1.2016\n",
      "Epoch [2/20], Step [20/704], Loss: 1.3566\n",
      "Epoch [2/20], Step [21/704], Loss: 1.0283\n",
      "Epoch [2/20], Step [22/704], Loss: 1.1783\n",
      "Epoch [2/20], Step [23/704], Loss: 1.0278\n",
      "Epoch [2/20], Step [24/704], Loss: 0.9868\n",
      "Epoch [2/20], Step [25/704], Loss: 1.3340\n",
      "Epoch [2/20], Step [26/704], Loss: 1.2280\n",
      "Epoch [2/20], Step [27/704], Loss: 1.0201\n",
      "Epoch [2/20], Step [28/704], Loss: 1.0988\n",
      "Epoch [2/20], Step [29/704], Loss: 1.0295\n",
      "Epoch [2/20], Step [30/704], Loss: 1.0819\n",
      "Epoch [2/20], Step [31/704], Loss: 1.3805\n",
      "Epoch [2/20], Step [32/704], Loss: 0.9542\n",
      "Epoch [2/20], Step [33/704], Loss: 1.3770\n",
      "Epoch [2/20], Step [34/704], Loss: 1.0574\n",
      "Epoch [2/20], Step [35/704], Loss: 0.8901\n",
      "Epoch [2/20], Step [36/704], Loss: 0.9636\n",
      "Epoch [2/20], Step [37/704], Loss: 1.4458\n",
      "Epoch [2/20], Step [38/704], Loss: 1.1914\n",
      "Epoch [2/20], Step [39/704], Loss: 1.0280\n",
      "Epoch [2/20], Step [40/704], Loss: 1.0109\n",
      "Epoch [2/20], Step [41/704], Loss: 1.0744\n",
      "Epoch [2/20], Step [42/704], Loss: 0.9130\n",
      "Epoch [2/20], Step [43/704], Loss: 0.9484\n",
      "Epoch [2/20], Step [44/704], Loss: 1.3096\n",
      "Epoch [2/20], Step [45/704], Loss: 1.0401\n",
      "Epoch [2/20], Step [46/704], Loss: 1.1992\n",
      "Epoch [2/20], Step [47/704], Loss: 1.1038\n",
      "Epoch [2/20], Step [48/704], Loss: 1.3914\n",
      "Epoch [2/20], Step [49/704], Loss: 1.1402\n",
      "Epoch [2/20], Step [50/704], Loss: 1.1677\n",
      "Epoch [2/20], Step [51/704], Loss: 1.2795\n",
      "Epoch [2/20], Step [52/704], Loss: 1.1948\n",
      "Epoch [2/20], Step [53/704], Loss: 1.2674\n",
      "Epoch [2/20], Step [54/704], Loss: 1.1766\n",
      "Epoch [2/20], Step [55/704], Loss: 1.1176\n",
      "Epoch [2/20], Step [56/704], Loss: 1.3747\n",
      "Epoch [2/20], Step [57/704], Loss: 1.0925\n",
      "Epoch [2/20], Step [58/704], Loss: 0.9980\n",
      "Epoch [2/20], Step [59/704], Loss: 0.9463\n",
      "Epoch [2/20], Step [60/704], Loss: 0.9749\n",
      "Epoch [2/20], Step [61/704], Loss: 1.4135\n",
      "Epoch [2/20], Step [62/704], Loss: 1.1543\n",
      "Epoch [2/20], Step [63/704], Loss: 1.1247\n",
      "Epoch [2/20], Step [64/704], Loss: 1.1405\n",
      "Epoch [2/20], Step [65/704], Loss: 1.0091\n",
      "Epoch [2/20], Step [66/704], Loss: 1.0422\n",
      "Epoch [2/20], Step [67/704], Loss: 1.3343\n",
      "Epoch [2/20], Step [68/704], Loss: 1.3656\n",
      "Epoch [2/20], Step [69/704], Loss: 0.8579\n",
      "Epoch [2/20], Step [70/704], Loss: 1.1403\n",
      "Epoch [2/20], Step [71/704], Loss: 1.1443\n",
      "Epoch [2/20], Step [72/704], Loss: 0.9768\n",
      "Epoch [2/20], Step [73/704], Loss: 0.9247\n",
      "Epoch [2/20], Step [74/704], Loss: 1.1935\n",
      "Epoch [2/20], Step [75/704], Loss: 0.9960\n",
      "Epoch [2/20], Step [76/704], Loss: 1.2481\n",
      "Epoch [2/20], Step [77/704], Loss: 1.2613\n",
      "Epoch [2/20], Step [78/704], Loss: 1.0892\n",
      "Epoch [2/20], Step [79/704], Loss: 1.0825\n",
      "Epoch [2/20], Step [80/704], Loss: 1.6765\n",
      "Epoch [2/20], Step [81/704], Loss: 1.1633\n",
      "Epoch [2/20], Step [82/704], Loss: 1.0831\n",
      "Epoch [2/20], Step [83/704], Loss: 0.9972\n",
      "Epoch [2/20], Step [84/704], Loss: 0.9825\n",
      "Epoch [2/20], Step [85/704], Loss: 1.3801\n",
      "Epoch [2/20], Step [86/704], Loss: 1.1592\n",
      "Epoch [2/20], Step [87/704], Loss: 1.1514\n",
      "Epoch [2/20], Step [88/704], Loss: 1.2445\n",
      "Epoch [2/20], Step [89/704], Loss: 1.1027\n",
      "Epoch [2/20], Step [90/704], Loss: 1.1644\n",
      "Epoch [2/20], Step [91/704], Loss: 1.0144\n",
      "Epoch [2/20], Step [92/704], Loss: 1.1143\n",
      "Epoch [2/20], Step [93/704], Loss: 1.0040\n",
      "Epoch [2/20], Step [94/704], Loss: 1.2714\n",
      "Epoch [2/20], Step [95/704], Loss: 1.1194\n",
      "Epoch [2/20], Step [96/704], Loss: 0.9942\n",
      "Epoch [2/20], Step [97/704], Loss: 1.2625\n",
      "Epoch [2/20], Step [98/704], Loss: 1.1704\n",
      "Epoch [2/20], Step [99/704], Loss: 1.2351\n",
      "Epoch [2/20], Step [100/704], Loss: 0.9546\n",
      "Epoch [2/20], Step [101/704], Loss: 0.9137\n",
      "Epoch [2/20], Step [102/704], Loss: 0.9553\n",
      "Epoch [2/20], Step [103/704], Loss: 1.0813\n",
      "Epoch [2/20], Step [104/704], Loss: 1.0312\n",
      "Epoch [2/20], Step [105/704], Loss: 0.9838\n",
      "Epoch [2/20], Step [106/704], Loss: 0.9754\n",
      "Epoch [2/20], Step [107/704], Loss: 1.0652\n",
      "Epoch [2/20], Step [108/704], Loss: 1.0755\n",
      "Epoch [2/20], Step [109/704], Loss: 1.1696\n",
      "Epoch [2/20], Step [110/704], Loss: 1.1367\n",
      "Epoch [2/20], Step [111/704], Loss: 1.0271\n",
      "Epoch [2/20], Step [112/704], Loss: 1.1754\n",
      "Epoch [2/20], Step [113/704], Loss: 0.8831\n",
      "Epoch [2/20], Step [114/704], Loss: 1.1488\n",
      "Epoch [2/20], Step [115/704], Loss: 0.9571\n",
      "Epoch [2/20], Step [116/704], Loss: 0.9707\n",
      "Epoch [2/20], Step [117/704], Loss: 0.9868\n",
      "Epoch [2/20], Step [118/704], Loss: 0.7416\n",
      "Epoch [2/20], Step [119/704], Loss: 1.0617\n",
      "Epoch [2/20], Step [120/704], Loss: 1.4208\n",
      "Epoch [2/20], Step [121/704], Loss: 1.1259\n",
      "Epoch [2/20], Step [122/704], Loss: 0.9928\n",
      "Epoch [2/20], Step [123/704], Loss: 1.2930\n",
      "Epoch [2/20], Step [124/704], Loss: 1.2105\n",
      "Epoch [2/20], Step [125/704], Loss: 0.9100\n",
      "Epoch [2/20], Step [126/704], Loss: 0.8714\n",
      "Epoch [2/20], Step [127/704], Loss: 1.1947\n",
      "Epoch [2/20], Step [128/704], Loss: 1.0266\n",
      "Epoch [2/20], Step [129/704], Loss: 1.2174\n",
      "Epoch [2/20], Step [130/704], Loss: 1.0148\n",
      "Epoch [2/20], Step [131/704], Loss: 1.2356\n",
      "Epoch [2/20], Step [132/704], Loss: 1.1152\n",
      "Epoch [2/20], Step [133/704], Loss: 1.0228\n",
      "Epoch [2/20], Step [134/704], Loss: 1.1380\n",
      "Epoch [2/20], Step [135/704], Loss: 0.9585\n",
      "Epoch [2/20], Step [136/704], Loss: 1.2331\n",
      "Epoch [2/20], Step [137/704], Loss: 1.3157\n",
      "Epoch [2/20], Step [138/704], Loss: 0.9546\n",
      "Epoch [2/20], Step [139/704], Loss: 0.8791\n",
      "Epoch [2/20], Step [140/704], Loss: 1.0510\n",
      "Epoch [2/20], Step [141/704], Loss: 1.0052\n",
      "Epoch [2/20], Step [142/704], Loss: 0.8366\n",
      "Epoch [2/20], Step [143/704], Loss: 1.1399\n",
      "Epoch [2/20], Step [144/704], Loss: 0.9170\n",
      "Epoch [2/20], Step [145/704], Loss: 1.0959\n",
      "Epoch [2/20], Step [146/704], Loss: 1.0821\n",
      "Epoch [2/20], Step [147/704], Loss: 1.1382\n",
      "Epoch [2/20], Step [148/704], Loss: 1.0272\n",
      "Epoch [2/20], Step [149/704], Loss: 0.9506\n",
      "Epoch [2/20], Step [150/704], Loss: 1.3362\n",
      "Epoch [2/20], Step [151/704], Loss: 0.9052\n",
      "Epoch [2/20], Step [152/704], Loss: 1.1388\n",
      "Epoch [2/20], Step [153/704], Loss: 0.9690\n",
      "Epoch [2/20], Step [154/704], Loss: 1.0289\n",
      "Epoch [2/20], Step [155/704], Loss: 1.1574\n",
      "Epoch [2/20], Step [156/704], Loss: 0.8025\n",
      "Epoch [2/20], Step [157/704], Loss: 0.9762\n",
      "Epoch [2/20], Step [158/704], Loss: 0.9940\n",
      "Epoch [2/20], Step [159/704], Loss: 0.8944\n",
      "Epoch [2/20], Step [160/704], Loss: 0.9333\n",
      "Epoch [2/20], Step [161/704], Loss: 1.1449\n",
      "Epoch [2/20], Step [162/704], Loss: 0.9484\n",
      "Epoch [2/20], Step [163/704], Loss: 1.0408\n",
      "Epoch [2/20], Step [164/704], Loss: 0.9555\n",
      "Epoch [2/20], Step [165/704], Loss: 0.9230\n",
      "Epoch [2/20], Step [166/704], Loss: 0.8620\n",
      "Epoch [2/20], Step [167/704], Loss: 1.1939\n",
      "Epoch [2/20], Step [168/704], Loss: 1.1352\n",
      "Epoch [2/20], Step [169/704], Loss: 0.7498\n",
      "Epoch [2/20], Step [170/704], Loss: 0.9579\n",
      "Epoch [2/20], Step [171/704], Loss: 1.0900\n",
      "Epoch [2/20], Step [172/704], Loss: 1.0744\n",
      "Epoch [2/20], Step [173/704], Loss: 0.8980\n",
      "Epoch [2/20], Step [174/704], Loss: 1.0525\n",
      "Epoch [2/20], Step [175/704], Loss: 1.1953\n",
      "Epoch [2/20], Step [176/704], Loss: 1.0960\n",
      "Epoch [2/20], Step [177/704], Loss: 1.1957\n",
      "Epoch [2/20], Step [178/704], Loss: 0.8984\n",
      "Epoch [2/20], Step [179/704], Loss: 0.9639\n",
      "Epoch [2/20], Step [180/704], Loss: 1.0168\n",
      "Epoch [2/20], Step [181/704], Loss: 1.0265\n",
      "Epoch [2/20], Step [182/704], Loss: 1.2053\n",
      "Epoch [2/20], Step [183/704], Loss: 1.0774\n",
      "Epoch [2/20], Step [184/704], Loss: 1.2214\n",
      "Epoch [2/20], Step [185/704], Loss: 0.7708\n",
      "Epoch [2/20], Step [186/704], Loss: 0.8774\n",
      "Epoch [2/20], Step [187/704], Loss: 0.9109\n",
      "Epoch [2/20], Step [188/704], Loss: 0.9182\n",
      "Epoch [2/20], Step [189/704], Loss: 0.9307\n",
      "Epoch [2/20], Step [190/704], Loss: 0.7531\n",
      "Epoch [2/20], Step [191/704], Loss: 1.1897\n",
      "Epoch [2/20], Step [192/704], Loss: 0.9164\n",
      "Epoch [2/20], Step [193/704], Loss: 1.0528\n",
      "Epoch [2/20], Step [194/704], Loss: 1.1511\n",
      "Epoch [2/20], Step [195/704], Loss: 0.9470\n",
      "Epoch [2/20], Step [196/704], Loss: 0.7312\n",
      "Epoch [2/20], Step [197/704], Loss: 0.9700\n",
      "Epoch [2/20], Step [198/704], Loss: 0.8874\n",
      "Epoch [2/20], Step [199/704], Loss: 0.9755\n",
      "Epoch [2/20], Step [200/704], Loss: 1.0601\n",
      "Epoch [2/20], Step [201/704], Loss: 0.9923\n",
      "Epoch [2/20], Step [202/704], Loss: 1.3444\n",
      "Epoch [2/20], Step [203/704], Loss: 1.1286\n",
      "Epoch [2/20], Step [204/704], Loss: 1.0727\n",
      "Epoch [2/20], Step [205/704], Loss: 0.8462\n",
      "Epoch [2/20], Step [206/704], Loss: 0.9899\n",
      "Epoch [2/20], Step [207/704], Loss: 1.0664\n",
      "Epoch [2/20], Step [208/704], Loss: 0.9964\n",
      "Epoch [2/20], Step [209/704], Loss: 0.6336\n",
      "Epoch [2/20], Step [210/704], Loss: 1.2521\n",
      "Epoch [2/20], Step [211/704], Loss: 1.1341\n",
      "Epoch [2/20], Step [212/704], Loss: 0.9760\n",
      "Epoch [2/20], Step [213/704], Loss: 0.8736\n",
      "Epoch [2/20], Step [214/704], Loss: 1.0010\n",
      "Epoch [2/20], Step [215/704], Loss: 1.2355\n",
      "Epoch [2/20], Step [216/704], Loss: 1.1269\n",
      "Epoch [2/20], Step [217/704], Loss: 1.1830\n",
      "Epoch [2/20], Step [218/704], Loss: 1.0392\n",
      "Epoch [2/20], Step [219/704], Loss: 0.8837\n",
      "Epoch [2/20], Step [220/704], Loss: 0.9783\n",
      "Epoch [2/20], Step [221/704], Loss: 1.1973\n",
      "Epoch [2/20], Step [222/704], Loss: 0.9811\n",
      "Epoch [2/20], Step [223/704], Loss: 1.4437\n",
      "Epoch [2/20], Step [224/704], Loss: 0.9606\n",
      "Epoch [2/20], Step [225/704], Loss: 1.2254\n",
      "Epoch [2/20], Step [226/704], Loss: 1.1375\n",
      "Epoch [2/20], Step [227/704], Loss: 0.9891\n",
      "Epoch [2/20], Step [228/704], Loss: 0.9117\n",
      "Epoch [2/20], Step [229/704], Loss: 0.9805\n",
      "Epoch [2/20], Step [230/704], Loss: 0.6718\n",
      "Epoch [2/20], Step [231/704], Loss: 0.8037\n",
      "Epoch [2/20], Step [232/704], Loss: 1.0330\n",
      "Epoch [2/20], Step [233/704], Loss: 1.0406\n",
      "Epoch [2/20], Step [234/704], Loss: 0.8571\n",
      "Epoch [2/20], Step [235/704], Loss: 1.1340\n",
      "Epoch [2/20], Step [236/704], Loss: 1.0847\n",
      "Epoch [2/20], Step [237/704], Loss: 0.9975\n",
      "Epoch [2/20], Step [238/704], Loss: 1.2225\n",
      "Epoch [2/20], Step [239/704], Loss: 0.8771\n",
      "Epoch [2/20], Step [240/704], Loss: 1.0270\n",
      "Epoch [2/20], Step [241/704], Loss: 1.0622\n",
      "Epoch [2/20], Step [242/704], Loss: 1.1148\n",
      "Epoch [2/20], Step [243/704], Loss: 0.9486\n",
      "Epoch [2/20], Step [244/704], Loss: 0.7935\n",
      "Epoch [2/20], Step [245/704], Loss: 1.2903\n",
      "Epoch [2/20], Step [246/704], Loss: 1.0597\n",
      "Epoch [2/20], Step [247/704], Loss: 0.9755\n",
      "Epoch [2/20], Step [248/704], Loss: 0.9062\n",
      "Epoch [2/20], Step [249/704], Loss: 1.0374\n",
      "Epoch [2/20], Step [250/704], Loss: 0.8794\n",
      "Epoch [2/20], Step [251/704], Loss: 0.9010\n",
      "Epoch [2/20], Step [252/704], Loss: 0.8716\n",
      "Epoch [2/20], Step [253/704], Loss: 1.0261\n",
      "Epoch [2/20], Step [254/704], Loss: 1.0336\n",
      "Epoch [2/20], Step [255/704], Loss: 0.7537\n",
      "Epoch [2/20], Step [256/704], Loss: 0.8843\n",
      "Epoch [2/20], Step [257/704], Loss: 1.0740\n",
      "Epoch [2/20], Step [258/704], Loss: 1.0890\n",
      "Epoch [2/20], Step [259/704], Loss: 0.9311\n",
      "Epoch [2/20], Step [260/704], Loss: 1.0686\n",
      "Epoch [2/20], Step [261/704], Loss: 0.9913\n",
      "Epoch [2/20], Step [262/704], Loss: 0.9177\n",
      "Epoch [2/20], Step [263/704], Loss: 0.8746\n",
      "Epoch [2/20], Step [264/704], Loss: 0.9709\n",
      "Epoch [2/20], Step [265/704], Loss: 0.9776\n",
      "Epoch [2/20], Step [266/704], Loss: 0.9695\n",
      "Epoch [2/20], Step [267/704], Loss: 0.9533\n",
      "Epoch [2/20], Step [268/704], Loss: 1.1098\n",
      "Epoch [2/20], Step [269/704], Loss: 1.0094\n",
      "Epoch [2/20], Step [270/704], Loss: 0.8638\n",
      "Epoch [2/20], Step [271/704], Loss: 1.0470\n",
      "Epoch [2/20], Step [272/704], Loss: 0.9386\n",
      "Epoch [2/20], Step [273/704], Loss: 0.8771\n",
      "Epoch [2/20], Step [274/704], Loss: 1.0067\n",
      "Epoch [2/20], Step [275/704], Loss: 0.9957\n",
      "Epoch [2/20], Step [276/704], Loss: 1.1543\n",
      "Epoch [2/20], Step [277/704], Loss: 1.2128\n",
      "Epoch [2/20], Step [278/704], Loss: 1.0255\n",
      "Epoch [2/20], Step [279/704], Loss: 1.0750\n",
      "Epoch [2/20], Step [280/704], Loss: 1.0274\n",
      "Epoch [2/20], Step [281/704], Loss: 1.1007\n",
      "Epoch [2/20], Step [282/704], Loss: 1.0893\n",
      "Epoch [2/20], Step [283/704], Loss: 1.0587\n",
      "Epoch [2/20], Step [284/704], Loss: 0.7019\n",
      "Epoch [2/20], Step [285/704], Loss: 1.3649\n",
      "Epoch [2/20], Step [286/704], Loss: 0.9436\n",
      "Epoch [2/20], Step [287/704], Loss: 0.9419\n",
      "Epoch [2/20], Step [288/704], Loss: 0.9144\n",
      "Epoch [2/20], Step [289/704], Loss: 0.8929\n",
      "Epoch [2/20], Step [290/704], Loss: 0.9376\n",
      "Epoch [2/20], Step [291/704], Loss: 1.2279\n",
      "Epoch [2/20], Step [292/704], Loss: 1.0206\n",
      "Epoch [2/20], Step [293/704], Loss: 0.7822\n",
      "Epoch [2/20], Step [294/704], Loss: 1.1642\n",
      "Epoch [2/20], Step [295/704], Loss: 1.0347\n",
      "Epoch [2/20], Step [296/704], Loss: 0.9939\n",
      "Epoch [2/20], Step [297/704], Loss: 0.9230\n",
      "Epoch [2/20], Step [298/704], Loss: 0.8278\n",
      "Epoch [2/20], Step [299/704], Loss: 0.9206\n",
      "Epoch [2/20], Step [300/704], Loss: 1.0912\n",
      "Epoch [2/20], Step [301/704], Loss: 0.9691\n",
      "Epoch [2/20], Step [302/704], Loss: 0.8410\n",
      "Epoch [2/20], Step [303/704], Loss: 0.9653\n",
      "Epoch [2/20], Step [304/704], Loss: 0.8525\n",
      "Epoch [2/20], Step [305/704], Loss: 0.8827\n",
      "Epoch [2/20], Step [306/704], Loss: 1.0944\n",
      "Epoch [2/20], Step [307/704], Loss: 0.7687\n",
      "Epoch [2/20], Step [308/704], Loss: 0.9690\n",
      "Epoch [2/20], Step [309/704], Loss: 0.9756\n",
      "Epoch [2/20], Step [310/704], Loss: 0.9224\n",
      "Epoch [2/20], Step [311/704], Loss: 1.0393\n",
      "Epoch [2/20], Step [312/704], Loss: 0.9845\n",
      "Epoch [2/20], Step [313/704], Loss: 0.8012\n",
      "Epoch [2/20], Step [314/704], Loss: 1.0470\n",
      "Epoch [2/20], Step [315/704], Loss: 0.6233\n",
      "Epoch [2/20], Step [316/704], Loss: 1.0888\n",
      "Epoch [2/20], Step [317/704], Loss: 1.1864\n",
      "Epoch [2/20], Step [318/704], Loss: 0.8974\n",
      "Epoch [2/20], Step [319/704], Loss: 0.9486\n",
      "Epoch [2/20], Step [320/704], Loss: 0.9703\n",
      "Epoch [2/20], Step [321/704], Loss: 0.7892\n",
      "Epoch [2/20], Step [322/704], Loss: 0.8469\n",
      "Epoch [2/20], Step [323/704], Loss: 0.9056\n",
      "Epoch [2/20], Step [324/704], Loss: 0.7930\n",
      "Epoch [2/20], Step [325/704], Loss: 0.9134\n",
      "Epoch [2/20], Step [326/704], Loss: 1.1781\n",
      "Epoch [2/20], Step [327/704], Loss: 0.9798\n",
      "Epoch [2/20], Step [328/704], Loss: 1.1249\n",
      "Epoch [2/20], Step [329/704], Loss: 0.7585\n",
      "Epoch [2/20], Step [330/704], Loss: 1.0819\n",
      "Epoch [2/20], Step [331/704], Loss: 0.9392\n",
      "Epoch [2/20], Step [332/704], Loss: 0.7879\n",
      "Epoch [2/20], Step [333/704], Loss: 0.8426\n",
      "Epoch [2/20], Step [334/704], Loss: 0.9001\n",
      "Epoch [2/20], Step [335/704], Loss: 0.8468\n",
      "Epoch [2/20], Step [336/704], Loss: 0.9869\n",
      "Epoch [2/20], Step [337/704], Loss: 0.8787\n",
      "Epoch [2/20], Step [338/704], Loss: 0.9021\n",
      "Epoch [2/20], Step [339/704], Loss: 0.9861\n",
      "Epoch [2/20], Step [340/704], Loss: 0.8052\n",
      "Epoch [2/20], Step [341/704], Loss: 0.9640\n",
      "Epoch [2/20], Step [342/704], Loss: 0.7529\n",
      "Epoch [2/20], Step [343/704], Loss: 0.7562\n",
      "Epoch [2/20], Step [344/704], Loss: 0.6817\n",
      "Epoch [2/20], Step [345/704], Loss: 0.8333\n",
      "Epoch [2/20], Step [346/704], Loss: 1.1031\n",
      "Epoch [2/20], Step [347/704], Loss: 0.9102\n",
      "Epoch [2/20], Step [348/704], Loss: 0.9086\n",
      "Epoch [2/20], Step [349/704], Loss: 1.0287\n",
      "Epoch [2/20], Step [350/704], Loss: 1.1162\n",
      "Epoch [2/20], Step [351/704], Loss: 1.0550\n",
      "Epoch [2/20], Step [352/704], Loss: 0.9722\n",
      "Epoch [2/20], Step [353/704], Loss: 1.2283\n",
      "Epoch [2/20], Step [354/704], Loss: 0.9941\n",
      "Epoch [2/20], Step [355/704], Loss: 1.0429\n",
      "Epoch [2/20], Step [356/704], Loss: 1.0850\n",
      "Epoch [2/20], Step [357/704], Loss: 0.9923\n",
      "Epoch [2/20], Step [358/704], Loss: 0.8409\n",
      "Epoch [2/20], Step [359/704], Loss: 1.0658\n",
      "Epoch [2/20], Step [360/704], Loss: 1.1280\n",
      "Epoch [2/20], Step [361/704], Loss: 0.8198\n",
      "Epoch [2/20], Step [362/704], Loss: 0.6973\n",
      "Epoch [2/20], Step [363/704], Loss: 0.9680\n",
      "Epoch [2/20], Step [364/704], Loss: 0.9332\n",
      "Epoch [2/20], Step [365/704], Loss: 1.0254\n",
      "Epoch [2/20], Step [366/704], Loss: 0.7854\n",
      "Epoch [2/20], Step [367/704], Loss: 1.0127\n",
      "Epoch [2/20], Step [368/704], Loss: 0.8508\n",
      "Epoch [2/20], Step [369/704], Loss: 0.9406\n",
      "Epoch [2/20], Step [370/704], Loss: 0.8771\n",
      "Epoch [2/20], Step [371/704], Loss: 1.0356\n",
      "Epoch [2/20], Step [372/704], Loss: 0.8656\n",
      "Epoch [2/20], Step [373/704], Loss: 1.0244\n",
      "Epoch [2/20], Step [374/704], Loss: 0.7901\n",
      "Epoch [2/20], Step [375/704], Loss: 0.6945\n",
      "Epoch [2/20], Step [376/704], Loss: 0.9429\n",
      "Epoch [2/20], Step [377/704], Loss: 0.7289\n",
      "Epoch [2/20], Step [378/704], Loss: 1.0297\n",
      "Epoch [2/20], Step [379/704], Loss: 0.9602\n",
      "Epoch [2/20], Step [380/704], Loss: 0.7451\n",
      "Epoch [2/20], Step [381/704], Loss: 0.8822\n",
      "Epoch [2/20], Step [382/704], Loss: 1.0211\n",
      "Epoch [2/20], Step [383/704], Loss: 1.2203\n",
      "Epoch [2/20], Step [384/704], Loss: 1.1973\n",
      "Epoch [2/20], Step [385/704], Loss: 0.8218\n",
      "Epoch [2/20], Step [386/704], Loss: 0.9769\n",
      "Epoch [2/20], Step [387/704], Loss: 1.0331\n",
      "Epoch [2/20], Step [388/704], Loss: 0.9902\n",
      "Epoch [2/20], Step [389/704], Loss: 1.0241\n",
      "Epoch [2/20], Step [390/704], Loss: 1.0261\n",
      "Epoch [2/20], Step [391/704], Loss: 0.7022\n",
      "Epoch [2/20], Step [392/704], Loss: 1.0217\n",
      "Epoch [2/20], Step [393/704], Loss: 0.9593\n",
      "Epoch [2/20], Step [394/704], Loss: 0.8737\n",
      "Epoch [2/20], Step [395/704], Loss: 1.1196\n",
      "Epoch [2/20], Step [396/704], Loss: 0.7929\n",
      "Epoch [2/20], Step [397/704], Loss: 0.8014\n",
      "Epoch [2/20], Step [398/704], Loss: 0.8928\n",
      "Epoch [2/20], Step [399/704], Loss: 0.6661\n",
      "Epoch [2/20], Step [400/704], Loss: 0.7111\n",
      "Epoch [2/20], Step [401/704], Loss: 1.0736\n",
      "Epoch [2/20], Step [402/704], Loss: 0.8698\n",
      "Epoch [2/20], Step [403/704], Loss: 0.8604\n",
      "Epoch [2/20], Step [404/704], Loss: 1.5660\n",
      "Epoch [2/20], Step [405/704], Loss: 0.8261\n",
      "Epoch [2/20], Step [406/704], Loss: 0.7350\n",
      "Epoch [2/20], Step [407/704], Loss: 0.9672\n",
      "Epoch [2/20], Step [408/704], Loss: 1.0447\n",
      "Epoch [2/20], Step [409/704], Loss: 0.7609\n",
      "Epoch [2/20], Step [410/704], Loss: 0.9603\n",
      "Epoch [2/20], Step [411/704], Loss: 0.8640\n",
      "Epoch [2/20], Step [412/704], Loss: 0.6548\n",
      "Epoch [2/20], Step [413/704], Loss: 0.5984\n",
      "Epoch [2/20], Step [414/704], Loss: 1.3138\n",
      "Epoch [2/20], Step [415/704], Loss: 0.9570\n",
      "Epoch [2/20], Step [416/704], Loss: 1.0539\n",
      "Epoch [2/20], Step [417/704], Loss: 1.0262\n",
      "Epoch [2/20], Step [418/704], Loss: 0.8776\n",
      "Epoch [2/20], Step [419/704], Loss: 0.6719\n",
      "Epoch [2/20], Step [420/704], Loss: 1.1434\n",
      "Epoch [2/20], Step [421/704], Loss: 0.9476\n",
      "Epoch [2/20], Step [422/704], Loss: 0.8053\n",
      "Epoch [2/20], Step [423/704], Loss: 0.9226\n",
      "Epoch [2/20], Step [424/704], Loss: 1.2921\n",
      "Epoch [2/20], Step [425/704], Loss: 1.0844\n",
      "Epoch [2/20], Step [426/704], Loss: 1.0203\n",
      "Epoch [2/20], Step [427/704], Loss: 0.9594\n",
      "Epoch [2/20], Step [428/704], Loss: 0.6910\n",
      "Epoch [2/20], Step [429/704], Loss: 0.8553\n",
      "Epoch [2/20], Step [430/704], Loss: 0.8276\n",
      "Epoch [2/20], Step [431/704], Loss: 0.8399\n",
      "Epoch [2/20], Step [432/704], Loss: 1.1874\n",
      "Epoch [2/20], Step [433/704], Loss: 0.9563\n",
      "Epoch [2/20], Step [434/704], Loss: 0.8845\n",
      "Epoch [2/20], Step [435/704], Loss: 0.9088\n",
      "Epoch [2/20], Step [436/704], Loss: 1.0680\n",
      "Epoch [2/20], Step [437/704], Loss: 0.7645\n",
      "Epoch [2/20], Step [438/704], Loss: 0.7911\n",
      "Epoch [2/20], Step [439/704], Loss: 1.1462\n",
      "Epoch [2/20], Step [440/704], Loss: 0.7987\n",
      "Epoch [2/20], Step [441/704], Loss: 1.0002\n",
      "Epoch [2/20], Step [442/704], Loss: 0.7774\n",
      "Epoch [2/20], Step [443/704], Loss: 1.1777\n",
      "Epoch [2/20], Step [444/704], Loss: 0.9897\n",
      "Epoch [2/20], Step [445/704], Loss: 0.9072\n",
      "Epoch [2/20], Step [446/704], Loss: 0.8845\n",
      "Epoch [2/20], Step [447/704], Loss: 0.8581\n",
      "Epoch [2/20], Step [448/704], Loss: 0.8461\n",
      "Epoch [2/20], Step [449/704], Loss: 1.3030\n",
      "Epoch [2/20], Step [450/704], Loss: 0.9185\n",
      "Epoch [2/20], Step [451/704], Loss: 0.8843\n",
      "Epoch [2/20], Step [452/704], Loss: 0.9007\n",
      "Epoch [2/20], Step [453/704], Loss: 1.2092\n",
      "Epoch [2/20], Step [454/704], Loss: 0.7187\n",
      "Epoch [2/20], Step [455/704], Loss: 0.9233\n",
      "Epoch [2/20], Step [456/704], Loss: 0.9230\n",
      "Epoch [2/20], Step [457/704], Loss: 0.9814\n",
      "Epoch [2/20], Step [458/704], Loss: 0.7404\n",
      "Epoch [2/20], Step [459/704], Loss: 0.8653\n",
      "Epoch [2/20], Step [460/704], Loss: 1.0545\n",
      "Epoch [2/20], Step [461/704], Loss: 0.7750\n",
      "Epoch [2/20], Step [462/704], Loss: 0.8640\n",
      "Epoch [2/20], Step [463/704], Loss: 0.9290\n",
      "Epoch [2/20], Step [464/704], Loss: 1.1109\n",
      "Epoch [2/20], Step [465/704], Loss: 0.9167\n",
      "Epoch [2/20], Step [466/704], Loss: 0.8284\n",
      "Epoch [2/20], Step [467/704], Loss: 0.9423\n",
      "Epoch [2/20], Step [468/704], Loss: 0.9275\n",
      "Epoch [2/20], Step [469/704], Loss: 0.6708\n",
      "Epoch [2/20], Step [470/704], Loss: 0.8266\n",
      "Epoch [2/20], Step [471/704], Loss: 0.7358\n",
      "Epoch [2/20], Step [472/704], Loss: 0.7216\n",
      "Epoch [2/20], Step [473/704], Loss: 0.9165\n",
      "Epoch [2/20], Step [474/704], Loss: 0.5968\n",
      "Epoch [2/20], Step [475/704], Loss: 0.6367\n",
      "Epoch [2/20], Step [476/704], Loss: 1.0130\n",
      "Epoch [2/20], Step [477/704], Loss: 0.7119\n",
      "Epoch [2/20], Step [478/704], Loss: 0.8391\n",
      "Epoch [2/20], Step [479/704], Loss: 0.8014\n",
      "Epoch [2/20], Step [480/704], Loss: 0.7857\n",
      "Epoch [2/20], Step [481/704], Loss: 0.9094\n",
      "Epoch [2/20], Step [482/704], Loss: 0.9748\n",
      "Epoch [2/20], Step [483/704], Loss: 0.5814\n",
      "Epoch [2/20], Step [484/704], Loss: 1.2037\n",
      "Epoch [2/20], Step [485/704], Loss: 0.9622\n",
      "Epoch [2/20], Step [486/704], Loss: 0.9561\n",
      "Epoch [2/20], Step [487/704], Loss: 1.0528\n",
      "Epoch [2/20], Step [488/704], Loss: 0.9326\n",
      "Epoch [2/20], Step [489/704], Loss: 0.7795\n",
      "Epoch [2/20], Step [490/704], Loss: 0.7101\n",
      "Epoch [2/20], Step [491/704], Loss: 0.8467\n",
      "Epoch [2/20], Step [492/704], Loss: 0.8973\n",
      "Epoch [2/20], Step [493/704], Loss: 0.7107\n",
      "Epoch [2/20], Step [494/704], Loss: 0.9031\n",
      "Epoch [2/20], Step [495/704], Loss: 0.9069\n",
      "Epoch [2/20], Step [496/704], Loss: 0.8303\n",
      "Epoch [2/20], Step [497/704], Loss: 0.6717\n",
      "Epoch [2/20], Step [498/704], Loss: 0.8349\n",
      "Epoch [2/20], Step [499/704], Loss: 0.7956\n",
      "Epoch [2/20], Step [500/704], Loss: 0.8400\n",
      "Epoch [2/20], Step [501/704], Loss: 0.9009\n",
      "Epoch [2/20], Step [502/704], Loss: 1.0273\n",
      "Epoch [2/20], Step [503/704], Loss: 0.9780\n",
      "Epoch [2/20], Step [504/704], Loss: 0.9289\n",
      "Epoch [2/20], Step [505/704], Loss: 0.9088\n",
      "Epoch [2/20], Step [506/704], Loss: 0.6151\n",
      "Epoch [2/20], Step [507/704], Loss: 0.5544\n",
      "Epoch [2/20], Step [508/704], Loss: 1.1202\n",
      "Epoch [2/20], Step [509/704], Loss: 0.9553\n",
      "Epoch [2/20], Step [510/704], Loss: 0.9940\n",
      "Epoch [2/20], Step [511/704], Loss: 1.2286\n",
      "Epoch [2/20], Step [512/704], Loss: 0.9155\n",
      "Epoch [2/20], Step [513/704], Loss: 0.8453\n",
      "Epoch [2/20], Step [514/704], Loss: 0.9822\n",
      "Epoch [2/20], Step [515/704], Loss: 0.9486\n",
      "Epoch [2/20], Step [516/704], Loss: 0.9388\n",
      "Epoch [2/20], Step [517/704], Loss: 0.8831\n",
      "Epoch [2/20], Step [518/704], Loss: 0.7324\n",
      "Epoch [2/20], Step [519/704], Loss: 0.6487\n",
      "Epoch [2/20], Step [520/704], Loss: 1.1174\n",
      "Epoch [2/20], Step [521/704], Loss: 0.9312\n",
      "Epoch [2/20], Step [522/704], Loss: 1.1143\n",
      "Epoch [2/20], Step [523/704], Loss: 0.9775\n",
      "Epoch [2/20], Step [524/704], Loss: 0.9870\n",
      "Epoch [2/20], Step [525/704], Loss: 0.9128\n",
      "Epoch [2/20], Step [526/704], Loss: 0.7470\n",
      "Epoch [2/20], Step [527/704], Loss: 0.9220\n",
      "Epoch [2/20], Step [528/704], Loss: 0.9041\n",
      "Epoch [2/20], Step [529/704], Loss: 0.6817\n",
      "Epoch [2/20], Step [530/704], Loss: 0.9302\n",
      "Epoch [2/20], Step [531/704], Loss: 1.1045\n",
      "Epoch [2/20], Step [532/704], Loss: 0.9346\n",
      "Epoch [2/20], Step [533/704], Loss: 1.0441\n",
      "Epoch [2/20], Step [534/704], Loss: 0.7372\n",
      "Epoch [2/20], Step [535/704], Loss: 0.6443\n",
      "Epoch [2/20], Step [536/704], Loss: 0.7321\n",
      "Epoch [2/20], Step [537/704], Loss: 0.9412\n",
      "Epoch [2/20], Step [538/704], Loss: 0.8617\n",
      "Epoch [2/20], Step [539/704], Loss: 0.8694\n",
      "Epoch [2/20], Step [540/704], Loss: 0.7809\n",
      "Epoch [2/20], Step [541/704], Loss: 0.8157\n",
      "Epoch [2/20], Step [542/704], Loss: 0.9152\n",
      "Epoch [2/20], Step [543/704], Loss: 1.1827\n",
      "Epoch [2/20], Step [544/704], Loss: 0.9986\n",
      "Epoch [2/20], Step [545/704], Loss: 0.8254\n",
      "Epoch [2/20], Step [546/704], Loss: 0.8862\n",
      "Epoch [2/20], Step [547/704], Loss: 0.7937\n",
      "Epoch [2/20], Step [548/704], Loss: 0.9097\n",
      "Epoch [2/20], Step [549/704], Loss: 0.7360\n",
      "Epoch [2/20], Step [550/704], Loss: 0.8305\n",
      "Epoch [2/20], Step [551/704], Loss: 0.9328\n",
      "Epoch [2/20], Step [552/704], Loss: 0.7716\n",
      "Epoch [2/20], Step [553/704], Loss: 0.8342\n",
      "Epoch [2/20], Step [554/704], Loss: 0.8620\n",
      "Epoch [2/20], Step [555/704], Loss: 1.1041\n",
      "Epoch [2/20], Step [556/704], Loss: 0.9556\n",
      "Epoch [2/20], Step [557/704], Loss: 0.8813\n",
      "Epoch [2/20], Step [558/704], Loss: 0.8354\n",
      "Epoch [2/20], Step [559/704], Loss: 0.7740\n",
      "Epoch [2/20], Step [560/704], Loss: 0.9797\n",
      "Epoch [2/20], Step [561/704], Loss: 0.9554\n",
      "Epoch [2/20], Step [562/704], Loss: 0.7523\n",
      "Epoch [2/20], Step [563/704], Loss: 0.9889\n",
      "Epoch [2/20], Step [564/704], Loss: 0.7765\n",
      "Epoch [2/20], Step [565/704], Loss: 0.8434\n",
      "Epoch [2/20], Step [566/704], Loss: 0.7161\n",
      "Epoch [2/20], Step [567/704], Loss: 0.6833\n",
      "Epoch [2/20], Step [568/704], Loss: 0.7384\n",
      "Epoch [2/20], Step [569/704], Loss: 0.8786\n",
      "Epoch [2/20], Step [570/704], Loss: 1.1958\n",
      "Epoch [2/20], Step [571/704], Loss: 1.0118\n",
      "Epoch [2/20], Step [572/704], Loss: 1.2600\n",
      "Epoch [2/20], Step [573/704], Loss: 1.0211\n",
      "Epoch [2/20], Step [574/704], Loss: 1.0543\n",
      "Epoch [2/20], Step [575/704], Loss: 0.9963\n",
      "Epoch [2/20], Step [576/704], Loss: 1.1309\n",
      "Epoch [2/20], Step [577/704], Loss: 0.9777\n",
      "Epoch [2/20], Step [578/704], Loss: 1.0587\n",
      "Epoch [2/20], Step [579/704], Loss: 0.6735\n",
      "Epoch [2/20], Step [580/704], Loss: 1.1054\n",
      "Epoch [2/20], Step [581/704], Loss: 0.6841\n",
      "Epoch [2/20], Step [582/704], Loss: 0.8173\n",
      "Epoch [2/20], Step [583/704], Loss: 0.6664\n",
      "Epoch [2/20], Step [584/704], Loss: 0.5545\n",
      "Epoch [2/20], Step [585/704], Loss: 0.8067\n",
      "Epoch [2/20], Step [586/704], Loss: 1.0361\n",
      "Epoch [2/20], Step [587/704], Loss: 0.8300\n",
      "Epoch [2/20], Step [588/704], Loss: 0.9451\n",
      "Epoch [2/20], Step [589/704], Loss: 0.9626\n",
      "Epoch [2/20], Step [590/704], Loss: 0.8895\n",
      "Epoch [2/20], Step [591/704], Loss: 0.9696\n",
      "Epoch [2/20], Step [592/704], Loss: 0.8012\n",
      "Epoch [2/20], Step [593/704], Loss: 0.6678\n",
      "Epoch [2/20], Step [594/704], Loss: 0.9294\n",
      "Epoch [2/20], Step [595/704], Loss: 0.9170\n",
      "Epoch [2/20], Step [596/704], Loss: 0.8983\n",
      "Epoch [2/20], Step [597/704], Loss: 0.9169\n",
      "Epoch [2/20], Step [598/704], Loss: 1.1647\n",
      "Epoch [2/20], Step [599/704], Loss: 0.9680\n",
      "Epoch [2/20], Step [600/704], Loss: 0.9946\n",
      "Epoch [2/20], Step [601/704], Loss: 1.0419\n",
      "Epoch [2/20], Step [602/704], Loss: 0.6722\n",
      "Epoch [2/20], Step [603/704], Loss: 1.0022\n",
      "Epoch [2/20], Step [604/704], Loss: 0.7315\n",
      "Epoch [2/20], Step [605/704], Loss: 0.6212\n",
      "Epoch [2/20], Step [606/704], Loss: 1.2733\n",
      "Epoch [2/20], Step [607/704], Loss: 0.7934\n",
      "Epoch [2/20], Step [608/704], Loss: 0.6660\n",
      "Epoch [2/20], Step [609/704], Loss: 0.6858\n",
      "Epoch [2/20], Step [610/704], Loss: 0.7827\n",
      "Epoch [2/20], Step [611/704], Loss: 0.7234\n",
      "Epoch [2/20], Step [612/704], Loss: 1.0491\n",
      "Epoch [2/20], Step [613/704], Loss: 0.9036\n",
      "Epoch [2/20], Step [614/704], Loss: 0.6185\n",
      "Epoch [2/20], Step [615/704], Loss: 0.9103\n",
      "Epoch [2/20], Step [616/704], Loss: 0.8545\n",
      "Epoch [2/20], Step [617/704], Loss: 1.1013\n",
      "Epoch [2/20], Step [618/704], Loss: 0.5580\n",
      "Epoch [2/20], Step [619/704], Loss: 1.1520\n",
      "Epoch [2/20], Step [620/704], Loss: 0.8414\n",
      "Epoch [2/20], Step [621/704], Loss: 0.7519\n",
      "Epoch [2/20], Step [622/704], Loss: 1.3088\n",
      "Epoch [2/20], Step [623/704], Loss: 1.0929\n",
      "Epoch [2/20], Step [624/704], Loss: 0.7706\n",
      "Epoch [2/20], Step [625/704], Loss: 0.8022\n",
      "Epoch [2/20], Step [626/704], Loss: 1.0892\n",
      "Epoch [2/20], Step [627/704], Loss: 0.6388\n",
      "Epoch [2/20], Step [628/704], Loss: 0.9453\n",
      "Epoch [2/20], Step [629/704], Loss: 1.0094\n",
      "Epoch [2/20], Step [630/704], Loss: 0.8121\n",
      "Epoch [2/20], Step [631/704], Loss: 0.7319\n",
      "Epoch [2/20], Step [632/704], Loss: 0.7191\n",
      "Epoch [2/20], Step [633/704], Loss: 1.3414\n",
      "Epoch [2/20], Step [634/704], Loss: 0.6557\n",
      "Epoch [2/20], Step [635/704], Loss: 0.7868\n",
      "Epoch [2/20], Step [636/704], Loss: 0.5191\n",
      "Epoch [2/20], Step [637/704], Loss: 0.7869\n",
      "Epoch [2/20], Step [638/704], Loss: 0.9977\n",
      "Epoch [2/20], Step [639/704], Loss: 0.7688\n",
      "Epoch [2/20], Step [640/704], Loss: 0.8134\n",
      "Epoch [2/20], Step [641/704], Loss: 0.8612\n",
      "Epoch [2/20], Step [642/704], Loss: 0.7912\n",
      "Epoch [2/20], Step [643/704], Loss: 0.9535\n",
      "Epoch [2/20], Step [644/704], Loss: 1.0369\n",
      "Epoch [2/20], Step [645/704], Loss: 0.7953\n",
      "Epoch [2/20], Step [646/704], Loss: 0.9480\n",
      "Epoch [2/20], Step [647/704], Loss: 0.7074\n",
      "Epoch [2/20], Step [648/704], Loss: 0.7141\n",
      "Epoch [2/20], Step [649/704], Loss: 0.6522\n",
      "Epoch [2/20], Step [650/704], Loss: 0.7663\n",
      "Epoch [2/20], Step [651/704], Loss: 0.6916\n",
      "Epoch [2/20], Step [652/704], Loss: 0.7672\n",
      "Epoch [2/20], Step [653/704], Loss: 0.5928\n",
      "Epoch [2/20], Step [654/704], Loss: 0.7735\n",
      "Epoch [2/20], Step [655/704], Loss: 0.9528\n",
      "Epoch [2/20], Step [656/704], Loss: 0.6746\n",
      "Epoch [2/20], Step [657/704], Loss: 0.8843\n",
      "Epoch [2/20], Step [658/704], Loss: 1.1887\n",
      "Epoch [2/20], Step [659/704], Loss: 0.6781\n",
      "Epoch [2/20], Step [660/704], Loss: 0.7931\n",
      "Epoch [2/20], Step [661/704], Loss: 0.8057\n",
      "Epoch [2/20], Step [662/704], Loss: 0.7694\n",
      "Epoch [2/20], Step [663/704], Loss: 0.6406\n",
      "Epoch [2/20], Step [664/704], Loss: 0.9455\n",
      "Epoch [2/20], Step [665/704], Loss: 0.7532\n",
      "Epoch [2/20], Step [666/704], Loss: 0.7407\n",
      "Epoch [2/20], Step [667/704], Loss: 0.8497\n",
      "Epoch [2/20], Step [668/704], Loss: 0.6839\n",
      "Epoch [2/20], Step [669/704], Loss: 0.8603\n",
      "Epoch [2/20], Step [670/704], Loss: 0.6579\n",
      "Epoch [2/20], Step [671/704], Loss: 0.4972\n",
      "Epoch [2/20], Step [672/704], Loss: 0.8264\n",
      "Epoch [2/20], Step [673/704], Loss: 0.9884\n",
      "Epoch [2/20], Step [674/704], Loss: 0.8105\n",
      "Epoch [2/20], Step [675/704], Loss: 0.7268\n",
      "Epoch [2/20], Step [676/704], Loss: 0.7514\n",
      "Epoch [2/20], Step [677/704], Loss: 0.8388\n",
      "Epoch [2/20], Step [678/704], Loss: 0.6763\n",
      "Epoch [2/20], Step [679/704], Loss: 0.7691\n",
      "Epoch [2/20], Step [680/704], Loss: 0.6819\n",
      "Epoch [2/20], Step [681/704], Loss: 0.6151\n",
      "Epoch [2/20], Step [682/704], Loss: 0.9346\n",
      "Epoch [2/20], Step [683/704], Loss: 0.5756\n",
      "Epoch [2/20], Step [684/704], Loss: 1.0155\n",
      "Epoch [2/20], Step [685/704], Loss: 0.7115\n",
      "Epoch [2/20], Step [686/704], Loss: 0.8187\n",
      "Epoch [2/20], Step [687/704], Loss: 0.9613\n",
      "Epoch [2/20], Step [688/704], Loss: 0.8674\n",
      "Epoch [2/20], Step [689/704], Loss: 0.6219\n",
      "Epoch [2/20], Step [690/704], Loss: 0.7358\n",
      "Epoch [2/20], Step [691/704], Loss: 0.7902\n",
      "Epoch [2/20], Step [692/704], Loss: 0.8242\n",
      "Epoch [2/20], Step [693/704], Loss: 1.3497\n",
      "Epoch [2/20], Step [694/704], Loss: 0.7875\n",
      "Epoch [2/20], Step [695/704], Loss: 0.6558\n",
      "Epoch [2/20], Step [696/704], Loss: 0.8110\n",
      "Epoch [2/20], Step [697/704], Loss: 0.7642\n",
      "Epoch [2/20], Step [698/704], Loss: 0.7290\n",
      "Epoch [2/20], Step [699/704], Loss: 0.7810\n",
      "Epoch [2/20], Step [700/704], Loss: 0.8911\n",
      "Epoch [2/20], Step [701/704], Loss: 0.8248\n",
      "Epoch [2/20], Step [702/704], Loss: 0.8589\n",
      "Epoch [2/20], Step [703/704], Loss: 0.8107\n",
      "Epoch [2/20], Step [704/704], Loss: 1.5759\n",
      "Epoch [2/20], Loss: 1.5759\n",
      "Epoch [3/20], Step [1/704], Loss: 0.9658\n",
      "Epoch [3/20], Step [2/704], Loss: 0.9364\n",
      "Epoch [3/20], Step [3/704], Loss: 0.8033\n",
      "Epoch [3/20], Step [4/704], Loss: 0.8086\n",
      "Epoch [3/20], Step [5/704], Loss: 0.8482\n",
      "Epoch [3/20], Step [6/704], Loss: 0.7745\n",
      "Epoch [3/20], Step [7/704], Loss: 1.0820\n",
      "Epoch [3/20], Step [8/704], Loss: 0.6981\n",
      "Epoch [3/20], Step [9/704], Loss: 0.7493\n",
      "Epoch [3/20], Step [10/704], Loss: 0.8527\n",
      "Epoch [3/20], Step [11/704], Loss: 0.5844\n",
      "Epoch [3/20], Step [12/704], Loss: 0.7587\n",
      "Epoch [3/20], Step [13/704], Loss: 0.5926\n",
      "Epoch [3/20], Step [14/704], Loss: 0.6828\n",
      "Epoch [3/20], Step [15/704], Loss: 0.8443\n",
      "Epoch [3/20], Step [16/704], Loss: 1.0763\n",
      "Epoch [3/20], Step [17/704], Loss: 0.7733\n",
      "Epoch [3/20], Step [18/704], Loss: 0.9518\n",
      "Epoch [3/20], Step [19/704], Loss: 0.8310\n",
      "Epoch [3/20], Step [20/704], Loss: 0.6597\n",
      "Epoch [3/20], Step [21/704], Loss: 0.7015\n",
      "Epoch [3/20], Step [22/704], Loss: 0.8503\n",
      "Epoch [3/20], Step [23/704], Loss: 0.8326\n",
      "Epoch [3/20], Step [24/704], Loss: 0.5676\n",
      "Epoch [3/20], Step [25/704], Loss: 1.0099\n",
      "Epoch [3/20], Step [26/704], Loss: 0.7636\n",
      "Epoch [3/20], Step [27/704], Loss: 0.7174\n",
      "Epoch [3/20], Step [28/704], Loss: 0.8510\n",
      "Epoch [3/20], Step [29/704], Loss: 0.8547\n",
      "Epoch [3/20], Step [30/704], Loss: 0.8807\n",
      "Epoch [3/20], Step [31/704], Loss: 0.8254\n",
      "Epoch [3/20], Step [32/704], Loss: 0.8548\n",
      "Epoch [3/20], Step [33/704], Loss: 0.6008\n",
      "Epoch [3/20], Step [34/704], Loss: 0.6673\n",
      "Epoch [3/20], Step [35/704], Loss: 1.0427\n",
      "Epoch [3/20], Step [36/704], Loss: 0.9565\n",
      "Epoch [3/20], Step [37/704], Loss: 1.0002\n",
      "Epoch [3/20], Step [38/704], Loss: 0.7340\n",
      "Epoch [3/20], Step [39/704], Loss: 0.8584\n",
      "Epoch [3/20], Step [40/704], Loss: 0.4929\n",
      "Epoch [3/20], Step [41/704], Loss: 0.8643\n",
      "Epoch [3/20], Step [42/704], Loss: 0.5549\n",
      "Epoch [3/20], Step [43/704], Loss: 0.6209\n",
      "Epoch [3/20], Step [44/704], Loss: 0.9886\n",
      "Epoch [3/20], Step [45/704], Loss: 0.8281\n",
      "Epoch [3/20], Step [46/704], Loss: 0.6936\n",
      "Epoch [3/20], Step [47/704], Loss: 0.7833\n",
      "Epoch [3/20], Step [48/704], Loss: 0.6895\n",
      "Epoch [3/20], Step [49/704], Loss: 0.7277\n",
      "Epoch [3/20], Step [50/704], Loss: 0.8502\n",
      "Epoch [3/20], Step [51/704], Loss: 0.8136\n",
      "Epoch [3/20], Step [52/704], Loss: 0.8794\n",
      "Epoch [3/20], Step [53/704], Loss: 0.7745\n",
      "Epoch [3/20], Step [54/704], Loss: 0.8014\n",
      "Epoch [3/20], Step [55/704], Loss: 1.0351\n",
      "Epoch [3/20], Step [56/704], Loss: 0.8460\n",
      "Epoch [3/20], Step [57/704], Loss: 0.8130\n",
      "Epoch [3/20], Step [58/704], Loss: 0.7193\n",
      "Epoch [3/20], Step [59/704], Loss: 0.7957\n",
      "Epoch [3/20], Step [60/704], Loss: 0.5234\n",
      "Epoch [3/20], Step [61/704], Loss: 0.5538\n",
      "Epoch [3/20], Step [62/704], Loss: 0.9742\n",
      "Epoch [3/20], Step [63/704], Loss: 1.0141\n",
      "Epoch [3/20], Step [64/704], Loss: 0.6814\n",
      "Epoch [3/20], Step [65/704], Loss: 0.6612\n",
      "Epoch [3/20], Step [66/704], Loss: 0.6504\n",
      "Epoch [3/20], Step [67/704], Loss: 0.9191\n",
      "Epoch [3/20], Step [68/704], Loss: 0.7395\n",
      "Epoch [3/20], Step [69/704], Loss: 0.9092\n",
      "Epoch [3/20], Step [70/704], Loss: 0.5816\n",
      "Epoch [3/20], Step [71/704], Loss: 0.9216\n",
      "Epoch [3/20], Step [72/704], Loss: 0.7160\n",
      "Epoch [3/20], Step [73/704], Loss: 0.7867\n",
      "Epoch [3/20], Step [74/704], Loss: 0.7087\n",
      "Epoch [3/20], Step [75/704], Loss: 0.6372\n",
      "Epoch [3/20], Step [76/704], Loss: 0.7939\n",
      "Epoch [3/20], Step [77/704], Loss: 0.5558\n",
      "Epoch [3/20], Step [78/704], Loss: 0.7474\n",
      "Epoch [3/20], Step [79/704], Loss: 0.9706\n",
      "Epoch [3/20], Step [80/704], Loss: 0.9110\n",
      "Epoch [3/20], Step [81/704], Loss: 0.4912\n",
      "Epoch [3/20], Step [82/704], Loss: 0.6482\n",
      "Epoch [3/20], Step [83/704], Loss: 0.6599\n",
      "Epoch [3/20], Step [84/704], Loss: 0.6286\n",
      "Epoch [3/20], Step [85/704], Loss: 0.8319\n",
      "Epoch [3/20], Step [86/704], Loss: 0.6970\n",
      "Epoch [3/20], Step [87/704], Loss: 0.7502\n",
      "Epoch [3/20], Step [88/704], Loss: 0.6006\n",
      "Epoch [3/20], Step [89/704], Loss: 0.5500\n",
      "Epoch [3/20], Step [90/704], Loss: 0.8165\n",
      "Epoch [3/20], Step [91/704], Loss: 0.8614\n",
      "Epoch [3/20], Step [92/704], Loss: 0.7010\n",
      "Epoch [3/20], Step [93/704], Loss: 0.9214\n",
      "Epoch [3/20], Step [94/704], Loss: 0.8961\n",
      "Epoch [3/20], Step [95/704], Loss: 0.6277\n",
      "Epoch [3/20], Step [96/704], Loss: 0.5627\n",
      "Epoch [3/20], Step [97/704], Loss: 0.8138\n",
      "Epoch [3/20], Step [98/704], Loss: 0.7233\n",
      "Epoch [3/20], Step [99/704], Loss: 0.6435\n",
      "Epoch [3/20], Step [100/704], Loss: 0.8881\n",
      "Epoch [3/20], Step [101/704], Loss: 0.6675\n",
      "Epoch [3/20], Step [102/704], Loss: 0.5700\n",
      "Epoch [3/20], Step [103/704], Loss: 0.5570\n",
      "Epoch [3/20], Step [104/704], Loss: 0.7789\n",
      "Epoch [3/20], Step [105/704], Loss: 0.6165\n",
      "Epoch [3/20], Step [106/704], Loss: 0.8276\n",
      "Epoch [3/20], Step [107/704], Loss: 0.9549\n",
      "Epoch [3/20], Step [108/704], Loss: 0.6859\n",
      "Epoch [3/20], Step [109/704], Loss: 0.5440\n",
      "Epoch [3/20], Step [110/704], Loss: 0.8658\n",
      "Epoch [3/20], Step [111/704], Loss: 0.7704\n",
      "Epoch [3/20], Step [112/704], Loss: 0.5431\n",
      "Epoch [3/20], Step [113/704], Loss: 0.6070\n",
      "Epoch [3/20], Step [114/704], Loss: 0.7629\n",
      "Epoch [3/20], Step [115/704], Loss: 0.6740\n",
      "Epoch [3/20], Step [116/704], Loss: 0.6034\n",
      "Epoch [3/20], Step [117/704], Loss: 0.8065\n",
      "Epoch [3/20], Step [118/704], Loss: 0.7857\n",
      "Epoch [3/20], Step [119/704], Loss: 0.7860\n",
      "Epoch [3/20], Step [120/704], Loss: 0.5753\n",
      "Epoch [3/20], Step [121/704], Loss: 0.6675\n",
      "Epoch [3/20], Step [122/704], Loss: 0.6238\n",
      "Epoch [3/20], Step [123/704], Loss: 0.7462\n",
      "Epoch [3/20], Step [124/704], Loss: 0.7317\n",
      "Epoch [3/20], Step [125/704], Loss: 0.7642\n",
      "Epoch [3/20], Step [126/704], Loss: 0.6829\n",
      "Epoch [3/20], Step [127/704], Loss: 0.7815\n",
      "Epoch [3/20], Step [128/704], Loss: 0.7572\n",
      "Epoch [3/20], Step [129/704], Loss: 0.5535\n",
      "Epoch [3/20], Step [130/704], Loss: 0.7827\n",
      "Epoch [3/20], Step [131/704], Loss: 0.7183\n",
      "Epoch [3/20], Step [132/704], Loss: 0.6603\n",
      "Epoch [3/20], Step [133/704], Loss: 0.7387\n",
      "Epoch [3/20], Step [134/704], Loss: 0.7091\n",
      "Epoch [3/20], Step [135/704], Loss: 0.6521\n",
      "Epoch [3/20], Step [136/704], Loss: 0.6163\n",
      "Epoch [3/20], Step [137/704], Loss: 0.5544\n",
      "Epoch [3/20], Step [138/704], Loss: 0.7351\n",
      "Epoch [3/20], Step [139/704], Loss: 0.7623\n",
      "Epoch [3/20], Step [140/704], Loss: 0.5689\n",
      "Epoch [3/20], Step [141/704], Loss: 0.6970\n",
      "Epoch [3/20], Step [142/704], Loss: 0.9636\n",
      "Epoch [3/20], Step [143/704], Loss: 0.5608\n",
      "Epoch [3/20], Step [144/704], Loss: 0.8023\n",
      "Epoch [3/20], Step [145/704], Loss: 0.5794\n",
      "Epoch [3/20], Step [146/704], Loss: 0.7750\n",
      "Epoch [3/20], Step [147/704], Loss: 0.9200\n",
      "Epoch [3/20], Step [148/704], Loss: 0.6368\n",
      "Epoch [3/20], Step [149/704], Loss: 0.7366\n",
      "Epoch [3/20], Step [150/704], Loss: 0.7498\n",
      "Epoch [3/20], Step [151/704], Loss: 0.9200\n",
      "Epoch [3/20], Step [152/704], Loss: 0.9511\n",
      "Epoch [3/20], Step [153/704], Loss: 0.5598\n",
      "Epoch [3/20], Step [154/704], Loss: 0.7813\n",
      "Epoch [3/20], Step [155/704], Loss: 0.8144\n",
      "Epoch [3/20], Step [156/704], Loss: 0.7274\n",
      "Epoch [3/20], Step [157/704], Loss: 0.7845\n",
      "Epoch [3/20], Step [158/704], Loss: 0.7941\n",
      "Epoch [3/20], Step [159/704], Loss: 0.6634\n",
      "Epoch [3/20], Step [160/704], Loss: 0.7375\n",
      "Epoch [3/20], Step [161/704], Loss: 0.6777\n",
      "Epoch [3/20], Step [162/704], Loss: 0.6988\n",
      "Epoch [3/20], Step [163/704], Loss: 0.7976\n",
      "Epoch [3/20], Step [164/704], Loss: 0.5839\n",
      "Epoch [3/20], Step [165/704], Loss: 0.7048\n",
      "Epoch [3/20], Step [166/704], Loss: 0.8941\n",
      "Epoch [3/20], Step [167/704], Loss: 0.5579\n",
      "Epoch [3/20], Step [168/704], Loss: 0.4966\n",
      "Epoch [3/20], Step [169/704], Loss: 1.0147\n",
      "Epoch [3/20], Step [170/704], Loss: 0.6507\n",
      "Epoch [3/20], Step [171/704], Loss: 0.8635\n",
      "Epoch [3/20], Step [172/704], Loss: 0.5771\n",
      "Epoch [3/20], Step [173/704], Loss: 0.5769\n",
      "Epoch [3/20], Step [174/704], Loss: 0.6187\n",
      "Epoch [3/20], Step [175/704], Loss: 0.7328\n",
      "Epoch [3/20], Step [176/704], Loss: 1.0135\n",
      "Epoch [3/20], Step [177/704], Loss: 0.8706\n",
      "Epoch [3/20], Step [178/704], Loss: 0.4858\n",
      "Epoch [3/20], Step [179/704], Loss: 0.8677\n",
      "Epoch [3/20], Step [180/704], Loss: 0.6211\n",
      "Epoch [3/20], Step [181/704], Loss: 0.4573\n",
      "Epoch [3/20], Step [182/704], Loss: 0.6640\n",
      "Epoch [3/20], Step [183/704], Loss: 0.5342\n",
      "Epoch [3/20], Step [184/704], Loss: 0.7336\n",
      "Epoch [3/20], Step [185/704], Loss: 0.6845\n",
      "Epoch [3/20], Step [186/704], Loss: 0.6500\n",
      "Epoch [3/20], Step [187/704], Loss: 0.8209\n",
      "Epoch [3/20], Step [188/704], Loss: 0.8382\n",
      "Epoch [3/20], Step [189/704], Loss: 0.6720\n",
      "Epoch [3/20], Step [190/704], Loss: 0.5535\n",
      "Epoch [3/20], Step [191/704], Loss: 0.7172\n",
      "Epoch [3/20], Step [192/704], Loss: 0.4674\n",
      "Epoch [3/20], Step [193/704], Loss: 0.8387\n",
      "Epoch [3/20], Step [194/704], Loss: 0.5807\n",
      "Epoch [3/20], Step [195/704], Loss: 0.8136\n",
      "Epoch [3/20], Step [196/704], Loss: 0.7336\n",
      "Epoch [3/20], Step [197/704], Loss: 0.5854\n",
      "Epoch [3/20], Step [198/704], Loss: 0.8203\n",
      "Epoch [3/20], Step [199/704], Loss: 0.5380\n",
      "Epoch [3/20], Step [200/704], Loss: 0.6020\n",
      "Epoch [3/20], Step [201/704], Loss: 0.9262\n",
      "Epoch [3/20], Step [202/704], Loss: 0.8379\n",
      "Epoch [3/20], Step [203/704], Loss: 0.4496\n",
      "Epoch [3/20], Step [204/704], Loss: 0.6503\n",
      "Epoch [3/20], Step [205/704], Loss: 0.8262\n",
      "Epoch [3/20], Step [206/704], Loss: 0.7566\n",
      "Epoch [3/20], Step [207/704], Loss: 0.9371\n",
      "Epoch [3/20], Step [208/704], Loss: 0.4894\n",
      "Epoch [3/20], Step [209/704], Loss: 0.6788\n",
      "Epoch [3/20], Step [210/704], Loss: 0.7703\n",
      "Epoch [3/20], Step [211/704], Loss: 0.7981\n",
      "Epoch [3/20], Step [212/704], Loss: 0.7727\n",
      "Epoch [3/20], Step [213/704], Loss: 0.6551\n",
      "Epoch [3/20], Step [214/704], Loss: 0.6188\n",
      "Epoch [3/20], Step [215/704], Loss: 0.6810\n",
      "Epoch [3/20], Step [216/704], Loss: 0.9089\n",
      "Epoch [3/20], Step [217/704], Loss: 0.6864\n",
      "Epoch [3/20], Step [218/704], Loss: 0.5966\n",
      "Epoch [3/20], Step [219/704], Loss: 0.6121\n",
      "Epoch [3/20], Step [220/704], Loss: 0.5217\n",
      "Epoch [3/20], Step [221/704], Loss: 0.6017\n",
      "Epoch [3/20], Step [222/704], Loss: 0.6728\n",
      "Epoch [3/20], Step [223/704], Loss: 0.8327\n",
      "Epoch [3/20], Step [224/704], Loss: 0.6367\n",
      "Epoch [3/20], Step [225/704], Loss: 0.5882\n",
      "Epoch [3/20], Step [226/704], Loss: 0.7843\n",
      "Epoch [3/20], Step [227/704], Loss: 0.9359\n",
      "Epoch [3/20], Step [228/704], Loss: 0.5858\n",
      "Epoch [3/20], Step [229/704], Loss: 0.4205\n",
      "Epoch [3/20], Step [230/704], Loss: 0.6738\n",
      "Epoch [3/20], Step [231/704], Loss: 0.7733\n",
      "Epoch [3/20], Step [232/704], Loss: 0.6434\n",
      "Epoch [3/20], Step [233/704], Loss: 0.7658\n",
      "Epoch [3/20], Step [234/704], Loss: 0.8884\n",
      "Epoch [3/20], Step [235/704], Loss: 0.7709\n",
      "Epoch [3/20], Step [236/704], Loss: 0.7890\n",
      "Epoch [3/20], Step [237/704], Loss: 0.7269\n",
      "Epoch [3/20], Step [238/704], Loss: 0.7327\n",
      "Epoch [3/20], Step [239/704], Loss: 0.4513\n",
      "Epoch [3/20], Step [240/704], Loss: 0.7538\n",
      "Epoch [3/20], Step [241/704], Loss: 0.7518\n",
      "Epoch [3/20], Step [242/704], Loss: 0.7440\n",
      "Epoch [3/20], Step [243/704], Loss: 0.7960\n",
      "Epoch [3/20], Step [244/704], Loss: 0.9479\n",
      "Epoch [3/20], Step [245/704], Loss: 0.9149\n",
      "Epoch [3/20], Step [246/704], Loss: 0.6559\n",
      "Epoch [3/20], Step [247/704], Loss: 0.6156\n",
      "Epoch [3/20], Step [248/704], Loss: 0.8325\n",
      "Epoch [3/20], Step [249/704], Loss: 0.6918\n",
      "Epoch [3/20], Step [250/704], Loss: 0.6770\n",
      "Epoch [3/20], Step [251/704], Loss: 0.8496\n",
      "Epoch [3/20], Step [252/704], Loss: 0.6989\n",
      "Epoch [3/20], Step [253/704], Loss: 0.5437\n",
      "Epoch [3/20], Step [254/704], Loss: 0.7921\n",
      "Epoch [3/20], Step [255/704], Loss: 0.6725\n",
      "Epoch [3/20], Step [256/704], Loss: 0.4143\n",
      "Epoch [3/20], Step [257/704], Loss: 0.7308\n",
      "Epoch [3/20], Step [258/704], Loss: 1.0932\n",
      "Epoch [3/20], Step [259/704], Loss: 0.6654\n",
      "Epoch [3/20], Step [260/704], Loss: 0.6529\n",
      "Epoch [3/20], Step [261/704], Loss: 0.8470\n",
      "Epoch [3/20], Step [262/704], Loss: 0.9244\n",
      "Epoch [3/20], Step [263/704], Loss: 0.6075\n",
      "Epoch [3/20], Step [264/704], Loss: 0.8694\n",
      "Epoch [3/20], Step [265/704], Loss: 0.5733\n",
      "Epoch [3/20], Step [266/704], Loss: 0.6185\n",
      "Epoch [3/20], Step [267/704], Loss: 0.6176\n",
      "Epoch [3/20], Step [268/704], Loss: 0.5347\n",
      "Epoch [3/20], Step [269/704], Loss: 0.5791\n",
      "Epoch [3/20], Step [270/704], Loss: 0.7272\n",
      "Epoch [3/20], Step [271/704], Loss: 0.8137\n",
      "Epoch [3/20], Step [272/704], Loss: 0.6228\n",
      "Epoch [3/20], Step [273/704], Loss: 0.5718\n",
      "Epoch [3/20], Step [274/704], Loss: 0.6927\n",
      "Epoch [3/20], Step [275/704], Loss: 0.6664\n",
      "Epoch [3/20], Step [276/704], Loss: 0.5072\n",
      "Epoch [3/20], Step [277/704], Loss: 0.8971\n",
      "Epoch [3/20], Step [278/704], Loss: 0.9371\n",
      "Epoch [3/20], Step [279/704], Loss: 0.9652\n",
      "Epoch [3/20], Step [280/704], Loss: 0.6759\n",
      "Epoch [3/20], Step [281/704], Loss: 0.6085\n",
      "Epoch [3/20], Step [282/704], Loss: 0.6992\n",
      "Epoch [3/20], Step [283/704], Loss: 0.5717\n",
      "Epoch [3/20], Step [284/704], Loss: 0.7301\n",
      "Epoch [3/20], Step [285/704], Loss: 0.7695\n",
      "Epoch [3/20], Step [286/704], Loss: 0.6651\n",
      "Epoch [3/20], Step [287/704], Loss: 0.5073\n",
      "Epoch [3/20], Step [288/704], Loss: 0.6428\n",
      "Epoch [3/20], Step [289/704], Loss: 0.7104\n",
      "Epoch [3/20], Step [290/704], Loss: 0.6338\n",
      "Epoch [3/20], Step [291/704], Loss: 0.6662\n",
      "Epoch [3/20], Step [292/704], Loss: 0.4827\n",
      "Epoch [3/20], Step [293/704], Loss: 0.7253\n",
      "Epoch [3/20], Step [294/704], Loss: 0.6997\n",
      "Epoch [3/20], Step [295/704], Loss: 0.4952\n",
      "Epoch [3/20], Step [296/704], Loss: 0.7042\n",
      "Epoch [3/20], Step [297/704], Loss: 0.6346\n",
      "Epoch [3/20], Step [298/704], Loss: 0.6083\n",
      "Epoch [3/20], Step [299/704], Loss: 0.5633\n",
      "Epoch [3/20], Step [300/704], Loss: 0.6700\n",
      "Epoch [3/20], Step [301/704], Loss: 0.5730\n",
      "Epoch [3/20], Step [302/704], Loss: 0.7100\n",
      "Epoch [3/20], Step [303/704], Loss: 0.5250\n",
      "Epoch [3/20], Step [304/704], Loss: 0.6420\n",
      "Epoch [3/20], Step [305/704], Loss: 0.5201\n",
      "Epoch [3/20], Step [306/704], Loss: 0.4650\n",
      "Epoch [3/20], Step [307/704], Loss: 0.5719\n",
      "Epoch [3/20], Step [308/704], Loss: 0.9294\n",
      "Epoch [3/20], Step [309/704], Loss: 0.7327\n",
      "Epoch [3/20], Step [310/704], Loss: 0.4451\n",
      "Epoch [3/20], Step [311/704], Loss: 0.4508\n",
      "Epoch [3/20], Step [312/704], Loss: 0.6832\n",
      "Epoch [3/20], Step [313/704], Loss: 0.8753\n",
      "Epoch [3/20], Step [314/704], Loss: 0.4199\n",
      "Epoch [3/20], Step [315/704], Loss: 0.3850\n",
      "Epoch [3/20], Step [316/704], Loss: 0.7558\n",
      "Epoch [3/20], Step [317/704], Loss: 0.7897\n",
      "Epoch [3/20], Step [318/704], Loss: 0.8254\n",
      "Epoch [3/20], Step [319/704], Loss: 0.9242\n",
      "Epoch [3/20], Step [320/704], Loss: 0.7495\n",
      "Epoch [3/20], Step [321/704], Loss: 0.6153\n",
      "Epoch [3/20], Step [322/704], Loss: 0.7066\n",
      "Epoch [3/20], Step [323/704], Loss: 0.6994\n",
      "Epoch [3/20], Step [324/704], Loss: 0.7442\n",
      "Epoch [3/20], Step [325/704], Loss: 0.4719\n",
      "Epoch [3/20], Step [326/704], Loss: 0.7247\n",
      "Epoch [3/20], Step [327/704], Loss: 0.6652\n",
      "Epoch [3/20], Step [328/704], Loss: 0.5752\n",
      "Epoch [3/20], Step [329/704], Loss: 0.8120\n",
      "Epoch [3/20], Step [330/704], Loss: 0.8431\n",
      "Epoch [3/20], Step [331/704], Loss: 0.6967\n",
      "Epoch [3/20], Step [332/704], Loss: 0.5649\n",
      "Epoch [3/20], Step [333/704], Loss: 0.7186\n",
      "Epoch [3/20], Step [334/704], Loss: 0.4893\n",
      "Epoch [3/20], Step [335/704], Loss: 0.5835\n",
      "Epoch [3/20], Step [336/704], Loss: 0.5300\n",
      "Epoch [3/20], Step [337/704], Loss: 0.9141\n",
      "Epoch [3/20], Step [338/704], Loss: 0.6486\n",
      "Epoch [3/20], Step [339/704], Loss: 0.5618\n",
      "Epoch [3/20], Step [340/704], Loss: 0.5593\n",
      "Epoch [3/20], Step [341/704], Loss: 0.6604\n",
      "Epoch [3/20], Step [342/704], Loss: 0.6941\n",
      "Epoch [3/20], Step [343/704], Loss: 0.6185\n",
      "Epoch [3/20], Step [344/704], Loss: 0.4795\n",
      "Epoch [3/20], Step [345/704], Loss: 0.6067\n",
      "Epoch [3/20], Step [346/704], Loss: 0.6959\n",
      "Epoch [3/20], Step [347/704], Loss: 0.7502\n",
      "Epoch [3/20], Step [348/704], Loss: 0.7575\n",
      "Epoch [3/20], Step [349/704], Loss: 0.5408\n",
      "Epoch [3/20], Step [350/704], Loss: 0.8424\n",
      "Epoch [3/20], Step [351/704], Loss: 0.7963\n",
      "Epoch [3/20], Step [352/704], Loss: 0.6007\n",
      "Epoch [3/20], Step [353/704], Loss: 0.8334\n",
      "Epoch [3/20], Step [354/704], Loss: 0.7958\n",
      "Epoch [3/20], Step [355/704], Loss: 0.6120\n",
      "Epoch [3/20], Step [356/704], Loss: 0.5883\n",
      "Epoch [3/20], Step [357/704], Loss: 0.9685\n",
      "Epoch [3/20], Step [358/704], Loss: 0.9584\n",
      "Epoch [3/20], Step [359/704], Loss: 0.8086\n",
      "Epoch [3/20], Step [360/704], Loss: 0.8939\n",
      "Epoch [3/20], Step [361/704], Loss: 0.6013\n",
      "Epoch [3/20], Step [362/704], Loss: 0.6920\n",
      "Epoch [3/20], Step [363/704], Loss: 0.7671\n",
      "Epoch [3/20], Step [364/704], Loss: 0.9758\n",
      "Epoch [3/20], Step [365/704], Loss: 0.5314\n",
      "Epoch [3/20], Step [366/704], Loss: 0.8510\n",
      "Epoch [3/20], Step [367/704], Loss: 0.7975\n",
      "Epoch [3/20], Step [368/704], Loss: 0.8275\n",
      "Epoch [3/20], Step [369/704], Loss: 0.5851\n",
      "Epoch [3/20], Step [370/704], Loss: 0.7201\n",
      "Epoch [3/20], Step [371/704], Loss: 0.5916\n",
      "Epoch [3/20], Step [372/704], Loss: 0.5606\n",
      "Epoch [3/20], Step [373/704], Loss: 0.6521\n",
      "Epoch [3/20], Step [374/704], Loss: 0.6469\n",
      "Epoch [3/20], Step [375/704], Loss: 0.5540\n",
      "Epoch [3/20], Step [376/704], Loss: 0.5625\n",
      "Epoch [3/20], Step [377/704], Loss: 0.7413\n",
      "Epoch [3/20], Step [378/704], Loss: 0.7720\n",
      "Epoch [3/20], Step [379/704], Loss: 0.6881\n",
      "Epoch [3/20], Step [380/704], Loss: 0.6290\n",
      "Epoch [3/20], Step [381/704], Loss: 0.5634\n",
      "Epoch [3/20], Step [382/704], Loss: 0.6703\n",
      "Epoch [3/20], Step [383/704], Loss: 0.5855\n",
      "Epoch [3/20], Step [384/704], Loss: 0.5673\n",
      "Epoch [3/20], Step [385/704], Loss: 0.6924\n",
      "Epoch [3/20], Step [386/704], Loss: 0.7737\n",
      "Epoch [3/20], Step [387/704], Loss: 0.5095\n",
      "Epoch [3/20], Step [388/704], Loss: 0.5912\n",
      "Epoch [3/20], Step [389/704], Loss: 0.6067\n",
      "Epoch [3/20], Step [390/704], Loss: 0.4365\n",
      "Epoch [3/20], Step [391/704], Loss: 0.9807\n",
      "Epoch [3/20], Step [392/704], Loss: 0.7295\n",
      "Epoch [3/20], Step [393/704], Loss: 0.6672\n",
      "Epoch [3/20], Step [394/704], Loss: 0.7872\n",
      "Epoch [3/20], Step [395/704], Loss: 0.6295\n",
      "Epoch [3/20], Step [396/704], Loss: 0.8176\n",
      "Epoch [3/20], Step [397/704], Loss: 0.6743\n",
      "Epoch [3/20], Step [398/704], Loss: 0.5751\n",
      "Epoch [3/20], Step [399/704], Loss: 0.5882\n",
      "Epoch [3/20], Step [400/704], Loss: 0.4666\n",
      "Epoch [3/20], Step [401/704], Loss: 0.3385\n",
      "Epoch [3/20], Step [402/704], Loss: 0.7536\n",
      "Epoch [3/20], Step [403/704], Loss: 0.7864\n",
      "Epoch [3/20], Step [404/704], Loss: 0.5044\n",
      "Epoch [3/20], Step [405/704], Loss: 0.8284\n",
      "Epoch [3/20], Step [406/704], Loss: 0.7866\n",
      "Epoch [3/20], Step [407/704], Loss: 0.6661\n",
      "Epoch [3/20], Step [408/704], Loss: 0.6095\n",
      "Epoch [3/20], Step [409/704], Loss: 0.6921\n",
      "Epoch [3/20], Step [410/704], Loss: 0.7294\n",
      "Epoch [3/20], Step [411/704], Loss: 0.7583\n",
      "Epoch [3/20], Step [412/704], Loss: 0.5132\n",
      "Epoch [3/20], Step [413/704], Loss: 0.5812\n",
      "Epoch [3/20], Step [414/704], Loss: 0.9032\n",
      "Epoch [3/20], Step [415/704], Loss: 0.7792\n",
      "Epoch [3/20], Step [416/704], Loss: 0.6339\n",
      "Epoch [3/20], Step [417/704], Loss: 0.5749\n",
      "Epoch [3/20], Step [418/704], Loss: 0.6281\n",
      "Epoch [3/20], Step [419/704], Loss: 0.8157\n",
      "Epoch [3/20], Step [420/704], Loss: 0.7819\n",
      "Epoch [3/20], Step [421/704], Loss: 0.7445\n",
      "Epoch [3/20], Step [422/704], Loss: 0.9314\n",
      "Epoch [3/20], Step [423/704], Loss: 0.7475\n",
      "Epoch [3/20], Step [424/704], Loss: 0.7270\n",
      "Epoch [3/20], Step [425/704], Loss: 0.7589\n",
      "Epoch [3/20], Step [426/704], Loss: 0.4592\n",
      "Epoch [3/20], Step [427/704], Loss: 0.7025\n",
      "Epoch [3/20], Step [428/704], Loss: 0.8836\n",
      "Epoch [3/20], Step [429/704], Loss: 0.7802\n",
      "Epoch [3/20], Step [430/704], Loss: 0.4754\n",
      "Epoch [3/20], Step [431/704], Loss: 0.9425\n",
      "Epoch [3/20], Step [432/704], Loss: 0.7121\n",
      "Epoch [3/20], Step [433/704], Loss: 0.6955\n",
      "Epoch [3/20], Step [434/704], Loss: 0.4288\n",
      "Epoch [3/20], Step [435/704], Loss: 0.6884\n",
      "Epoch [3/20], Step [436/704], Loss: 0.6781\n",
      "Epoch [3/20], Step [437/704], Loss: 0.4435\n",
      "Epoch [3/20], Step [438/704], Loss: 0.7777\n",
      "Epoch [3/20], Step [439/704], Loss: 0.8773\n",
      "Epoch [3/20], Step [440/704], Loss: 0.5637\n",
      "Epoch [3/20], Step [441/704], Loss: 0.7249\n",
      "Epoch [3/20], Step [442/704], Loss: 0.6786\n",
      "Epoch [3/20], Step [443/704], Loss: 0.7957\n",
      "Epoch [3/20], Step [444/704], Loss: 0.6039\n",
      "Epoch [3/20], Step [445/704], Loss: 0.6212\n",
      "Epoch [3/20], Step [446/704], Loss: 0.5120\n",
      "Epoch [3/20], Step [447/704], Loss: 0.6543\n",
      "Epoch [3/20], Step [448/704], Loss: 0.4487\n",
      "Epoch [3/20], Step [449/704], Loss: 0.6061\n",
      "Epoch [3/20], Step [450/704], Loss: 0.7058\n",
      "Epoch [3/20], Step [451/704], Loss: 0.5289\n",
      "Epoch [3/20], Step [452/704], Loss: 0.6270\n",
      "Epoch [3/20], Step [453/704], Loss: 0.7675\n",
      "Epoch [3/20], Step [454/704], Loss: 0.5790\n",
      "Epoch [3/20], Step [455/704], Loss: 0.6186\n",
      "Epoch [3/20], Step [456/704], Loss: 0.6838\n",
      "Epoch [3/20], Step [457/704], Loss: 0.6100\n",
      "Epoch [3/20], Step [458/704], Loss: 0.7133\n",
      "Epoch [3/20], Step [459/704], Loss: 0.7174\n",
      "Epoch [3/20], Step [460/704], Loss: 0.6952\n",
      "Epoch [3/20], Step [461/704], Loss: 0.6333\n",
      "Epoch [3/20], Step [462/704], Loss: 0.6220\n",
      "Epoch [3/20], Step [463/704], Loss: 0.6610\n",
      "Epoch [3/20], Step [464/704], Loss: 0.5438\n",
      "Epoch [3/20], Step [465/704], Loss: 0.7623\n",
      "Epoch [3/20], Step [466/704], Loss: 0.5783\n",
      "Epoch [3/20], Step [467/704], Loss: 0.4256\n",
      "Epoch [3/20], Step [468/704], Loss: 0.7132\n",
      "Epoch [3/20], Step [469/704], Loss: 0.5419\n",
      "Epoch [3/20], Step [470/704], Loss: 0.5323\n",
      "Epoch [3/20], Step [471/704], Loss: 0.5581\n",
      "Epoch [3/20], Step [472/704], Loss: 0.6562\n",
      "Epoch [3/20], Step [473/704], Loss: 0.4837\n",
      "Epoch [3/20], Step [474/704], Loss: 0.9116\n",
      "Epoch [3/20], Step [475/704], Loss: 0.5867\n",
      "Epoch [3/20], Step [476/704], Loss: 0.7228\n",
      "Epoch [3/20], Step [477/704], Loss: 0.6387\n",
      "Epoch [3/20], Step [478/704], Loss: 0.5732\n",
      "Epoch [3/20], Step [479/704], Loss: 0.6077\n",
      "Epoch [3/20], Step [480/704], Loss: 0.7585\n",
      "Epoch [3/20], Step [481/704], Loss: 0.5621\n",
      "Epoch [3/20], Step [482/704], Loss: 0.6528\n",
      "Epoch [3/20], Step [483/704], Loss: 0.8818\n",
      "Epoch [3/20], Step [484/704], Loss: 0.6919\n",
      "Epoch [3/20], Step [485/704], Loss: 0.7560\n",
      "Epoch [3/20], Step [486/704], Loss: 0.6428\n",
      "Epoch [3/20], Step [487/704], Loss: 0.6146\n",
      "Epoch [3/20], Step [488/704], Loss: 0.7448\n",
      "Epoch [3/20], Step [489/704], Loss: 0.6505\n",
      "Epoch [3/20], Step [490/704], Loss: 0.7235\n",
      "Epoch [3/20], Step [491/704], Loss: 0.4532\n",
      "Epoch [3/20], Step [492/704], Loss: 0.9301\n",
      "Epoch [3/20], Step [493/704], Loss: 0.6490\n",
      "Epoch [3/20], Step [494/704], Loss: 0.8763\n",
      "Epoch [3/20], Step [495/704], Loss: 0.5456\n",
      "Epoch [3/20], Step [496/704], Loss: 0.6317\n",
      "Epoch [3/20], Step [497/704], Loss: 0.4868\n",
      "Epoch [3/20], Step [498/704], Loss: 0.5633\n",
      "Epoch [3/20], Step [499/704], Loss: 0.8246\n",
      "Epoch [3/20], Step [500/704], Loss: 0.7206\n",
      "Epoch [3/20], Step [501/704], Loss: 0.5452\n",
      "Epoch [3/20], Step [502/704], Loss: 0.4870\n",
      "Epoch [3/20], Step [503/704], Loss: 0.7952\n",
      "Epoch [3/20], Step [504/704], Loss: 0.8660\n",
      "Epoch [3/20], Step [505/704], Loss: 0.8167\n",
      "Epoch [3/20], Step [506/704], Loss: 0.5883\n",
      "Epoch [3/20], Step [507/704], Loss: 0.5932\n",
      "Epoch [3/20], Step [508/704], Loss: 0.7835\n",
      "Epoch [3/20], Step [509/704], Loss: 0.7757\n",
      "Epoch [3/20], Step [510/704], Loss: 0.6802\n",
      "Epoch [3/20], Step [511/704], Loss: 0.5969\n",
      "Epoch [3/20], Step [512/704], Loss: 0.6170\n",
      "Epoch [3/20], Step [513/704], Loss: 0.9269\n",
      "Epoch [3/20], Step [514/704], Loss: 0.5943\n",
      "Epoch [3/20], Step [515/704], Loss: 0.6883\n",
      "Epoch [3/20], Step [516/704], Loss: 0.6250\n",
      "Epoch [3/20], Step [517/704], Loss: 0.6806\n",
      "Epoch [3/20], Step [518/704], Loss: 0.6780\n",
      "Epoch [3/20], Step [519/704], Loss: 0.7642\n",
      "Epoch [3/20], Step [520/704], Loss: 0.7881\n",
      "Epoch [3/20], Step [521/704], Loss: 0.6523\n",
      "Epoch [3/20], Step [522/704], Loss: 0.4960\n",
      "Epoch [3/20], Step [523/704], Loss: 0.7227\n",
      "Epoch [3/20], Step [524/704], Loss: 0.8570\n",
      "Epoch [3/20], Step [525/704], Loss: 0.6238\n",
      "Epoch [3/20], Step [526/704], Loss: 0.6268\n",
      "Epoch [3/20], Step [527/704], Loss: 0.5372\n",
      "Epoch [3/20], Step [528/704], Loss: 0.6601\n",
      "Epoch [3/20], Step [529/704], Loss: 0.8249\n",
      "Epoch [3/20], Step [530/704], Loss: 0.4449\n",
      "Epoch [3/20], Step [531/704], Loss: 0.7446\n",
      "Epoch [3/20], Step [532/704], Loss: 0.8452\n",
      "Epoch [3/20], Step [533/704], Loss: 0.5078\n",
      "Epoch [3/20], Step [534/704], Loss: 0.7831\n",
      "Epoch [3/20], Step [535/704], Loss: 0.6323\n",
      "Epoch [3/20], Step [536/704], Loss: 0.6618\n",
      "Epoch [3/20], Step [537/704], Loss: 0.4396\n",
      "Epoch [3/20], Step [538/704], Loss: 0.7979\n",
      "Epoch [3/20], Step [539/704], Loss: 0.9502\n",
      "Epoch [3/20], Step [540/704], Loss: 0.7292\n",
      "Epoch [3/20], Step [541/704], Loss: 0.5007\n",
      "Epoch [3/20], Step [542/704], Loss: 0.9138\n",
      "Epoch [3/20], Step [543/704], Loss: 0.5878\n",
      "Epoch [3/20], Step [544/704], Loss: 0.6562\n",
      "Epoch [3/20], Step [545/704], Loss: 0.8120\n",
      "Epoch [3/20], Step [546/704], Loss: 0.7398\n",
      "Epoch [3/20], Step [547/704], Loss: 0.4063\n",
      "Epoch [3/20], Step [548/704], Loss: 0.5442\n",
      "Epoch [3/20], Step [549/704], Loss: 0.6167\n",
      "Epoch [3/20], Step [550/704], Loss: 0.8682\n",
      "Epoch [3/20], Step [551/704], Loss: 0.5772\n",
      "Epoch [3/20], Step [552/704], Loss: 0.8208\n",
      "Epoch [3/20], Step [553/704], Loss: 0.5637\n",
      "Epoch [3/20], Step [554/704], Loss: 0.7730\n",
      "Epoch [3/20], Step [555/704], Loss: 0.6438\n",
      "Epoch [3/20], Step [556/704], Loss: 0.9610\n",
      "Epoch [3/20], Step [557/704], Loss: 0.6182\n",
      "Epoch [3/20], Step [558/704], Loss: 0.5627\n",
      "Epoch [3/20], Step [559/704], Loss: 0.9149\n",
      "Epoch [3/20], Step [560/704], Loss: 0.5648\n",
      "Epoch [3/20], Step [561/704], Loss: 0.6115\n",
      "Epoch [3/20], Step [562/704], Loss: 0.4188\n",
      "Epoch [3/20], Step [563/704], Loss: 0.5998\n",
      "Epoch [3/20], Step [564/704], Loss: 0.5391\n",
      "Epoch [3/20], Step [565/704], Loss: 0.6031\n",
      "Epoch [3/20], Step [566/704], Loss: 0.7830\n",
      "Epoch [3/20], Step [567/704], Loss: 0.7720\n",
      "Epoch [3/20], Step [568/704], Loss: 0.6651\n",
      "Epoch [3/20], Step [569/704], Loss: 0.4349\n",
      "Epoch [3/20], Step [570/704], Loss: 0.6927\n",
      "Epoch [3/20], Step [571/704], Loss: 0.5800\n",
      "Epoch [3/20], Step [572/704], Loss: 0.6518\n",
      "Epoch [3/20], Step [573/704], Loss: 0.5648\n",
      "Epoch [3/20], Step [574/704], Loss: 0.9771\n",
      "Epoch [3/20], Step [575/704], Loss: 0.4643\n",
      "Epoch [3/20], Step [576/704], Loss: 0.4285\n",
      "Epoch [3/20], Step [577/704], Loss: 0.6863\n",
      "Epoch [3/20], Step [578/704], Loss: 0.4445\n",
      "Epoch [3/20], Step [579/704], Loss: 0.4889\n",
      "Epoch [3/20], Step [580/704], Loss: 0.5993\n",
      "Epoch [3/20], Step [581/704], Loss: 0.6513\n",
      "Epoch [3/20], Step [582/704], Loss: 0.7225\n",
      "Epoch [3/20], Step [583/704], Loss: 0.9328\n",
      "Epoch [3/20], Step [584/704], Loss: 0.5499\n",
      "Epoch [3/20], Step [585/704], Loss: 0.9572\n",
      "Epoch [3/20], Step [586/704], Loss: 0.7397\n",
      "Epoch [3/20], Step [587/704], Loss: 0.7232\n",
      "Epoch [3/20], Step [588/704], Loss: 0.5131\n",
      "Epoch [3/20], Step [589/704], Loss: 0.5857\n",
      "Epoch [3/20], Step [590/704], Loss: 0.7331\n",
      "Epoch [3/20], Step [591/704], Loss: 0.6419\n",
      "Epoch [3/20], Step [592/704], Loss: 0.5492\n",
      "Epoch [3/20], Step [593/704], Loss: 0.6578\n",
      "Epoch [3/20], Step [594/704], Loss: 0.8515\n",
      "Epoch [3/20], Step [595/704], Loss: 0.7113\n",
      "Epoch [3/20], Step [596/704], Loss: 0.4973\n",
      "Epoch [3/20], Step [597/704], Loss: 0.7464\n",
      "Epoch [3/20], Step [598/704], Loss: 0.6627\n",
      "Epoch [3/20], Step [599/704], Loss: 0.7696\n",
      "Epoch [3/20], Step [600/704], Loss: 0.6025\n",
      "Epoch [3/20], Step [601/704], Loss: 0.7005\n",
      "Epoch [3/20], Step [602/704], Loss: 0.6051\n",
      "Epoch [3/20], Step [603/704], Loss: 0.6476\n",
      "Epoch [3/20], Step [604/704], Loss: 0.7739\n",
      "Epoch [3/20], Step [605/704], Loss: 0.5711\n",
      "Epoch [3/20], Step [606/704], Loss: 0.5304\n",
      "Epoch [3/20], Step [607/704], Loss: 0.7852\n",
      "Epoch [3/20], Step [608/704], Loss: 0.3789\n",
      "Epoch [3/20], Step [609/704], Loss: 0.7914\n",
      "Epoch [3/20], Step [610/704], Loss: 0.9303\n",
      "Epoch [3/20], Step [611/704], Loss: 0.4620\n",
      "Epoch [3/20], Step [612/704], Loss: 0.5723\n",
      "Epoch [3/20], Step [613/704], Loss: 0.4443\n",
      "Epoch [3/20], Step [614/704], Loss: 0.7500\n",
      "Epoch [3/20], Step [615/704], Loss: 0.5631\n",
      "Epoch [3/20], Step [616/704], Loss: 0.7204\n",
      "Epoch [3/20], Step [617/704], Loss: 0.7652\n",
      "Epoch [3/20], Step [618/704], Loss: 0.5137\n",
      "Epoch [3/20], Step [619/704], Loss: 0.5635\n",
      "Epoch [3/20], Step [620/704], Loss: 0.5364\n",
      "Epoch [3/20], Step [621/704], Loss: 0.5313\n",
      "Epoch [3/20], Step [622/704], Loss: 0.5843\n",
      "Epoch [3/20], Step [623/704], Loss: 0.5824\n",
      "Epoch [3/20], Step [624/704], Loss: 0.5992\n",
      "Epoch [3/20], Step [625/704], Loss: 0.6321\n",
      "Epoch [3/20], Step [626/704], Loss: 0.5837\n",
      "Epoch [3/20], Step [627/704], Loss: 0.6387\n",
      "Epoch [3/20], Step [628/704], Loss: 0.6398\n",
      "Epoch [3/20], Step [629/704], Loss: 0.6099\n",
      "Epoch [3/20], Step [630/704], Loss: 0.7449\n",
      "Epoch [3/20], Step [631/704], Loss: 0.6983\n",
      "Epoch [3/20], Step [632/704], Loss: 0.7741\n",
      "Epoch [3/20], Step [633/704], Loss: 0.7696\n",
      "Epoch [3/20], Step [634/704], Loss: 0.5991\n",
      "Epoch [3/20], Step [635/704], Loss: 0.7502\n",
      "Epoch [3/20], Step [636/704], Loss: 0.7909\n",
      "Epoch [3/20], Step [637/704], Loss: 1.0342\n",
      "Epoch [3/20], Step [638/704], Loss: 0.5646\n",
      "Epoch [3/20], Step [639/704], Loss: 0.6362\n",
      "Epoch [3/20], Step [640/704], Loss: 0.4708\n",
      "Epoch [3/20], Step [641/704], Loss: 0.6312\n",
      "Epoch [3/20], Step [642/704], Loss: 0.8555\n",
      "Epoch [3/20], Step [643/704], Loss: 0.6269\n",
      "Epoch [3/20], Step [644/704], Loss: 0.7412\n",
      "Epoch [3/20], Step [645/704], Loss: 0.7020\n",
      "Epoch [3/20], Step [646/704], Loss: 0.7494\n",
      "Epoch [3/20], Step [647/704], Loss: 0.7968\n",
      "Epoch [3/20], Step [648/704], Loss: 0.7307\n",
      "Epoch [3/20], Step [649/704], Loss: 0.7400\n",
      "Epoch [3/20], Step [650/704], Loss: 0.8236\n",
      "Epoch [3/20], Step [651/704], Loss: 1.0803\n",
      "Epoch [3/20], Step [652/704], Loss: 0.7433\n",
      "Epoch [3/20], Step [653/704], Loss: 0.5948\n",
      "Epoch [3/20], Step [654/704], Loss: 0.8909\n",
      "Epoch [3/20], Step [655/704], Loss: 0.6991\n",
      "Epoch [3/20], Step [656/704], Loss: 0.6507\n",
      "Epoch [3/20], Step [657/704], Loss: 0.7445\n",
      "Epoch [3/20], Step [658/704], Loss: 0.5994\n",
      "Epoch [3/20], Step [659/704], Loss: 0.5131\n",
      "Epoch [3/20], Step [660/704], Loss: 0.7071\n",
      "Epoch [3/20], Step [661/704], Loss: 0.5416\n",
      "Epoch [3/20], Step [662/704], Loss: 0.6181\n",
      "Epoch [3/20], Step [663/704], Loss: 0.8001\n",
      "Epoch [3/20], Step [664/704], Loss: 0.5838\n",
      "Epoch [3/20], Step [665/704], Loss: 0.7198\n",
      "Epoch [3/20], Step [666/704], Loss: 0.6981\n",
      "Epoch [3/20], Step [667/704], Loss: 0.6592\n",
      "Epoch [3/20], Step [668/704], Loss: 0.5627\n",
      "Epoch [3/20], Step [669/704], Loss: 0.8062\n",
      "Epoch [3/20], Step [670/704], Loss: 0.5312\n",
      "Epoch [3/20], Step [671/704], Loss: 0.6931\n",
      "Epoch [3/20], Step [672/704], Loss: 0.6244\n",
      "Epoch [3/20], Step [673/704], Loss: 0.8136\n",
      "Epoch [3/20], Step [674/704], Loss: 0.5706\n",
      "Epoch [3/20], Step [675/704], Loss: 0.4304\n",
      "Epoch [3/20], Step [676/704], Loss: 0.6060\n",
      "Epoch [3/20], Step [677/704], Loss: 0.7111\n",
      "Epoch [3/20], Step [678/704], Loss: 0.8139\n",
      "Epoch [3/20], Step [679/704], Loss: 0.6271\n",
      "Epoch [3/20], Step [680/704], Loss: 0.7547\n",
      "Epoch [3/20], Step [681/704], Loss: 0.6654\n",
      "Epoch [3/20], Step [682/704], Loss: 0.8744\n",
      "Epoch [3/20], Step [683/704], Loss: 0.6627\n",
      "Epoch [3/20], Step [684/704], Loss: 0.7770\n",
      "Epoch [3/20], Step [685/704], Loss: 0.7967\n",
      "Epoch [3/20], Step [686/704], Loss: 0.8853\n",
      "Epoch [3/20], Step [687/704], Loss: 0.4780\n",
      "Epoch [3/20], Step [688/704], Loss: 0.5891\n",
      "Epoch [3/20], Step [689/704], Loss: 0.7147\n",
      "Epoch [3/20], Step [690/704], Loss: 0.6115\n",
      "Epoch [3/20], Step [691/704], Loss: 0.6774\n",
      "Epoch [3/20], Step [692/704], Loss: 0.5642\n",
      "Epoch [3/20], Step [693/704], Loss: 0.8518\n",
      "Epoch [3/20], Step [694/704], Loss: 0.7665\n",
      "Epoch [3/20], Step [695/704], Loss: 0.6575\n",
      "Epoch [3/20], Step [696/704], Loss: 0.4704\n",
      "Epoch [3/20], Step [697/704], Loss: 0.5748\n",
      "Epoch [3/20], Step [698/704], Loss: 0.7272\n",
      "Epoch [3/20], Step [699/704], Loss: 0.5785\n",
      "Epoch [3/20], Step [700/704], Loss: 0.8699\n",
      "Epoch [3/20], Step [701/704], Loss: 0.5939\n",
      "Epoch [3/20], Step [702/704], Loss: 0.5592\n",
      "Epoch [3/20], Step [703/704], Loss: 0.4879\n",
      "Epoch [3/20], Step [704/704], Loss: 0.4082\n",
      "Epoch [3/20], Loss: 0.4082\n",
      "Epoch [4/20], Step [1/704], Loss: 0.4940\n",
      "Epoch [4/20], Step [2/704], Loss: 0.7337\n",
      "Epoch [4/20], Step [3/704], Loss: 0.5784\n",
      "Epoch [4/20], Step [4/704], Loss: 0.3940\n",
      "Epoch [4/20], Step [5/704], Loss: 0.7215\n",
      "Epoch [4/20], Step [6/704], Loss: 0.6181\n",
      "Epoch [4/20], Step [7/704], Loss: 0.5168\n",
      "Epoch [4/20], Step [8/704], Loss: 0.5540\n",
      "Epoch [4/20], Step [9/704], Loss: 0.6046\n",
      "Epoch [4/20], Step [10/704], Loss: 0.7228\n",
      "Epoch [4/20], Step [11/704], Loss: 0.5697\n",
      "Epoch [4/20], Step [12/704], Loss: 0.7029\n",
      "Epoch [4/20], Step [13/704], Loss: 0.7172\n",
      "Epoch [4/20], Step [14/704], Loss: 0.4514\n",
      "Epoch [4/20], Step [15/704], Loss: 0.6780\n",
      "Epoch [4/20], Step [16/704], Loss: 0.3899\n",
      "Epoch [4/20], Step [17/704], Loss: 0.4728\n",
      "Epoch [4/20], Step [18/704], Loss: 0.4366\n",
      "Epoch [4/20], Step [19/704], Loss: 0.5820\n",
      "Epoch [4/20], Step [20/704], Loss: 0.5185\n",
      "Epoch [4/20], Step [21/704], Loss: 0.5616\n",
      "Epoch [4/20], Step [22/704], Loss: 0.5380\n",
      "Epoch [4/20], Step [23/704], Loss: 0.5725\n",
      "Epoch [4/20], Step [24/704], Loss: 0.4609\n",
      "Epoch [4/20], Step [25/704], Loss: 0.6065\n",
      "Epoch [4/20], Step [26/704], Loss: 0.6059\n",
      "Epoch [4/20], Step [27/704], Loss: 0.6222\n",
      "Epoch [4/20], Step [28/704], Loss: 0.5146\n",
      "Epoch [4/20], Step [29/704], Loss: 0.5392\n",
      "Epoch [4/20], Step [30/704], Loss: 0.5832\n",
      "Epoch [4/20], Step [31/704], Loss: 0.4481\n",
      "Epoch [4/20], Step [32/704], Loss: 0.4020\n",
      "Epoch [4/20], Step [33/704], Loss: 0.6105\n",
      "Epoch [4/20], Step [34/704], Loss: 0.6932\n",
      "Epoch [4/20], Step [35/704], Loss: 0.4879\n",
      "Epoch [4/20], Step [36/704], Loss: 0.5081\n",
      "Epoch [4/20], Step [37/704], Loss: 0.7731\n",
      "Epoch [4/20], Step [38/704], Loss: 0.4380\n",
      "Epoch [4/20], Step [39/704], Loss: 0.3137\n",
      "Epoch [4/20], Step [40/704], Loss: 0.5551\n",
      "Epoch [4/20], Step [41/704], Loss: 0.6691\n",
      "Epoch [4/20], Step [42/704], Loss: 0.4006\n",
      "Epoch [4/20], Step [43/704], Loss: 0.7259\n",
      "Epoch [4/20], Step [44/704], Loss: 0.6776\n",
      "Epoch [4/20], Step [45/704], Loss: 0.5064\n",
      "Epoch [4/20], Step [46/704], Loss: 0.4173\n",
      "Epoch [4/20], Step [47/704], Loss: 0.6868\n",
      "Epoch [4/20], Step [48/704], Loss: 0.5593\n",
      "Epoch [4/20], Step [49/704], Loss: 0.4175\n",
      "Epoch [4/20], Step [50/704], Loss: 0.4698\n",
      "Epoch [4/20], Step [51/704], Loss: 0.7883\n",
      "Epoch [4/20], Step [52/704], Loss: 0.6151\n",
      "Epoch [4/20], Step [53/704], Loss: 0.6265\n",
      "Epoch [4/20], Step [54/704], Loss: 0.7088\n",
      "Epoch [4/20], Step [55/704], Loss: 0.4935\n",
      "Epoch [4/20], Step [56/704], Loss: 0.4314\n",
      "Epoch [4/20], Step [57/704], Loss: 0.4432\n",
      "Epoch [4/20], Step [58/704], Loss: 0.4605\n",
      "Epoch [4/20], Step [59/704], Loss: 0.4799\n",
      "Epoch [4/20], Step [60/704], Loss: 0.6997\n",
      "Epoch [4/20], Step [61/704], Loss: 0.7836\n",
      "Epoch [4/20], Step [62/704], Loss: 0.3760\n",
      "Epoch [4/20], Step [63/704], Loss: 0.4832\n",
      "Epoch [4/20], Step [64/704], Loss: 0.5695\n",
      "Epoch [4/20], Step [65/704], Loss: 0.7541\n",
      "Epoch [4/20], Step [66/704], Loss: 0.5295\n",
      "Epoch [4/20], Step [67/704], Loss: 0.4183\n",
      "Epoch [4/20], Step [68/704], Loss: 0.5729\n",
      "Epoch [4/20], Step [69/704], Loss: 0.4732\n",
      "Epoch [4/20], Step [70/704], Loss: 0.5139\n",
      "Epoch [4/20], Step [71/704], Loss: 0.7067\n",
      "Epoch [4/20], Step [72/704], Loss: 0.4258\n",
      "Epoch [4/20], Step [73/704], Loss: 0.6039\n",
      "Epoch [4/20], Step [74/704], Loss: 0.5815\n",
      "Epoch [4/20], Step [75/704], Loss: 0.7310\n",
      "Epoch [4/20], Step [76/704], Loss: 0.4891\n",
      "Epoch [4/20], Step [77/704], Loss: 0.4082\n",
      "Epoch [4/20], Step [78/704], Loss: 0.4721\n",
      "Epoch [4/20], Step [79/704], Loss: 0.4578\n",
      "Epoch [4/20], Step [80/704], Loss: 0.6867\n",
      "Epoch [4/20], Step [81/704], Loss: 0.3766\n",
      "Epoch [4/20], Step [82/704], Loss: 0.5753\n",
      "Epoch [4/20], Step [83/704], Loss: 0.3068\n",
      "Epoch [4/20], Step [84/704], Loss: 0.4571\n",
      "Epoch [4/20], Step [85/704], Loss: 0.6181\n",
      "Epoch [4/20], Step [86/704], Loss: 0.4074\n",
      "Epoch [4/20], Step [87/704], Loss: 0.7203\n",
      "Epoch [4/20], Step [88/704], Loss: 0.5769\n",
      "Epoch [4/20], Step [89/704], Loss: 0.4385\n",
      "Epoch [4/20], Step [90/704], Loss: 0.5343\n",
      "Epoch [4/20], Step [91/704], Loss: 0.4590\n",
      "Epoch [4/20], Step [92/704], Loss: 0.6761\n",
      "Epoch [4/20], Step [93/704], Loss: 0.4535\n",
      "Epoch [4/20], Step [94/704], Loss: 0.4409\n",
      "Epoch [4/20], Step [95/704], Loss: 0.5962\n",
      "Epoch [4/20], Step [96/704], Loss: 0.5571\n",
      "Epoch [4/20], Step [97/704], Loss: 0.5118\n",
      "Epoch [4/20], Step [98/704], Loss: 0.3376\n",
      "Epoch [4/20], Step [99/704], Loss: 0.5956\n",
      "Epoch [4/20], Step [100/704], Loss: 0.4100\n",
      "Epoch [4/20], Step [101/704], Loss: 0.5429\n",
      "Epoch [4/20], Step [102/704], Loss: 0.6614\n",
      "Epoch [4/20], Step [103/704], Loss: 0.5460\n",
      "Epoch [4/20], Step [104/704], Loss: 0.6110\n",
      "Epoch [4/20], Step [105/704], Loss: 0.6060\n",
      "Epoch [4/20], Step [106/704], Loss: 0.5003\n",
      "Epoch [4/20], Step [107/704], Loss: 0.6018\n",
      "Epoch [4/20], Step [108/704], Loss: 0.4149\n",
      "Epoch [4/20], Step [109/704], Loss: 0.5734\n",
      "Epoch [4/20], Step [110/704], Loss: 0.6320\n",
      "Epoch [4/20], Step [111/704], Loss: 0.3730\n",
      "Epoch [4/20], Step [112/704], Loss: 0.6457\n",
      "Epoch [4/20], Step [113/704], Loss: 0.3827\n",
      "Epoch [4/20], Step [114/704], Loss: 0.6982\n",
      "Epoch [4/20], Step [115/704], Loss: 0.5960\n",
      "Epoch [4/20], Step [116/704], Loss: 0.4322\n",
      "Epoch [4/20], Step [117/704], Loss: 0.5075\n",
      "Epoch [4/20], Step [118/704], Loss: 0.4954\n",
      "Epoch [4/20], Step [119/704], Loss: 0.7639\n",
      "Epoch [4/20], Step [120/704], Loss: 0.4504\n",
      "Epoch [4/20], Step [121/704], Loss: 0.4510\n",
      "Epoch [4/20], Step [122/704], Loss: 0.5825\n",
      "Epoch [4/20], Step [123/704], Loss: 0.7132\n",
      "Epoch [4/20], Step [124/704], Loss: 0.5505\n",
      "Epoch [4/20], Step [125/704], Loss: 0.6730\n",
      "Epoch [4/20], Step [126/704], Loss: 0.4855\n",
      "Epoch [4/20], Step [127/704], Loss: 0.4772\n",
      "Epoch [4/20], Step [128/704], Loss: 0.5637\n",
      "Epoch [4/20], Step [129/704], Loss: 0.3819\n",
      "Epoch [4/20], Step [130/704], Loss: 0.4706\n",
      "Epoch [4/20], Step [131/704], Loss: 0.5549\n",
      "Epoch [4/20], Step [132/704], Loss: 0.7068\n",
      "Epoch [4/20], Step [133/704], Loss: 0.6307\n",
      "Epoch [4/20], Step [134/704], Loss: 0.4583\n",
      "Epoch [4/20], Step [135/704], Loss: 0.5751\n",
      "Epoch [4/20], Step [136/704], Loss: 0.8937\n",
      "Epoch [4/20], Step [137/704], Loss: 0.6833\n",
      "Epoch [4/20], Step [138/704], Loss: 0.4635\n",
      "Epoch [4/20], Step [139/704], Loss: 0.6472\n",
      "Epoch [4/20], Step [140/704], Loss: 0.4915\n",
      "Epoch [4/20], Step [141/704], Loss: 0.8101\n",
      "Epoch [4/20], Step [142/704], Loss: 0.7366\n",
      "Epoch [4/20], Step [143/704], Loss: 0.5028\n",
      "Epoch [4/20], Step [144/704], Loss: 0.6271\n",
      "Epoch [4/20], Step [145/704], Loss: 0.6149\n",
      "Epoch [4/20], Step [146/704], Loss: 0.5346\n",
      "Epoch [4/20], Step [147/704], Loss: 0.5448\n",
      "Epoch [4/20], Step [148/704], Loss: 0.5679\n",
      "Epoch [4/20], Step [149/704], Loss: 0.6700\n",
      "Epoch [4/20], Step [150/704], Loss: 0.4493\n",
      "Epoch [4/20], Step [151/704], Loss: 0.4397\n",
      "Epoch [4/20], Step [152/704], Loss: 0.5461\n",
      "Epoch [4/20], Step [153/704], Loss: 0.6015\n",
      "Epoch [4/20], Step [154/704], Loss: 0.6005\n",
      "Epoch [4/20], Step [155/704], Loss: 0.5148\n",
      "Epoch [4/20], Step [156/704], Loss: 0.4168\n",
      "Epoch [4/20], Step [157/704], Loss: 0.4791\n",
      "Epoch [4/20], Step [158/704], Loss: 0.4532\n",
      "Epoch [4/20], Step [159/704], Loss: 0.4620\n",
      "Epoch [4/20], Step [160/704], Loss: 0.5609\n",
      "Epoch [4/20], Step [161/704], Loss: 0.3291\n",
      "Epoch [4/20], Step [162/704], Loss: 0.4678\n",
      "Epoch [4/20], Step [163/704], Loss: 0.4706\n",
      "Epoch [4/20], Step [164/704], Loss: 0.6264\n",
      "Epoch [4/20], Step [165/704], Loss: 0.5100\n",
      "Epoch [4/20], Step [166/704], Loss: 0.5325\n",
      "Epoch [4/20], Step [167/704], Loss: 0.5904\n",
      "Epoch [4/20], Step [168/704], Loss: 0.6261\n",
      "Epoch [4/20], Step [169/704], Loss: 0.6201\n",
      "Epoch [4/20], Step [170/704], Loss: 0.4875\n",
      "Epoch [4/20], Step [171/704], Loss: 0.7899\n",
      "Epoch [4/20], Step [172/704], Loss: 0.5898\n",
      "Epoch [4/20], Step [173/704], Loss: 0.6118\n",
      "Epoch [4/20], Step [174/704], Loss: 0.5555\n",
      "Epoch [4/20], Step [175/704], Loss: 0.3663\n",
      "Epoch [4/20], Step [176/704], Loss: 0.6599\n",
      "Epoch [4/20], Step [177/704], Loss: 0.6106\n",
      "Epoch [4/20], Step [178/704], Loss: 0.7293\n",
      "Epoch [4/20], Step [179/704], Loss: 0.4289\n",
      "Epoch [4/20], Step [180/704], Loss: 0.6982\n",
      "Epoch [4/20], Step [181/704], Loss: 0.4240\n",
      "Epoch [4/20], Step [182/704], Loss: 0.5062\n",
      "Epoch [4/20], Step [183/704], Loss: 0.7122\n",
      "Epoch [4/20], Step [184/704], Loss: 0.5360\n",
      "Epoch [4/20], Step [185/704], Loss: 0.4414\n",
      "Epoch [4/20], Step [186/704], Loss: 0.6483\n",
      "Epoch [4/20], Step [187/704], Loss: 0.5498\n",
      "Epoch [4/20], Step [188/704], Loss: 0.4470\n",
      "Epoch [4/20], Step [189/704], Loss: 0.4545\n",
      "Epoch [4/20], Step [190/704], Loss: 0.5169\n",
      "Epoch [4/20], Step [191/704], Loss: 0.5857\n",
      "Epoch [4/20], Step [192/704], Loss: 0.4403\n",
      "Epoch [4/20], Step [193/704], Loss: 0.6110\n",
      "Epoch [4/20], Step [194/704], Loss: 0.7632\n",
      "Epoch [4/20], Step [195/704], Loss: 0.5065\n",
      "Epoch [4/20], Step [196/704], Loss: 0.4223\n",
      "Epoch [4/20], Step [197/704], Loss: 0.5213\n",
      "Epoch [4/20], Step [198/704], Loss: 0.5226\n",
      "Epoch [4/20], Step [199/704], Loss: 0.7088\n",
      "Epoch [4/20], Step [200/704], Loss: 0.4397\n",
      "Epoch [4/20], Step [201/704], Loss: 0.4623\n",
      "Epoch [4/20], Step [202/704], Loss: 0.5155\n",
      "Epoch [4/20], Step [203/704], Loss: 0.5639\n",
      "Epoch [4/20], Step [204/704], Loss: 0.5243\n",
      "Epoch [4/20], Step [205/704], Loss: 0.7062\n",
      "Epoch [4/20], Step [206/704], Loss: 0.6036\n",
      "Epoch [4/20], Step [207/704], Loss: 0.4515\n",
      "Epoch [4/20], Step [208/704], Loss: 0.5134\n",
      "Epoch [4/20], Step [209/704], Loss: 0.6184\n",
      "Epoch [4/20], Step [210/704], Loss: 0.4592\n",
      "Epoch [4/20], Step [211/704], Loss: 0.5893\n",
      "Epoch [4/20], Step [212/704], Loss: 0.7108\n",
      "Epoch [4/20], Step [213/704], Loss: 0.4858\n",
      "Epoch [4/20], Step [214/704], Loss: 0.5116\n",
      "Epoch [4/20], Step [215/704], Loss: 0.6038\n",
      "Epoch [4/20], Step [216/704], Loss: 0.6731\n",
      "Epoch [4/20], Step [217/704], Loss: 0.5383\n",
      "Epoch [4/20], Step [218/704], Loss: 0.5565\n",
      "Epoch [4/20], Step [219/704], Loss: 0.6021\n",
      "Epoch [4/20], Step [220/704], Loss: 0.5851\n",
      "Epoch [4/20], Step [221/704], Loss: 0.6276\n",
      "Epoch [4/20], Step [222/704], Loss: 0.3100\n",
      "Epoch [4/20], Step [223/704], Loss: 0.4301\n",
      "Epoch [4/20], Step [224/704], Loss: 0.4888\n",
      "Epoch [4/20], Step [225/704], Loss: 0.4216\n",
      "Epoch [4/20], Step [226/704], Loss: 0.4500\n",
      "Epoch [4/20], Step [227/704], Loss: 0.6891\n",
      "Epoch [4/20], Step [228/704], Loss: 0.4952\n",
      "Epoch [4/20], Step [229/704], Loss: 0.4639\n",
      "Epoch [4/20], Step [230/704], Loss: 0.3859\n",
      "Epoch [4/20], Step [231/704], Loss: 0.6740\n",
      "Epoch [4/20], Step [232/704], Loss: 0.7848\n",
      "Epoch [4/20], Step [233/704], Loss: 0.5721\n",
      "Epoch [4/20], Step [234/704], Loss: 0.7197\n",
      "Epoch [4/20], Step [235/704], Loss: 0.4533\n",
      "Epoch [4/20], Step [236/704], Loss: 0.4599\n",
      "Epoch [4/20], Step [237/704], Loss: 0.5755\n",
      "Epoch [4/20], Step [238/704], Loss: 0.6362\n",
      "Epoch [4/20], Step [239/704], Loss: 0.6062\n",
      "Epoch [4/20], Step [240/704], Loss: 0.4795\n",
      "Epoch [4/20], Step [241/704], Loss: 0.5526\n",
      "Epoch [4/20], Step [242/704], Loss: 0.4768\n",
      "Epoch [4/20], Step [243/704], Loss: 0.4551\n",
      "Epoch [4/20], Step [244/704], Loss: 0.3510\n",
      "Epoch [4/20], Step [245/704], Loss: 0.7096\n",
      "Epoch [4/20], Step [246/704], Loss: 0.7434\n",
      "Epoch [4/20], Step [247/704], Loss: 0.5868\n",
      "Epoch [4/20], Step [248/704], Loss: 0.4914\n",
      "Epoch [4/20], Step [249/704], Loss: 0.3851\n",
      "Epoch [4/20], Step [250/704], Loss: 0.3736\n",
      "Epoch [4/20], Step [251/704], Loss: 0.6091\n",
      "Epoch [4/20], Step [252/704], Loss: 0.3707\n",
      "Epoch [4/20], Step [253/704], Loss: 0.5001\n",
      "Epoch [4/20], Step [254/704], Loss: 0.7449\n",
      "Epoch [4/20], Step [255/704], Loss: 0.4596\n",
      "Epoch [4/20], Step [256/704], Loss: 0.5193\n",
      "Epoch [4/20], Step [257/704], Loss: 0.4633\n",
      "Epoch [4/20], Step [258/704], Loss: 0.6101\n",
      "Epoch [4/20], Step [259/704], Loss: 0.3973\n",
      "Epoch [4/20], Step [260/704], Loss: 0.5595\n",
      "Epoch [4/20], Step [261/704], Loss: 0.6063\n",
      "Epoch [4/20], Step [262/704], Loss: 0.8446\n",
      "Epoch [4/20], Step [263/704], Loss: 0.5438\n",
      "Epoch [4/20], Step [264/704], Loss: 0.5950\n",
      "Epoch [4/20], Step [265/704], Loss: 0.5564\n",
      "Epoch [4/20], Step [266/704], Loss: 0.5694\n",
      "Epoch [4/20], Step [267/704], Loss: 0.5365\n",
      "Epoch [4/20], Step [268/704], Loss: 0.5548\n",
      "Epoch [4/20], Step [269/704], Loss: 0.4507\n",
      "Epoch [4/20], Step [270/704], Loss: 0.7334\n",
      "Epoch [4/20], Step [271/704], Loss: 0.6888\n",
      "Epoch [4/20], Step [272/704], Loss: 0.5912\n",
      "Epoch [4/20], Step [273/704], Loss: 0.3776\n",
      "Epoch [4/20], Step [274/704], Loss: 0.6201\n",
      "Epoch [4/20], Step [275/704], Loss: 0.6022\n",
      "Epoch [4/20], Step [276/704], Loss: 0.6988\n",
      "Epoch [4/20], Step [277/704], Loss: 0.5027\n",
      "Epoch [4/20], Step [278/704], Loss: 0.4728\n",
      "Epoch [4/20], Step [279/704], Loss: 0.6391\n",
      "Epoch [4/20], Step [280/704], Loss: 0.3619\n",
      "Epoch [4/20], Step [281/704], Loss: 0.5671\n",
      "Epoch [4/20], Step [282/704], Loss: 0.4746\n",
      "Epoch [4/20], Step [283/704], Loss: 0.3981\n",
      "Epoch [4/20], Step [284/704], Loss: 0.5063\n",
      "Epoch [4/20], Step [285/704], Loss: 0.5805\n",
      "Epoch [4/20], Step [286/704], Loss: 0.4919\n",
      "Epoch [4/20], Step [287/704], Loss: 0.6595\n",
      "Epoch [4/20], Step [288/704], Loss: 0.8618\n",
      "Epoch [4/20], Step [289/704], Loss: 0.3835\n",
      "Epoch [4/20], Step [290/704], Loss: 0.4581\n",
      "Epoch [4/20], Step [291/704], Loss: 0.6709\n",
      "Epoch [4/20], Step [292/704], Loss: 0.5850\n",
      "Epoch [4/20], Step [293/704], Loss: 0.6147\n",
      "Epoch [4/20], Step [294/704], Loss: 0.4274\n",
      "Epoch [4/20], Step [295/704], Loss: 0.5504\n",
      "Epoch [4/20], Step [296/704], Loss: 0.4376\n",
      "Epoch [4/20], Step [297/704], Loss: 0.4505\n",
      "Epoch [4/20], Step [298/704], Loss: 0.6747\n",
      "Epoch [4/20], Step [299/704], Loss: 0.5372\n",
      "Epoch [4/20], Step [300/704], Loss: 0.6455\n",
      "Epoch [4/20], Step [301/704], Loss: 0.3972\n",
      "Epoch [4/20], Step [302/704], Loss: 0.9284\n",
      "Epoch [4/20], Step [303/704], Loss: 0.4522\n",
      "Epoch [4/20], Step [304/704], Loss: 0.4997\n",
      "Epoch [4/20], Step [305/704], Loss: 0.3347\n",
      "Epoch [4/20], Step [306/704], Loss: 0.5465\n",
      "Epoch [4/20], Step [307/704], Loss: 0.4753\n",
      "Epoch [4/20], Step [308/704], Loss: 0.7314\n",
      "Epoch [4/20], Step [309/704], Loss: 0.4770\n",
      "Epoch [4/20], Step [310/704], Loss: 0.3600\n",
      "Epoch [4/20], Step [311/704], Loss: 0.5110\n",
      "Epoch [4/20], Step [312/704], Loss: 0.7154\n",
      "Epoch [4/20], Step [313/704], Loss: 0.5119\n",
      "Epoch [4/20], Step [314/704], Loss: 0.9475\n",
      "Epoch [4/20], Step [315/704], Loss: 0.5242\n",
      "Epoch [4/20], Step [316/704], Loss: 0.5515\n",
      "Epoch [4/20], Step [317/704], Loss: 0.6046\n",
      "Epoch [4/20], Step [318/704], Loss: 0.5790\n",
      "Epoch [4/20], Step [319/704], Loss: 0.4326\n",
      "Epoch [4/20], Step [320/704], Loss: 0.6103\n",
      "Epoch [4/20], Step [321/704], Loss: 0.6022\n",
      "Epoch [4/20], Step [322/704], Loss: 0.5566\n",
      "Epoch [4/20], Step [323/704], Loss: 0.5295\n",
      "Epoch [4/20], Step [324/704], Loss: 0.6714\n",
      "Epoch [4/20], Step [325/704], Loss: 0.7649\n",
      "Epoch [4/20], Step [326/704], Loss: 0.6103\n",
      "Epoch [4/20], Step [327/704], Loss: 0.3694\n",
      "Epoch [4/20], Step [328/704], Loss: 0.3130\n",
      "Epoch [4/20], Step [329/704], Loss: 0.6563\n",
      "Epoch [4/20], Step [330/704], Loss: 0.4184\n",
      "Epoch [4/20], Step [331/704], Loss: 0.8933\n",
      "Epoch [4/20], Step [332/704], Loss: 0.8239\n",
      "Epoch [4/20], Step [333/704], Loss: 0.5121\n",
      "Epoch [4/20], Step [334/704], Loss: 0.4393\n",
      "Epoch [4/20], Step [335/704], Loss: 0.4992\n",
      "Epoch [4/20], Step [336/704], Loss: 0.6941\n",
      "Epoch [4/20], Step [337/704], Loss: 0.6870\n",
      "Epoch [4/20], Step [338/704], Loss: 0.6340\n",
      "Epoch [4/20], Step [339/704], Loss: 0.7481\n",
      "Epoch [4/20], Step [340/704], Loss: 0.5240\n",
      "Epoch [4/20], Step [341/704], Loss: 0.6412\n",
      "Epoch [4/20], Step [342/704], Loss: 0.6051\n",
      "Epoch [4/20], Step [343/704], Loss: 0.5534\n",
      "Epoch [4/20], Step [344/704], Loss: 0.4841\n",
      "Epoch [4/20], Step [345/704], Loss: 0.7325\n",
      "Epoch [4/20], Step [346/704], Loss: 0.5244\n",
      "Epoch [4/20], Step [347/704], Loss: 0.8153\n",
      "Epoch [4/20], Step [348/704], Loss: 0.4503\n",
      "Epoch [4/20], Step [349/704], Loss: 0.5950\n",
      "Epoch [4/20], Step [350/704], Loss: 0.3874\n",
      "Epoch [4/20], Step [351/704], Loss: 0.4854\n",
      "Epoch [4/20], Step [352/704], Loss: 0.4863\n",
      "Epoch [4/20], Step [353/704], Loss: 0.5078\n",
      "Epoch [4/20], Step [354/704], Loss: 0.5882\n",
      "Epoch [4/20], Step [355/704], Loss: 0.4036\n",
      "Epoch [4/20], Step [356/704], Loss: 0.5190\n",
      "Epoch [4/20], Step [357/704], Loss: 0.5370\n",
      "Epoch [4/20], Step [358/704], Loss: 0.6183\n",
      "Epoch [4/20], Step [359/704], Loss: 0.2818\n",
      "Epoch [4/20], Step [360/704], Loss: 0.4899\n",
      "Epoch [4/20], Step [361/704], Loss: 0.6705\n",
      "Epoch [4/20], Step [362/704], Loss: 0.5425\n",
      "Epoch [4/20], Step [363/704], Loss: 0.5537\n",
      "Epoch [4/20], Step [364/704], Loss: 0.8069\n",
      "Epoch [4/20], Step [365/704], Loss: 0.6085\n",
      "Epoch [4/20], Step [366/704], Loss: 0.4649\n",
      "Epoch [4/20], Step [367/704], Loss: 0.5706\n",
      "Epoch [4/20], Step [368/704], Loss: 0.5896\n",
      "Epoch [4/20], Step [369/704], Loss: 0.5621\n",
      "Epoch [4/20], Step [370/704], Loss: 0.6002\n",
      "Epoch [4/20], Step [371/704], Loss: 0.4033\n",
      "Epoch [4/20], Step [372/704], Loss: 0.4586\n",
      "Epoch [4/20], Step [373/704], Loss: 0.4675\n",
      "Epoch [4/20], Step [374/704], Loss: 0.3981\n",
      "Epoch [4/20], Step [375/704], Loss: 0.5920\n",
      "Epoch [4/20], Step [376/704], Loss: 0.5908\n",
      "Epoch [4/20], Step [377/704], Loss: 0.4456\n",
      "Epoch [4/20], Step [378/704], Loss: 0.7727\n",
      "Epoch [4/20], Step [379/704], Loss: 0.5507\n",
      "Epoch [4/20], Step [380/704], Loss: 0.4692\n",
      "Epoch [4/20], Step [381/704], Loss: 0.5541\n",
      "Epoch [4/20], Step [382/704], Loss: 0.5068\n",
      "Epoch [4/20], Step [383/704], Loss: 0.5751\n",
      "Epoch [4/20], Step [384/704], Loss: 0.5482\n",
      "Epoch [4/20], Step [385/704], Loss: 0.4668\n",
      "Epoch [4/20], Step [386/704], Loss: 0.5628\n",
      "Epoch [4/20], Step [387/704], Loss: 0.5843\n",
      "Epoch [4/20], Step [388/704], Loss: 0.5283\n",
      "Epoch [4/20], Step [389/704], Loss: 0.3938\n",
      "Epoch [4/20], Step [390/704], Loss: 0.4257\n",
      "Epoch [4/20], Step [391/704], Loss: 0.4575\n",
      "Epoch [4/20], Step [392/704], Loss: 0.4652\n",
      "Epoch [4/20], Step [393/704], Loss: 0.6406\n",
      "Epoch [4/20], Step [394/704], Loss: 0.5039\n",
      "Epoch [4/20], Step [395/704], Loss: 0.6837\n",
      "Epoch [4/20], Step [396/704], Loss: 0.5281\n",
      "Epoch [4/20], Step [397/704], Loss: 0.7015\n",
      "Epoch [4/20], Step [398/704], Loss: 0.4947\n",
      "Epoch [4/20], Step [399/704], Loss: 0.3634\n",
      "Epoch [4/20], Step [400/704], Loss: 0.4712\n",
      "Epoch [4/20], Step [401/704], Loss: 0.4453\n",
      "Epoch [4/20], Step [402/704], Loss: 0.6447\n",
      "Epoch [4/20], Step [403/704], Loss: 0.6053\n",
      "Epoch [4/20], Step [404/704], Loss: 0.5656\n",
      "Epoch [4/20], Step [405/704], Loss: 0.5997\n",
      "Epoch [4/20], Step [406/704], Loss: 0.7334\n",
      "Epoch [4/20], Step [407/704], Loss: 0.7735\n",
      "Epoch [4/20], Step [408/704], Loss: 0.3978\n",
      "Epoch [4/20], Step [409/704], Loss: 0.4656\n",
      "Epoch [4/20], Step [410/704], Loss: 0.6756\n",
      "Epoch [4/20], Step [411/704], Loss: 0.4228\n",
      "Epoch [4/20], Step [412/704], Loss: 0.5027\n",
      "Epoch [4/20], Step [413/704], Loss: 0.4990\n",
      "Epoch [4/20], Step [414/704], Loss: 0.5316\n",
      "Epoch [4/20], Step [415/704], Loss: 0.7036\n",
      "Epoch [4/20], Step [416/704], Loss: 0.5233\n",
      "Epoch [4/20], Step [417/704], Loss: 0.5316\n",
      "Epoch [4/20], Step [418/704], Loss: 0.3949\n",
      "Epoch [4/20], Step [419/704], Loss: 0.4486\n",
      "Epoch [4/20], Step [420/704], Loss: 0.5253\n",
      "Epoch [4/20], Step [421/704], Loss: 0.5010\n",
      "Epoch [4/20], Step [422/704], Loss: 0.4838\n",
      "Epoch [4/20], Step [423/704], Loss: 0.6665\n",
      "Epoch [4/20], Step [424/704], Loss: 0.7051\n",
      "Epoch [4/20], Step [425/704], Loss: 0.6820\n",
      "Epoch [4/20], Step [426/704], Loss: 0.5951\n",
      "Epoch [4/20], Step [427/704], Loss: 0.5785\n",
      "Epoch [4/20], Step [428/704], Loss: 0.5317\n",
      "Epoch [4/20], Step [429/704], Loss: 0.6368\n",
      "Epoch [4/20], Step [430/704], Loss: 0.6004\n",
      "Epoch [4/20], Step [431/704], Loss: 0.5848\n",
      "Epoch [4/20], Step [432/704], Loss: 0.5401\n",
      "Epoch [4/20], Step [433/704], Loss: 0.6420\n",
      "Epoch [4/20], Step [434/704], Loss: 0.6531\n",
      "Epoch [4/20], Step [435/704], Loss: 0.6362\n",
      "Epoch [4/20], Step [436/704], Loss: 0.4763\n",
      "Epoch [4/20], Step [437/704], Loss: 0.6353\n",
      "Epoch [4/20], Step [438/704], Loss: 0.3743\n",
      "Epoch [4/20], Step [439/704], Loss: 0.4332\n",
      "Epoch [4/20], Step [440/704], Loss: 0.5121\n",
      "Epoch [4/20], Step [441/704], Loss: 0.6220\n",
      "Epoch [4/20], Step [442/704], Loss: 0.5727\n",
      "Epoch [4/20], Step [443/704], Loss: 0.4813\n",
      "Epoch [4/20], Step [444/704], Loss: 0.4869\n",
      "Epoch [4/20], Step [445/704], Loss: 0.6770\n",
      "Epoch [4/20], Step [446/704], Loss: 0.5459\n",
      "Epoch [4/20], Step [447/704], Loss: 0.6583\n",
      "Epoch [4/20], Step [448/704], Loss: 0.6034\n",
      "Epoch [4/20], Step [449/704], Loss: 0.5354\n",
      "Epoch [4/20], Step [450/704], Loss: 0.5792\n",
      "Epoch [4/20], Step [451/704], Loss: 0.6994\n",
      "Epoch [4/20], Step [452/704], Loss: 0.4712\n",
      "Epoch [4/20], Step [453/704], Loss: 0.3404\n",
      "Epoch [4/20], Step [454/704], Loss: 0.4187\n",
      "Epoch [4/20], Step [455/704], Loss: 0.5876\n",
      "Epoch [4/20], Step [456/704], Loss: 0.4235\n",
      "Epoch [4/20], Step [457/704], Loss: 0.5054\n",
      "Epoch [4/20], Step [458/704], Loss: 0.5528\n",
      "Epoch [4/20], Step [459/704], Loss: 0.6440\n",
      "Epoch [4/20], Step [460/704], Loss: 0.6874\n",
      "Epoch [4/20], Step [461/704], Loss: 0.5667\n",
      "Epoch [4/20], Step [462/704], Loss: 0.5811\n",
      "Epoch [4/20], Step [463/704], Loss: 0.4879\n",
      "Epoch [4/20], Step [464/704], Loss: 0.6811\n",
      "Epoch [4/20], Step [465/704], Loss: 0.6675\n",
      "Epoch [4/20], Step [466/704], Loss: 0.4193\n",
      "Epoch [4/20], Step [467/704], Loss: 0.4288\n",
      "Epoch [4/20], Step [468/704], Loss: 0.4580\n",
      "Epoch [4/20], Step [469/704], Loss: 0.4800\n",
      "Epoch [4/20], Step [470/704], Loss: 0.7035\n",
      "Epoch [4/20], Step [471/704], Loss: 0.7026\n",
      "Epoch [4/20], Step [472/704], Loss: 0.6176\n",
      "Epoch [4/20], Step [473/704], Loss: 0.5942\n",
      "Epoch [4/20], Step [474/704], Loss: 0.5144\n",
      "Epoch [4/20], Step [475/704], Loss: 0.5620\n",
      "Epoch [4/20], Step [476/704], Loss: 0.4741\n",
      "Epoch [4/20], Step [477/704], Loss: 0.5643\n",
      "Epoch [4/20], Step [478/704], Loss: 0.4692\n",
      "Epoch [4/20], Step [479/704], Loss: 0.7889\n",
      "Epoch [4/20], Step [480/704], Loss: 0.4339\n",
      "Epoch [4/20], Step [481/704], Loss: 0.5730\n",
      "Epoch [4/20], Step [482/704], Loss: 1.0195\n",
      "Epoch [4/20], Step [483/704], Loss: 0.4050\n",
      "Epoch [4/20], Step [484/704], Loss: 0.4405\n",
      "Epoch [4/20], Step [485/704], Loss: 0.4123\n",
      "Epoch [4/20], Step [486/704], Loss: 0.5740\n",
      "Epoch [4/20], Step [487/704], Loss: 0.4827\n",
      "Epoch [4/20], Step [488/704], Loss: 0.7156\n",
      "Epoch [4/20], Step [489/704], Loss: 0.5778\n",
      "Epoch [4/20], Step [490/704], Loss: 0.6654\n",
      "Epoch [4/20], Step [491/704], Loss: 0.3900\n",
      "Epoch [4/20], Step [492/704], Loss: 0.3902\n",
      "Epoch [4/20], Step [493/704], Loss: 0.4680\n",
      "Epoch [4/20], Step [494/704], Loss: 0.5893\n",
      "Epoch [4/20], Step [495/704], Loss: 0.4391\n",
      "Epoch [4/20], Step [496/704], Loss: 0.2553\n",
      "Epoch [4/20], Step [497/704], Loss: 0.5514\n",
      "Epoch [4/20], Step [498/704], Loss: 0.6084\n",
      "Epoch [4/20], Step [499/704], Loss: 0.6313\n",
      "Epoch [4/20], Step [500/704], Loss: 0.6810\n",
      "Epoch [4/20], Step [501/704], Loss: 0.4981\n",
      "Epoch [4/20], Step [502/704], Loss: 0.3914\n",
      "Epoch [4/20], Step [503/704], Loss: 0.3945\n",
      "Epoch [4/20], Step [504/704], Loss: 0.7205\n",
      "Epoch [4/20], Step [505/704], Loss: 0.5448\n",
      "Epoch [4/20], Step [506/704], Loss: 0.5560\n",
      "Epoch [4/20], Step [507/704], Loss: 0.6065\n",
      "Epoch [4/20], Step [508/704], Loss: 0.7290\n",
      "Epoch [4/20], Step [509/704], Loss: 0.7085\n",
      "Epoch [4/20], Step [510/704], Loss: 0.5362\n",
      "Epoch [4/20], Step [511/704], Loss: 0.4549\n",
      "Epoch [4/20], Step [512/704], Loss: 0.4794\n",
      "Epoch [4/20], Step [513/704], Loss: 0.5110\n",
      "Epoch [4/20], Step [514/704], Loss: 0.5549\n",
      "Epoch [4/20], Step [515/704], Loss: 0.4468\n",
      "Epoch [4/20], Step [516/704], Loss: 0.8305\n",
      "Epoch [4/20], Step [517/704], Loss: 0.4805\n",
      "Epoch [4/20], Step [518/704], Loss: 0.6290\n",
      "Epoch [4/20], Step [519/704], Loss: 0.3943\n",
      "Epoch [4/20], Step [520/704], Loss: 0.4974\n",
      "Epoch [4/20], Step [521/704], Loss: 0.5250\n",
      "Epoch [4/20], Step [522/704], Loss: 0.5838\n",
      "Epoch [4/20], Step [523/704], Loss: 0.4346\n",
      "Epoch [4/20], Step [524/704], Loss: 0.5116\n",
      "Epoch [4/20], Step [525/704], Loss: 0.5764\n",
      "Epoch [4/20], Step [526/704], Loss: 0.4314\n",
      "Epoch [4/20], Step [527/704], Loss: 0.6015\n",
      "Epoch [4/20], Step [528/704], Loss: 0.5347\n",
      "Epoch [4/20], Step [529/704], Loss: 0.3953\n",
      "Epoch [4/20], Step [530/704], Loss: 0.4021\n",
      "Epoch [4/20], Step [531/704], Loss: 0.5109\n",
      "Epoch [4/20], Step [532/704], Loss: 0.6883\n",
      "Epoch [4/20], Step [533/704], Loss: 0.4350\n",
      "Epoch [4/20], Step [534/704], Loss: 0.4116\n",
      "Epoch [4/20], Step [535/704], Loss: 0.4547\n",
      "Epoch [4/20], Step [536/704], Loss: 0.5222\n",
      "Epoch [4/20], Step [537/704], Loss: 0.4595\n",
      "Epoch [4/20], Step [538/704], Loss: 0.5480\n",
      "Epoch [4/20], Step [539/704], Loss: 0.8005\n",
      "Epoch [4/20], Step [540/704], Loss: 0.3677\n",
      "Epoch [4/20], Step [541/704], Loss: 0.5911\n",
      "Epoch [4/20], Step [542/704], Loss: 0.5386\n",
      "Epoch [4/20], Step [543/704], Loss: 0.4342\n",
      "Epoch [4/20], Step [544/704], Loss: 0.6014\n",
      "Epoch [4/20], Step [545/704], Loss: 0.6364\n",
      "Epoch [4/20], Step [546/704], Loss: 0.8351\n",
      "Epoch [4/20], Step [547/704], Loss: 0.5433\n",
      "Epoch [4/20], Step [548/704], Loss: 0.4344\n",
      "Epoch [4/20], Step [549/704], Loss: 0.5712\n",
      "Epoch [4/20], Step [550/704], Loss: 0.4444\n",
      "Epoch [4/20], Step [551/704], Loss: 0.5066\n",
      "Epoch [4/20], Step [552/704], Loss: 0.5831\n",
      "Epoch [4/20], Step [553/704], Loss: 0.7433\n",
      "Epoch [4/20], Step [554/704], Loss: 0.4986\n",
      "Epoch [4/20], Step [555/704], Loss: 0.6852\n",
      "Epoch [4/20], Step [556/704], Loss: 0.5220\n",
      "Epoch [4/20], Step [557/704], Loss: 0.9388\n",
      "Epoch [4/20], Step [558/704], Loss: 0.5707\n",
      "Epoch [4/20], Step [559/704], Loss: 0.7901\n",
      "Epoch [4/20], Step [560/704], Loss: 0.6984\n",
      "Epoch [4/20], Step [561/704], Loss: 0.6507\n",
      "Epoch [4/20], Step [562/704], Loss: 0.6128\n",
      "Epoch [4/20], Step [563/704], Loss: 0.6698\n",
      "Epoch [4/20], Step [564/704], Loss: 0.6244\n",
      "Epoch [4/20], Step [565/704], Loss: 0.4866\n",
      "Epoch [4/20], Step [566/704], Loss: 0.5740\n",
      "Epoch [4/20], Step [567/704], Loss: 0.5581\n",
      "Epoch [4/20], Step [568/704], Loss: 0.7654\n",
      "Epoch [4/20], Step [569/704], Loss: 0.5187\n",
      "Epoch [4/20], Step [570/704], Loss: 0.3725\n",
      "Epoch [4/20], Step [571/704], Loss: 0.6304\n",
      "Epoch [4/20], Step [572/704], Loss: 0.5174\n",
      "Epoch [4/20], Step [573/704], Loss: 0.3492\n",
      "Epoch [4/20], Step [574/704], Loss: 0.7147\n",
      "Epoch [4/20], Step [575/704], Loss: 0.4781\n",
      "Epoch [4/20], Step [576/704], Loss: 0.7091\n",
      "Epoch [4/20], Step [577/704], Loss: 0.5257\n",
      "Epoch [4/20], Step [578/704], Loss: 0.6620\n",
      "Epoch [4/20], Step [579/704], Loss: 0.4921\n",
      "Epoch [4/20], Step [580/704], Loss: 0.6213\n",
      "Epoch [4/20], Step [581/704], Loss: 0.6231\n",
      "Epoch [4/20], Step [582/704], Loss: 0.5699\n",
      "Epoch [4/20], Step [583/704], Loss: 0.4301\n",
      "Epoch [4/20], Step [584/704], Loss: 0.4462\n",
      "Epoch [4/20], Step [585/704], Loss: 0.5286\n",
      "Epoch [4/20], Step [586/704], Loss: 0.4702\n",
      "Epoch [4/20], Step [587/704], Loss: 0.6171\n",
      "Epoch [4/20], Step [588/704], Loss: 0.4420\n",
      "Epoch [4/20], Step [589/704], Loss: 0.4917\n",
      "Epoch [4/20], Step [590/704], Loss: 0.5226\n",
      "Epoch [4/20], Step [591/704], Loss: 0.5194\n",
      "Epoch [4/20], Step [592/704], Loss: 0.4758\n",
      "Epoch [4/20], Step [593/704], Loss: 0.5531\n",
      "Epoch [4/20], Step [594/704], Loss: 0.4455\n",
      "Epoch [4/20], Step [595/704], Loss: 0.6377\n",
      "Epoch [4/20], Step [596/704], Loss: 0.5687\n",
      "Epoch [4/20], Step [597/704], Loss: 0.4407\n",
      "Epoch [4/20], Step [598/704], Loss: 0.5430\n",
      "Epoch [4/20], Step [599/704], Loss: 0.6261\n",
      "Epoch [4/20], Step [600/704], Loss: 0.3567\n",
      "Epoch [4/20], Step [601/704], Loss: 0.4933\n",
      "Epoch [4/20], Step [602/704], Loss: 0.7488\n",
      "Epoch [4/20], Step [603/704], Loss: 0.5876\n",
      "Epoch [4/20], Step [604/704], Loss: 0.6521\n",
      "Epoch [4/20], Step [605/704], Loss: 0.4536\n",
      "Epoch [4/20], Step [606/704], Loss: 0.5105\n",
      "Epoch [4/20], Step [607/704], Loss: 0.4128\n",
      "Epoch [4/20], Step [608/704], Loss: 0.4131\n",
      "Epoch [4/20], Step [609/704], Loss: 0.6446\n",
      "Epoch [4/20], Step [610/704], Loss: 0.6899\n",
      "Epoch [4/20], Step [611/704], Loss: 0.4956\n",
      "Epoch [4/20], Step [612/704], Loss: 0.4532\n",
      "Epoch [4/20], Step [613/704], Loss: 0.5682\n",
      "Epoch [4/20], Step [614/704], Loss: 0.4137\n",
      "Epoch [4/20], Step [615/704], Loss: 0.6210\n",
      "Epoch [4/20], Step [616/704], Loss: 0.5612\n",
      "Epoch [4/20], Step [617/704], Loss: 0.3537\n",
      "Epoch [4/20], Step [618/704], Loss: 0.4314\n",
      "Epoch [4/20], Step [619/704], Loss: 0.5162\n",
      "Epoch [4/20], Step [620/704], Loss: 0.5184\n",
      "Epoch [4/20], Step [621/704], Loss: 0.5292\n",
      "Epoch [4/20], Step [622/704], Loss: 0.6403\n",
      "Epoch [4/20], Step [623/704], Loss: 0.5778\n",
      "Epoch [4/20], Step [624/704], Loss: 0.5841\n",
      "Epoch [4/20], Step [625/704], Loss: 0.4977\n",
      "Epoch [4/20], Step [626/704], Loss: 0.5002\n",
      "Epoch [4/20], Step [627/704], Loss: 0.4744\n",
      "Epoch [4/20], Step [628/704], Loss: 0.4966\n",
      "Epoch [4/20], Step [629/704], Loss: 0.6376\n",
      "Epoch [4/20], Step [630/704], Loss: 0.5480\n",
      "Epoch [4/20], Step [631/704], Loss: 0.5856\n",
      "Epoch [4/20], Step [632/704], Loss: 0.5878\n",
      "Epoch [4/20], Step [633/704], Loss: 0.4775\n",
      "Epoch [4/20], Step [634/704], Loss: 0.5438\n",
      "Epoch [4/20], Step [635/704], Loss: 0.5636\n",
      "Epoch [4/20], Step [636/704], Loss: 0.3571\n",
      "Epoch [4/20], Step [637/704], Loss: 0.7365\n",
      "Epoch [4/20], Step [638/704], Loss: 0.5999\n",
      "Epoch [4/20], Step [639/704], Loss: 0.2386\n",
      "Epoch [4/20], Step [640/704], Loss: 0.4275\n",
      "Epoch [4/20], Step [641/704], Loss: 0.5791\n",
      "Epoch [4/20], Step [642/704], Loss: 0.4999\n",
      "Epoch [4/20], Step [643/704], Loss: 0.3868\n",
      "Epoch [4/20], Step [644/704], Loss: 0.6111\n",
      "Epoch [4/20], Step [645/704], Loss: 0.5313\n",
      "Epoch [4/20], Step [646/704], Loss: 0.5884\n",
      "Epoch [4/20], Step [647/704], Loss: 0.6929\n",
      "Epoch [4/20], Step [648/704], Loss: 0.4518\n",
      "Epoch [4/20], Step [649/704], Loss: 0.2921\n",
      "Epoch [4/20], Step [650/704], Loss: 0.4718\n",
      "Epoch [4/20], Step [651/704], Loss: 0.7856\n",
      "Epoch [4/20], Step [652/704], Loss: 0.5102\n",
      "Epoch [4/20], Step [653/704], Loss: 0.5316\n",
      "Epoch [4/20], Step [654/704], Loss: 0.5156\n",
      "Epoch [4/20], Step [655/704], Loss: 0.4154\n",
      "Epoch [4/20], Step [656/704], Loss: 0.5088\n",
      "Epoch [4/20], Step [657/704], Loss: 0.5995\n",
      "Epoch [4/20], Step [658/704], Loss: 0.3127\n",
      "Epoch [4/20], Step [659/704], Loss: 0.3107\n",
      "Epoch [4/20], Step [660/704], Loss: 0.5674\n",
      "Epoch [4/20], Step [661/704], Loss: 0.6156\n",
      "Epoch [4/20], Step [662/704], Loss: 0.8525\n",
      "Epoch [4/20], Step [663/704], Loss: 0.4608\n",
      "Epoch [4/20], Step [664/704], Loss: 0.4678\n",
      "Epoch [4/20], Step [665/704], Loss: 0.5291\n",
      "Epoch [4/20], Step [666/704], Loss: 0.6197\n",
      "Epoch [4/20], Step [667/704], Loss: 0.7201\n",
      "Epoch [4/20], Step [668/704], Loss: 0.5095\n",
      "Epoch [4/20], Step [669/704], Loss: 0.5686\n",
      "Epoch [4/20], Step [670/704], Loss: 0.5537\n",
      "Epoch [4/20], Step [671/704], Loss: 0.4904\n",
      "Epoch [4/20], Step [672/704], Loss: 0.4290\n",
      "Epoch [4/20], Step [673/704], Loss: 0.5962\n",
      "Epoch [4/20], Step [674/704], Loss: 0.5319\n",
      "Epoch [4/20], Step [675/704], Loss: 0.5163\n",
      "Epoch [4/20], Step [676/704], Loss: 0.6180\n",
      "Epoch [4/20], Step [677/704], Loss: 0.3656\n",
      "Epoch [4/20], Step [678/704], Loss: 0.6688\n",
      "Epoch [4/20], Step [679/704], Loss: 0.3688\n",
      "Epoch [4/20], Step [680/704], Loss: 0.3381\n",
      "Epoch [4/20], Step [681/704], Loss: 0.6410\n",
      "Epoch [4/20], Step [682/704], Loss: 0.3351\n",
      "Epoch [4/20], Step [683/704], Loss: 0.6314\n",
      "Epoch [4/20], Step [684/704], Loss: 0.7073\n",
      "Epoch [4/20], Step [685/704], Loss: 0.5112\n",
      "Epoch [4/20], Step [686/704], Loss: 0.5947\n",
      "Epoch [4/20], Step [687/704], Loss: 0.5287\n",
      "Epoch [4/20], Step [688/704], Loss: 0.6226\n",
      "Epoch [4/20], Step [689/704], Loss: 0.5535\n",
      "Epoch [4/20], Step [690/704], Loss: 0.4882\n",
      "Epoch [4/20], Step [691/704], Loss: 0.4990\n",
      "Epoch [4/20], Step [692/704], Loss: 0.5105\n",
      "Epoch [4/20], Step [693/704], Loss: 0.6212\n",
      "Epoch [4/20], Step [694/704], Loss: 0.3648\n",
      "Epoch [4/20], Step [695/704], Loss: 0.6664\n",
      "Epoch [4/20], Step [696/704], Loss: 0.5431\n",
      "Epoch [4/20], Step [697/704], Loss: 0.7006\n",
      "Epoch [4/20], Step [698/704], Loss: 0.8453\n",
      "Epoch [4/20], Step [699/704], Loss: 0.6519\n",
      "Epoch [4/20], Step [700/704], Loss: 0.7051\n",
      "Epoch [4/20], Step [701/704], Loss: 0.5144\n",
      "Epoch [4/20], Step [702/704], Loss: 0.4215\n",
      "Epoch [4/20], Step [703/704], Loss: 0.5999\n",
      "Epoch [4/20], Step [704/704], Loss: 1.4943\n",
      "Epoch [4/20], Loss: 1.4943\n",
      "Epoch [5/20], Step [1/704], Loss: 0.3810\n",
      "Epoch [5/20], Step [2/704], Loss: 0.4418\n",
      "Epoch [5/20], Step [3/704], Loss: 0.7311\n",
      "Epoch [5/20], Step [4/704], Loss: 0.8870\n",
      "Epoch [5/20], Step [5/704], Loss: 0.8442\n",
      "Epoch [5/20], Step [6/704], Loss: 0.5725\n",
      "Epoch [5/20], Step [7/704], Loss: 0.6308\n",
      "Epoch [5/20], Step [8/704], Loss: 0.7084\n",
      "Epoch [5/20], Step [9/704], Loss: 0.7250\n",
      "Epoch [5/20], Step [10/704], Loss: 0.5746\n",
      "Epoch [5/20], Step [11/704], Loss: 0.9267\n",
      "Epoch [5/20], Step [12/704], Loss: 0.6080\n",
      "Epoch [5/20], Step [13/704], Loss: 0.5989\n",
      "Epoch [5/20], Step [14/704], Loss: 0.8964\n",
      "Epoch [5/20], Step [15/704], Loss: 0.7904\n",
      "Epoch [5/20], Step [16/704], Loss: 0.3323\n",
      "Epoch [5/20], Step [17/704], Loss: 0.3623\n",
      "Epoch [5/20], Step [18/704], Loss: 0.7192\n",
      "Epoch [5/20], Step [19/704], Loss: 0.6932\n",
      "Epoch [5/20], Step [20/704], Loss: 0.5640\n",
      "Epoch [5/20], Step [21/704], Loss: 0.5828\n",
      "Epoch [5/20], Step [22/704], Loss: 0.3603\n",
      "Epoch [5/20], Step [23/704], Loss: 0.6675\n",
      "Epoch [5/20], Step [24/704], Loss: 0.5526\n",
      "Epoch [5/20], Step [25/704], Loss: 0.5546\n",
      "Epoch [5/20], Step [26/704], Loss: 0.6936\n",
      "Epoch [5/20], Step [27/704], Loss: 0.5652\n",
      "Epoch [5/20], Step [28/704], Loss: 0.4069\n",
      "Epoch [5/20], Step [29/704], Loss: 0.5970\n",
      "Epoch [5/20], Step [30/704], Loss: 0.3383\n",
      "Epoch [5/20], Step [31/704], Loss: 0.4191\n",
      "Epoch [5/20], Step [32/704], Loss: 0.5640\n",
      "Epoch [5/20], Step [33/704], Loss: 0.6789\n",
      "Epoch [5/20], Step [34/704], Loss: 0.4337\n",
      "Epoch [5/20], Step [35/704], Loss: 0.6353\n",
      "Epoch [5/20], Step [36/704], Loss: 0.5923\n",
      "Epoch [5/20], Step [37/704], Loss: 0.4325\n",
      "Epoch [5/20], Step [38/704], Loss: 0.5010\n",
      "Epoch [5/20], Step [39/704], Loss: 0.4159\n",
      "Epoch [5/20], Step [40/704], Loss: 0.5996\n",
      "Epoch [5/20], Step [41/704], Loss: 0.8337\n",
      "Epoch [5/20], Step [42/704], Loss: 0.3720\n",
      "Epoch [5/20], Step [43/704], Loss: 0.2175\n",
      "Epoch [5/20], Step [44/704], Loss: 0.5041\n",
      "Epoch [5/20], Step [45/704], Loss: 0.3485\n",
      "Epoch [5/20], Step [46/704], Loss: 0.5712\n",
      "Epoch [5/20], Step [47/704], Loss: 0.4513\n",
      "Epoch [5/20], Step [48/704], Loss: 0.6168\n",
      "Epoch [5/20], Step [49/704], Loss: 0.6261\n",
      "Epoch [5/20], Step [50/704], Loss: 0.6274\n",
      "Epoch [5/20], Step [51/704], Loss: 0.4743\n",
      "Epoch [5/20], Step [52/704], Loss: 0.4541\n",
      "Epoch [5/20], Step [53/704], Loss: 0.3572\n",
      "Epoch [5/20], Step [54/704], Loss: 0.5275\n",
      "Epoch [5/20], Step [55/704], Loss: 0.3629\n",
      "Epoch [5/20], Step [56/704], Loss: 0.4116\n",
      "Epoch [5/20], Step [57/704], Loss: 0.3315\n",
      "Epoch [5/20], Step [58/704], Loss: 0.5063\n",
      "Epoch [5/20], Step [59/704], Loss: 0.4913\n",
      "Epoch [5/20], Step [60/704], Loss: 0.3611\n",
      "Epoch [5/20], Step [61/704], Loss: 0.6192\n",
      "Epoch [5/20], Step [62/704], Loss: 0.5835\n",
      "Epoch [5/20], Step [63/704], Loss: 0.4506\n",
      "Epoch [5/20], Step [64/704], Loss: 0.8045\n",
      "Epoch [5/20], Step [65/704], Loss: 0.5345\n",
      "Epoch [5/20], Step [66/704], Loss: 0.4764\n",
      "Epoch [5/20], Step [67/704], Loss: 0.4422\n",
      "Epoch [5/20], Step [68/704], Loss: 0.4420\n",
      "Epoch [5/20], Step [69/704], Loss: 0.4261\n",
      "Epoch [5/20], Step [70/704], Loss: 0.8046\n",
      "Epoch [5/20], Step [71/704], Loss: 0.3516\n",
      "Epoch [5/20], Step [72/704], Loss: 0.4136\n",
      "Epoch [5/20], Step [73/704], Loss: 0.3392\n",
      "Epoch [5/20], Step [74/704], Loss: 0.5493\n",
      "Epoch [5/20], Step [75/704], Loss: 0.3630\n",
      "Epoch [5/20], Step [76/704], Loss: 0.4631\n",
      "Epoch [5/20], Step [77/704], Loss: 0.4819\n",
      "Epoch [5/20], Step [78/704], Loss: 0.3160\n",
      "Epoch [5/20], Step [79/704], Loss: 0.4604\n",
      "Epoch [5/20], Step [80/704], Loss: 0.4367\n",
      "Epoch [5/20], Step [81/704], Loss: 0.5657\n",
      "Epoch [5/20], Step [82/704], Loss: 0.4394\n",
      "Epoch [5/20], Step [83/704], Loss: 0.3205\n",
      "Epoch [5/20], Step [84/704], Loss: 0.4147\n",
      "Epoch [5/20], Step [85/704], Loss: 0.6879\n",
      "Epoch [5/20], Step [86/704], Loss: 0.5484\n",
      "Epoch [5/20], Step [87/704], Loss: 0.2995\n",
      "Epoch [5/20], Step [88/704], Loss: 0.3197\n",
      "Epoch [5/20], Step [89/704], Loss: 0.5480\n",
      "Epoch [5/20], Step [90/704], Loss: 0.5856\n",
      "Epoch [5/20], Step [91/704], Loss: 0.5888\n",
      "Epoch [5/20], Step [92/704], Loss: 0.4289\n",
      "Epoch [5/20], Step [93/704], Loss: 0.3325\n",
      "Epoch [5/20], Step [94/704], Loss: 0.4046\n",
      "Epoch [5/20], Step [95/704], Loss: 0.3215\n",
      "Epoch [5/20], Step [96/704], Loss: 0.4968\n",
      "Epoch [5/20], Step [97/704], Loss: 0.3842\n",
      "Epoch [5/20], Step [98/704], Loss: 0.5099\n",
      "Epoch [5/20], Step [99/704], Loss: 0.4174\n",
      "Epoch [5/20], Step [100/704], Loss: 0.2974\n",
      "Epoch [5/20], Step [101/704], Loss: 0.4252\n",
      "Epoch [5/20], Step [102/704], Loss: 0.7044\n",
      "Epoch [5/20], Step [103/704], Loss: 0.3982\n",
      "Epoch [5/20], Step [104/704], Loss: 0.5756\n",
      "Epoch [5/20], Step [105/704], Loss: 0.3632\n",
      "Epoch [5/20], Step [106/704], Loss: 0.5608\n",
      "Epoch [5/20], Step [107/704], Loss: 0.2287\n",
      "Epoch [5/20], Step [108/704], Loss: 0.4695\n",
      "Epoch [5/20], Step [109/704], Loss: 0.4059\n",
      "Epoch [5/20], Step [110/704], Loss: 0.4334\n",
      "Epoch [5/20], Step [111/704], Loss: 0.4756\n",
      "Epoch [5/20], Step [112/704], Loss: 0.4540\n",
      "Epoch [5/20], Step [113/704], Loss: 0.3380\n",
      "Epoch [5/20], Step [114/704], Loss: 0.3904\n",
      "Epoch [5/20], Step [115/704], Loss: 0.5665\n",
      "Epoch [5/20], Step [116/704], Loss: 0.3445\n",
      "Epoch [5/20], Step [117/704], Loss: 0.4930\n",
      "Epoch [5/20], Step [118/704], Loss: 0.3932\n",
      "Epoch [5/20], Step [119/704], Loss: 0.7514\n",
      "Epoch [5/20], Step [120/704], Loss: 0.5442\n",
      "Epoch [5/20], Step [121/704], Loss: 0.4981\n",
      "Epoch [5/20], Step [122/704], Loss: 0.3892\n",
      "Epoch [5/20], Step [123/704], Loss: 0.4321\n",
      "Epoch [5/20], Step [124/704], Loss: 0.4467\n",
      "Epoch [5/20], Step [125/704], Loss: 0.3593\n",
      "Epoch [5/20], Step [126/704], Loss: 0.3800\n",
      "Epoch [5/20], Step [127/704], Loss: 0.3932\n",
      "Epoch [5/20], Step [128/704], Loss: 0.5410\n",
      "Epoch [5/20], Step [129/704], Loss: 0.4783\n",
      "Epoch [5/20], Step [130/704], Loss: 0.5673\n",
      "Epoch [5/20], Step [131/704], Loss: 0.3620\n",
      "Epoch [5/20], Step [132/704], Loss: 0.4636\n",
      "Epoch [5/20], Step [133/704], Loss: 0.4058\n",
      "Epoch [5/20], Step [134/704], Loss: 0.2705\n",
      "Epoch [5/20], Step [135/704], Loss: 0.4649\n",
      "Epoch [5/20], Step [136/704], Loss: 0.4127\n",
      "Epoch [5/20], Step [137/704], Loss: 0.3987\n",
      "Epoch [5/20], Step [138/704], Loss: 0.6640\n",
      "Epoch [5/20], Step [139/704], Loss: 0.5307\n",
      "Epoch [5/20], Step [140/704], Loss: 0.6273\n",
      "Epoch [5/20], Step [141/704], Loss: 0.6432\n",
      "Epoch [5/20], Step [142/704], Loss: 0.4109\n",
      "Epoch [5/20], Step [143/704], Loss: 0.3391\n",
      "Epoch [5/20], Step [144/704], Loss: 0.4845\n",
      "Epoch [5/20], Step [145/704], Loss: 0.7203\n",
      "Epoch [5/20], Step [146/704], Loss: 0.4367\n",
      "Epoch [5/20], Step [147/704], Loss: 0.5069\n",
      "Epoch [5/20], Step [148/704], Loss: 0.5017\n",
      "Epoch [5/20], Step [149/704], Loss: 0.3393\n",
      "Epoch [5/20], Step [150/704], Loss: 0.5656\n",
      "Epoch [5/20], Step [151/704], Loss: 0.5168\n",
      "Epoch [5/20], Step [152/704], Loss: 0.3579\n",
      "Epoch [5/20], Step [153/704], Loss: 0.5676\n",
      "Epoch [5/20], Step [154/704], Loss: 0.3935\n",
      "Epoch [5/20], Step [155/704], Loss: 0.4262\n",
      "Epoch [5/20], Step [156/704], Loss: 0.4993\n",
      "Epoch [5/20], Step [157/704], Loss: 0.3684\n",
      "Epoch [5/20], Step [158/704], Loss: 0.4935\n",
      "Epoch [5/20], Step [159/704], Loss: 0.5190\n",
      "Epoch [5/20], Step [160/704], Loss: 0.8077\n",
      "Epoch [5/20], Step [161/704], Loss: 0.2965\n",
      "Epoch [5/20], Step [162/704], Loss: 0.2992\n",
      "Epoch [5/20], Step [163/704], Loss: 0.3578\n",
      "Epoch [5/20], Step [164/704], Loss: 0.3432\n",
      "Epoch [5/20], Step [165/704], Loss: 0.7065\n",
      "Epoch [5/20], Step [166/704], Loss: 0.7527\n",
      "Epoch [5/20], Step [167/704], Loss: 0.3668\n",
      "Epoch [5/20], Step [168/704], Loss: 0.5021\n",
      "Epoch [5/20], Step [169/704], Loss: 0.3349\n",
      "Epoch [5/20], Step [170/704], Loss: 0.1852\n",
      "Epoch [5/20], Step [171/704], Loss: 0.5269\n",
      "Epoch [5/20], Step [172/704], Loss: 0.3784\n",
      "Epoch [5/20], Step [173/704], Loss: 0.5399\n",
      "Epoch [5/20], Step [174/704], Loss: 0.3420\n",
      "Epoch [5/20], Step [175/704], Loss: 0.8043\n",
      "Epoch [5/20], Step [176/704], Loss: 0.6483\n",
      "Epoch [5/20], Step [177/704], Loss: 0.5097\n",
      "Epoch [5/20], Step [178/704], Loss: 0.2288\n",
      "Epoch [5/20], Step [179/704], Loss: 0.5414\n",
      "Epoch [5/20], Step [180/704], Loss: 0.4016\n",
      "Epoch [5/20], Step [181/704], Loss: 0.5755\n",
      "Epoch [5/20], Step [182/704], Loss: 0.2684\n",
      "Epoch [5/20], Step [183/704], Loss: 0.5962\n",
      "Epoch [5/20], Step [184/704], Loss: 0.7068\n",
      "Epoch [5/20], Step [185/704], Loss: 0.5922\n",
      "Epoch [5/20], Step [186/704], Loss: 0.5060\n",
      "Epoch [5/20], Step [187/704], Loss: 0.5324\n",
      "Epoch [5/20], Step [188/704], Loss: 0.4419\n",
      "Epoch [5/20], Step [189/704], Loss: 0.4855\n",
      "Epoch [5/20], Step [190/704], Loss: 0.5407\n",
      "Epoch [5/20], Step [191/704], Loss: 0.4267\n",
      "Epoch [5/20], Step [192/704], Loss: 0.4110\n",
      "Epoch [5/20], Step [193/704], Loss: 0.3833\n",
      "Epoch [5/20], Step [194/704], Loss: 0.4319\n",
      "Epoch [5/20], Step [195/704], Loss: 0.4011\n",
      "Epoch [5/20], Step [196/704], Loss: 0.2812\n",
      "Epoch [5/20], Step [197/704], Loss: 0.4250\n",
      "Epoch [5/20], Step [198/704], Loss: 0.5989\n",
      "Epoch [5/20], Step [199/704], Loss: 0.4421\n",
      "Epoch [5/20], Step [200/704], Loss: 0.3071\n",
      "Epoch [5/20], Step [201/704], Loss: 0.4162\n",
      "Epoch [5/20], Step [202/704], Loss: 0.4452\n",
      "Epoch [5/20], Step [203/704], Loss: 0.4617\n",
      "Epoch [5/20], Step [204/704], Loss: 0.5192\n",
      "Epoch [5/20], Step [205/704], Loss: 0.3744\n",
      "Epoch [5/20], Step [206/704], Loss: 0.3125\n",
      "Epoch [5/20], Step [207/704], Loss: 0.5699\n",
      "Epoch [5/20], Step [208/704], Loss: 0.4080\n",
      "Epoch [5/20], Step [209/704], Loss: 0.4693\n",
      "Epoch [5/20], Step [210/704], Loss: 0.4272\n",
      "Epoch [5/20], Step [211/704], Loss: 0.2505\n",
      "Epoch [5/20], Step [212/704], Loss: 0.3638\n",
      "Epoch [5/20], Step [213/704], Loss: 0.5664\n",
      "Epoch [5/20], Step [214/704], Loss: 0.5644\n",
      "Epoch [5/20], Step [215/704], Loss: 0.2856\n",
      "Epoch [5/20], Step [216/704], Loss: 0.4174\n",
      "Epoch [5/20], Step [217/704], Loss: 0.4752\n",
      "Epoch [5/20], Step [218/704], Loss: 0.2832\n",
      "Epoch [5/20], Step [219/704], Loss: 0.4175\n",
      "Epoch [5/20], Step [220/704], Loss: 0.3126\n",
      "Epoch [5/20], Step [221/704], Loss: 0.2125\n",
      "Epoch [5/20], Step [222/704], Loss: 0.6066\n",
      "Epoch [5/20], Step [223/704], Loss: 0.4835\n",
      "Epoch [5/20], Step [224/704], Loss: 0.4993\n",
      "Epoch [5/20], Step [225/704], Loss: 0.5279\n",
      "Epoch [5/20], Step [226/704], Loss: 0.4555\n",
      "Epoch [5/20], Step [227/704], Loss: 0.5055\n",
      "Epoch [5/20], Step [228/704], Loss: 0.5665\n",
      "Epoch [5/20], Step [229/704], Loss: 0.4321\n",
      "Epoch [5/20], Step [230/704], Loss: 0.3211\n",
      "Epoch [5/20], Step [231/704], Loss: 0.6015\n",
      "Epoch [5/20], Step [232/704], Loss: 0.5341\n",
      "Epoch [5/20], Step [233/704], Loss: 0.4949\n",
      "Epoch [5/20], Step [234/704], Loss: 0.4830\n",
      "Epoch [5/20], Step [235/704], Loss: 0.3672\n",
      "Epoch [5/20], Step [236/704], Loss: 0.6114\n",
      "Epoch [5/20], Step [237/704], Loss: 0.3452\n",
      "Epoch [5/20], Step [238/704], Loss: 0.3697\n",
      "Epoch [5/20], Step [239/704], Loss: 0.3510\n",
      "Epoch [5/20], Step [240/704], Loss: 0.4002\n",
      "Epoch [5/20], Step [241/704], Loss: 0.5267\n",
      "Epoch [5/20], Step [242/704], Loss: 0.3087\n",
      "Epoch [5/20], Step [243/704], Loss: 0.5108\n",
      "Epoch [5/20], Step [244/704], Loss: 0.5299\n",
      "Epoch [5/20], Step [245/704], Loss: 0.4770\n",
      "Epoch [5/20], Step [246/704], Loss: 0.3760\n",
      "Epoch [5/20], Step [247/704], Loss: 0.4039\n",
      "Epoch [5/20], Step [248/704], Loss: 0.5706\n",
      "Epoch [5/20], Step [249/704], Loss: 0.4318\n",
      "Epoch [5/20], Step [250/704], Loss: 0.3667\n",
      "Epoch [5/20], Step [251/704], Loss: 0.3505\n",
      "Epoch [5/20], Step [252/704], Loss: 0.4937\n",
      "Epoch [5/20], Step [253/704], Loss: 0.5771\n",
      "Epoch [5/20], Step [254/704], Loss: 0.5296\n",
      "Epoch [5/20], Step [255/704], Loss: 0.4458\n",
      "Epoch [5/20], Step [256/704], Loss: 0.5129\n",
      "Epoch [5/20], Step [257/704], Loss: 0.3333\n",
      "Epoch [5/20], Step [258/704], Loss: 0.4874\n",
      "Epoch [5/20], Step [259/704], Loss: 0.6570\n",
      "Epoch [5/20], Step [260/704], Loss: 0.6811\n",
      "Epoch [5/20], Step [261/704], Loss: 0.4830\n",
      "Epoch [5/20], Step [262/704], Loss: 0.4170\n",
      "Epoch [5/20], Step [263/704], Loss: 0.4437\n",
      "Epoch [5/20], Step [264/704], Loss: 0.4976\n",
      "Epoch [5/20], Step [265/704], Loss: 0.3363\n",
      "Epoch [5/20], Step [266/704], Loss: 0.4962\n",
      "Epoch [5/20], Step [267/704], Loss: 0.5174\n",
      "Epoch [5/20], Step [268/704], Loss: 0.4940\n",
      "Epoch [5/20], Step [269/704], Loss: 0.3651\n",
      "Epoch [5/20], Step [270/704], Loss: 0.5938\n",
      "Epoch [5/20], Step [271/704], Loss: 0.5501\n",
      "Epoch [5/20], Step [272/704], Loss: 0.4032\n",
      "Epoch [5/20], Step [273/704], Loss: 0.5827\n",
      "Epoch [5/20], Step [274/704], Loss: 0.3348\n",
      "Epoch [5/20], Step [275/704], Loss: 0.5042\n",
      "Epoch [5/20], Step [276/704], Loss: 0.3426\n",
      "Epoch [5/20], Step [277/704], Loss: 0.3078\n",
      "Epoch [5/20], Step [278/704], Loss: 0.5451\n",
      "Epoch [5/20], Step [279/704], Loss: 0.3936\n",
      "Epoch [5/20], Step [280/704], Loss: 0.2995\n",
      "Epoch [5/20], Step [281/704], Loss: 0.5676\n",
      "Epoch [5/20], Step [282/704], Loss: 0.4372\n",
      "Epoch [5/20], Step [283/704], Loss: 0.5242\n",
      "Epoch [5/20], Step [284/704], Loss: 0.7167\n",
      "Epoch [5/20], Step [285/704], Loss: 0.3606\n",
      "Epoch [5/20], Step [286/704], Loss: 0.5839\n",
      "Epoch [5/20], Step [287/704], Loss: 0.4495\n",
      "Epoch [5/20], Step [288/704], Loss: 0.4936\n",
      "Epoch [5/20], Step [289/704], Loss: 0.4497\n",
      "Epoch [5/20], Step [290/704], Loss: 0.2759\n",
      "Epoch [5/20], Step [291/704], Loss: 0.4467\n",
      "Epoch [5/20], Step [292/704], Loss: 0.5834\n",
      "Epoch [5/20], Step [293/704], Loss: 0.4268\n",
      "Epoch [5/20], Step [294/704], Loss: 0.5787\n",
      "Epoch [5/20], Step [295/704], Loss: 0.4111\n",
      "Epoch [5/20], Step [296/704], Loss: 0.5630\n",
      "Epoch [5/20], Step [297/704], Loss: 0.5336\n",
      "Epoch [5/20], Step [298/704], Loss: 0.5042\n",
      "Epoch [5/20], Step [299/704], Loss: 0.4517\n",
      "Epoch [5/20], Step [300/704], Loss: 0.3389\n",
      "Epoch [5/20], Step [301/704], Loss: 0.3313\n",
      "Epoch [5/20], Step [302/704], Loss: 0.4487\n",
      "Epoch [5/20], Step [303/704], Loss: 0.3896\n",
      "Epoch [5/20], Step [304/704], Loss: 0.3356\n",
      "Epoch [5/20], Step [305/704], Loss: 0.3936\n",
      "Epoch [5/20], Step [306/704], Loss: 0.5118\n",
      "Epoch [5/20], Step [307/704], Loss: 0.5221\n",
      "Epoch [5/20], Step [308/704], Loss: 0.3193\n",
      "Epoch [5/20], Step [309/704], Loss: 0.4845\n",
      "Epoch [5/20], Step [310/704], Loss: 0.4011\n",
      "Epoch [5/20], Step [311/704], Loss: 0.6669\n",
      "Epoch [5/20], Step [312/704], Loss: 0.3784\n",
      "Epoch [5/20], Step [313/704], Loss: 0.3228\n",
      "Epoch [5/20], Step [314/704], Loss: 0.4441\n",
      "Epoch [5/20], Step [315/704], Loss: 0.3933\n",
      "Epoch [5/20], Step [316/704], Loss: 0.4295\n",
      "Epoch [5/20], Step [317/704], Loss: 0.5137\n",
      "Epoch [5/20], Step [318/704], Loss: 0.5006\n",
      "Epoch [5/20], Step [319/704], Loss: 0.4591\n",
      "Epoch [5/20], Step [320/704], Loss: 0.1961\n",
      "Epoch [5/20], Step [321/704], Loss: 0.3448\n",
      "Epoch [5/20], Step [322/704], Loss: 0.5085\n",
      "Epoch [5/20], Step [323/704], Loss: 0.4726\n",
      "Epoch [5/20], Step [324/704], Loss: 0.3341\n",
      "Epoch [5/20], Step [325/704], Loss: 0.5789\n",
      "Epoch [5/20], Step [326/704], Loss: 0.5895\n",
      "Epoch [5/20], Step [327/704], Loss: 0.3427\n",
      "Epoch [5/20], Step [328/704], Loss: 0.5361\n",
      "Epoch [5/20], Step [329/704], Loss: 0.2194\n",
      "Epoch [5/20], Step [330/704], Loss: 0.5685\n",
      "Epoch [5/20], Step [331/704], Loss: 0.6934\n",
      "Epoch [5/20], Step [332/704], Loss: 0.4280\n",
      "Epoch [5/20], Step [333/704], Loss: 0.5441\n",
      "Epoch [5/20], Step [334/704], Loss: 0.4650\n",
      "Epoch [5/20], Step [335/704], Loss: 0.4534\n",
      "Epoch [5/20], Step [336/704], Loss: 0.3355\n",
      "Epoch [5/20], Step [337/704], Loss: 0.4747\n",
      "Epoch [5/20], Step [338/704], Loss: 0.4830\n",
      "Epoch [5/20], Step [339/704], Loss: 0.6317\n",
      "Epoch [5/20], Step [340/704], Loss: 0.3093\n",
      "Epoch [5/20], Step [341/704], Loss: 0.5822\n",
      "Epoch [5/20], Step [342/704], Loss: 0.2697\n",
      "Epoch [5/20], Step [343/704], Loss: 0.3719\n",
      "Epoch [5/20], Step [344/704], Loss: 0.5154\n",
      "Epoch [5/20], Step [345/704], Loss: 0.3786\n",
      "Epoch [5/20], Step [346/704], Loss: 0.4201\n",
      "Epoch [5/20], Step [347/704], Loss: 0.2542\n",
      "Epoch [5/20], Step [348/704], Loss: 0.4438\n",
      "Epoch [5/20], Step [349/704], Loss: 0.5008\n",
      "Epoch [5/20], Step [350/704], Loss: 0.3809\n",
      "Epoch [5/20], Step [351/704], Loss: 0.3112\n",
      "Epoch [5/20], Step [352/704], Loss: 0.4273\n",
      "Epoch [5/20], Step [353/704], Loss: 0.5125\n",
      "Epoch [5/20], Step [354/704], Loss: 0.5217\n",
      "Epoch [5/20], Step [355/704], Loss: 0.3440\n",
      "Epoch [5/20], Step [356/704], Loss: 0.2645\n",
      "Epoch [5/20], Step [357/704], Loss: 0.4691\n",
      "Epoch [5/20], Step [358/704], Loss: 0.4781\n",
      "Epoch [5/20], Step [359/704], Loss: 0.3062\n",
      "Epoch [5/20], Step [360/704], Loss: 0.2715\n",
      "Epoch [5/20], Step [361/704], Loss: 0.4601\n",
      "Epoch [5/20], Step [362/704], Loss: 0.4990\n",
      "Epoch [5/20], Step [363/704], Loss: 0.6502\n",
      "Epoch [5/20], Step [364/704], Loss: 0.3252\n",
      "Epoch [5/20], Step [365/704], Loss: 0.5249\n",
      "Epoch [5/20], Step [366/704], Loss: 0.3946\n",
      "Epoch [5/20], Step [367/704], Loss: 0.3215\n",
      "Epoch [5/20], Step [368/704], Loss: 0.4264\n",
      "Epoch [5/20], Step [369/704], Loss: 0.5886\n",
      "Epoch [5/20], Step [370/704], Loss: 0.5600\n",
      "Epoch [5/20], Step [371/704], Loss: 0.4634\n",
      "Epoch [5/20], Step [372/704], Loss: 0.5206\n",
      "Epoch [5/20], Step [373/704], Loss: 0.6081\n",
      "Epoch [5/20], Step [374/704], Loss: 0.3460\n",
      "Epoch [5/20], Step [375/704], Loss: 0.6059\n",
      "Epoch [5/20], Step [376/704], Loss: 0.5905\n",
      "Epoch [5/20], Step [377/704], Loss: 0.5728\n",
      "Epoch [5/20], Step [378/704], Loss: 0.4369\n",
      "Epoch [5/20], Step [379/704], Loss: 0.5772\n",
      "Epoch [5/20], Step [380/704], Loss: 0.4733\n",
      "Epoch [5/20], Step [381/704], Loss: 0.6426\n",
      "Epoch [5/20], Step [382/704], Loss: 0.3550\n",
      "Epoch [5/20], Step [383/704], Loss: 0.3951\n",
      "Epoch [5/20], Step [384/704], Loss: 0.5583\n",
      "Epoch [5/20], Step [385/704], Loss: 0.3719\n",
      "Epoch [5/20], Step [386/704], Loss: 0.5132\n",
      "Epoch [5/20], Step [387/704], Loss: 0.3987\n",
      "Epoch [5/20], Step [388/704], Loss: 0.6298\n",
      "Epoch [5/20], Step [389/704], Loss: 0.4507\n",
      "Epoch [5/20], Step [390/704], Loss: 0.4868\n",
      "Epoch [5/20], Step [391/704], Loss: 0.2881\n",
      "Epoch [5/20], Step [392/704], Loss: 0.3301\n",
      "Epoch [5/20], Step [393/704], Loss: 0.3009\n",
      "Epoch [5/20], Step [394/704], Loss: 0.5130\n",
      "Epoch [5/20], Step [395/704], Loss: 0.4300\n",
      "Epoch [5/20], Step [396/704], Loss: 0.5397\n",
      "Epoch [5/20], Step [397/704], Loss: 0.3310\n",
      "Epoch [5/20], Step [398/704], Loss: 0.5661\n",
      "Epoch [5/20], Step [399/704], Loss: 0.4008\n",
      "Epoch [5/20], Step [400/704], Loss: 0.6164\n",
      "Epoch [5/20], Step [401/704], Loss: 0.5996\n",
      "Epoch [5/20], Step [402/704], Loss: 0.4753\n",
      "Epoch [5/20], Step [403/704], Loss: 0.5570\n",
      "Epoch [5/20], Step [404/704], Loss: 0.4453\n",
      "Epoch [5/20], Step [405/704], Loss: 0.4845\n",
      "Epoch [5/20], Step [406/704], Loss: 0.4800\n",
      "Epoch [5/20], Step [407/704], Loss: 0.4250\n",
      "Epoch [5/20], Step [408/704], Loss: 0.5217\n",
      "Epoch [5/20], Step [409/704], Loss: 0.4069\n",
      "Epoch [5/20], Step [410/704], Loss: 0.2264\n",
      "Epoch [5/20], Step [411/704], Loss: 0.3576\n",
      "Epoch [5/20], Step [412/704], Loss: 0.5369\n",
      "Epoch [5/20], Step [413/704], Loss: 0.3635\n",
      "Epoch [5/20], Step [414/704], Loss: 0.4980\n",
      "Epoch [5/20], Step [415/704], Loss: 0.5189\n",
      "Epoch [5/20], Step [416/704], Loss: 0.3946\n",
      "Epoch [5/20], Step [417/704], Loss: 0.5316\n",
      "Epoch [5/20], Step [418/704], Loss: 0.3538\n",
      "Epoch [5/20], Step [419/704], Loss: 0.3490\n",
      "Epoch [5/20], Step [420/704], Loss: 0.5172\n",
      "Epoch [5/20], Step [421/704], Loss: 0.6093\n",
      "Epoch [5/20], Step [422/704], Loss: 0.3607\n",
      "Epoch [5/20], Step [423/704], Loss: 0.3483\n",
      "Epoch [5/20], Step [424/704], Loss: 0.5823\n",
      "Epoch [5/20], Step [425/704], Loss: 0.4262\n",
      "Epoch [5/20], Step [426/704], Loss: 0.3304\n",
      "Epoch [5/20], Step [427/704], Loss: 0.5072\n",
      "Epoch [5/20], Step [428/704], Loss: 0.3923\n",
      "Epoch [5/20], Step [429/704], Loss: 0.7699\n",
      "Epoch [5/20], Step [430/704], Loss: 0.4263\n",
      "Epoch [5/20], Step [431/704], Loss: 0.4572\n",
      "Epoch [5/20], Step [432/704], Loss: 0.4124\n",
      "Epoch [5/20], Step [433/704], Loss: 0.5660\n",
      "Epoch [5/20], Step [434/704], Loss: 0.4217\n",
      "Epoch [5/20], Step [435/704], Loss: 0.4042\n",
      "Epoch [5/20], Step [436/704], Loss: 0.4294\n",
      "Epoch [5/20], Step [437/704], Loss: 0.2832\n",
      "Epoch [5/20], Step [438/704], Loss: 0.4394\n",
      "Epoch [5/20], Step [439/704], Loss: 0.3464\n",
      "Epoch [5/20], Step [440/704], Loss: 0.5998\n",
      "Epoch [5/20], Step [441/704], Loss: 0.3151\n",
      "Epoch [5/20], Step [442/704], Loss: 0.3455\n",
      "Epoch [5/20], Step [443/704], Loss: 0.3697\n",
      "Epoch [5/20], Step [444/704], Loss: 0.4062\n",
      "Epoch [5/20], Step [445/704], Loss: 0.3810\n",
      "Epoch [5/20], Step [446/704], Loss: 0.7002\n",
      "Epoch [5/20], Step [447/704], Loss: 0.5908\n",
      "Epoch [5/20], Step [448/704], Loss: 0.4465\n",
      "Epoch [5/20], Step [449/704], Loss: 0.6313\n",
      "Epoch [5/20], Step [450/704], Loss: 0.4405\n",
      "Epoch [5/20], Step [451/704], Loss: 0.6599\n",
      "Epoch [5/20], Step [452/704], Loss: 0.9306\n",
      "Epoch [5/20], Step [453/704], Loss: 0.4691\n",
      "Epoch [5/20], Step [454/704], Loss: 0.5858\n",
      "Epoch [5/20], Step [455/704], Loss: 0.4745\n",
      "Epoch [5/20], Step [456/704], Loss: 0.3491\n",
      "Epoch [5/20], Step [457/704], Loss: 0.6389\n",
      "Epoch [5/20], Step [458/704], Loss: 0.4301\n",
      "Epoch [5/20], Step [459/704], Loss: 0.3587\n",
      "Epoch [5/20], Step [460/704], Loss: 0.4881\n",
      "Epoch [5/20], Step [461/704], Loss: 0.4360\n",
      "Epoch [5/20], Step [462/704], Loss: 0.4289\n",
      "Epoch [5/20], Step [463/704], Loss: 0.2929\n",
      "Epoch [5/20], Step [464/704], Loss: 0.5438\n",
      "Epoch [5/20], Step [465/704], Loss: 0.3790\n",
      "Epoch [5/20], Step [466/704], Loss: 0.6665\n",
      "Epoch [5/20], Step [467/704], Loss: 0.4088\n",
      "Epoch [5/20], Step [468/704], Loss: 0.3877\n",
      "Epoch [5/20], Step [469/704], Loss: 0.3165\n",
      "Epoch [5/20], Step [470/704], Loss: 0.6652\n",
      "Epoch [5/20], Step [471/704], Loss: 0.3571\n",
      "Epoch [5/20], Step [472/704], Loss: 0.4694\n",
      "Epoch [5/20], Step [473/704], Loss: 0.4646\n",
      "Epoch [5/20], Step [474/704], Loss: 0.6371\n",
      "Epoch [5/20], Step [475/704], Loss: 0.5055\n",
      "Epoch [5/20], Step [476/704], Loss: 0.5985\n",
      "Epoch [5/20], Step [477/704], Loss: 0.8292\n",
      "Epoch [5/20], Step [478/704], Loss: 0.5236\n",
      "Epoch [5/20], Step [479/704], Loss: 0.4976\n",
      "Epoch [5/20], Step [480/704], Loss: 0.7001\n",
      "Epoch [5/20], Step [481/704], Loss: 0.4658\n",
      "Epoch [5/20], Step [482/704], Loss: 0.4645\n",
      "Epoch [5/20], Step [483/704], Loss: 0.4943\n",
      "Epoch [5/20], Step [484/704], Loss: 0.6195\n",
      "Epoch [5/20], Step [485/704], Loss: 0.4568\n",
      "Epoch [5/20], Step [486/704], Loss: 0.5907\n",
      "Epoch [5/20], Step [487/704], Loss: 0.4771\n",
      "Epoch [5/20], Step [488/704], Loss: 0.3517\n",
      "Epoch [5/20], Step [489/704], Loss: 0.3626\n",
      "Epoch [5/20], Step [490/704], Loss: 0.6116\n",
      "Epoch [5/20], Step [491/704], Loss: 0.5754\n",
      "Epoch [5/20], Step [492/704], Loss: 0.4086\n",
      "Epoch [5/20], Step [493/704], Loss: 0.4934\n",
      "Epoch [5/20], Step [494/704], Loss: 0.6079\n",
      "Epoch [5/20], Step [495/704], Loss: 0.4678\n",
      "Epoch [5/20], Step [496/704], Loss: 0.2381\n",
      "Epoch [5/20], Step [497/704], Loss: 0.3801\n",
      "Epoch [5/20], Step [498/704], Loss: 0.4126\n",
      "Epoch [5/20], Step [499/704], Loss: 0.6271\n",
      "Epoch [5/20], Step [500/704], Loss: 0.4217\n",
      "Epoch [5/20], Step [501/704], Loss: 0.4827\n",
      "Epoch [5/20], Step [502/704], Loss: 0.4369\n",
      "Epoch [5/20], Step [503/704], Loss: 0.3392\n",
      "Epoch [5/20], Step [504/704], Loss: 0.3916\n",
      "Epoch [5/20], Step [505/704], Loss: 0.5638\n",
      "Epoch [5/20], Step [506/704], Loss: 0.5514\n",
      "Epoch [5/20], Step [507/704], Loss: 0.5598\n",
      "Epoch [5/20], Step [508/704], Loss: 0.2084\n",
      "Epoch [5/20], Step [509/704], Loss: 0.5340\n",
      "Epoch [5/20], Step [510/704], Loss: 0.3784\n",
      "Epoch [5/20], Step [511/704], Loss: 0.4019\n",
      "Epoch [5/20], Step [512/704], Loss: 0.3466\n",
      "Epoch [5/20], Step [513/704], Loss: 0.5754\n",
      "Epoch [5/20], Step [514/704], Loss: 0.3509\n",
      "Epoch [5/20], Step [515/704], Loss: 0.5296\n",
      "Epoch [5/20], Step [516/704], Loss: 0.5665\n",
      "Epoch [5/20], Step [517/704], Loss: 0.3924\n",
      "Epoch [5/20], Step [518/704], Loss: 0.3749\n",
      "Epoch [5/20], Step [519/704], Loss: 0.3423\n",
      "Epoch [5/20], Step [520/704], Loss: 0.4313\n",
      "Epoch [5/20], Step [521/704], Loss: 0.3637\n",
      "Epoch [5/20], Step [522/704], Loss: 0.8584\n",
      "Epoch [5/20], Step [523/704], Loss: 0.4746\n",
      "Epoch [5/20], Step [524/704], Loss: 0.3541\n",
      "Epoch [5/20], Step [525/704], Loss: 0.4426\n",
      "Epoch [5/20], Step [526/704], Loss: 0.4270\n",
      "Epoch [5/20], Step [527/704], Loss: 0.5017\n",
      "Epoch [5/20], Step [528/704], Loss: 0.3318\n",
      "Epoch [5/20], Step [529/704], Loss: 0.4472\n",
      "Epoch [5/20], Step [530/704], Loss: 0.3756\n",
      "Epoch [5/20], Step [531/704], Loss: 0.3111\n",
      "Epoch [5/20], Step [532/704], Loss: 0.3104\n",
      "Epoch [5/20], Step [533/704], Loss: 0.5675\n",
      "Epoch [5/20], Step [534/704], Loss: 0.5163\n",
      "Epoch [5/20], Step [535/704], Loss: 0.4696\n",
      "Epoch [5/20], Step [536/704], Loss: 0.4979\n",
      "Epoch [5/20], Step [537/704], Loss: 0.7523\n",
      "Epoch [5/20], Step [538/704], Loss: 0.4500\n",
      "Epoch [5/20], Step [539/704], Loss: 0.4425\n",
      "Epoch [5/20], Step [540/704], Loss: 0.5189\n",
      "Epoch [5/20], Step [541/704], Loss: 0.3001\n",
      "Epoch [5/20], Step [542/704], Loss: 0.4767\n",
      "Epoch [5/20], Step [543/704], Loss: 0.3854\n",
      "Epoch [5/20], Step [544/704], Loss: 0.5422\n",
      "Epoch [5/20], Step [545/704], Loss: 0.4461\n",
      "Epoch [5/20], Step [546/704], Loss: 0.4896\n",
      "Epoch [5/20], Step [547/704], Loss: 0.4274\n",
      "Epoch [5/20], Step [548/704], Loss: 0.6391\n",
      "Epoch [5/20], Step [549/704], Loss: 0.5684\n",
      "Epoch [5/20], Step [550/704], Loss: 0.5769\n",
      "Epoch [5/20], Step [551/704], Loss: 0.3831\n",
      "Epoch [5/20], Step [552/704], Loss: 0.5466\n",
      "Epoch [5/20], Step [553/704], Loss: 0.3459\n",
      "Epoch [5/20], Step [554/704], Loss: 0.3557\n",
      "Epoch [5/20], Step [555/704], Loss: 0.4965\n",
      "Epoch [5/20], Step [556/704], Loss: 0.4560\n",
      "Epoch [5/20], Step [557/704], Loss: 0.5844\n",
      "Epoch [5/20], Step [558/704], Loss: 0.6127\n",
      "Epoch [5/20], Step [559/704], Loss: 0.3893\n",
      "Epoch [5/20], Step [560/704], Loss: 0.6067\n",
      "Epoch [5/20], Step [561/704], Loss: 0.5884\n",
      "Epoch [5/20], Step [562/704], Loss: 0.3242\n",
      "Epoch [5/20], Step [563/704], Loss: 0.5320\n",
      "Epoch [5/20], Step [564/704], Loss: 0.5308\n",
      "Epoch [5/20], Step [565/704], Loss: 0.3457\n",
      "Epoch [5/20], Step [566/704], Loss: 0.6598\n",
      "Epoch [5/20], Step [567/704], Loss: 0.4534\n",
      "Epoch [5/20], Step [568/704], Loss: 0.3724\n",
      "Epoch [5/20], Step [569/704], Loss: 0.5657\n",
      "Epoch [5/20], Step [570/704], Loss: 0.2790\n",
      "Epoch [5/20], Step [571/704], Loss: 0.4456\n",
      "Epoch [5/20], Step [572/704], Loss: 0.5649\n",
      "Epoch [5/20], Step [573/704], Loss: 0.3416\n",
      "Epoch [5/20], Step [574/704], Loss: 0.4880\n",
      "Epoch [5/20], Step [575/704], Loss: 0.5019\n",
      "Epoch [5/20], Step [576/704], Loss: 0.5435\n",
      "Epoch [5/20], Step [577/704], Loss: 0.3325\n",
      "Epoch [5/20], Step [578/704], Loss: 0.4080\n",
      "Epoch [5/20], Step [579/704], Loss: 0.4837\n",
      "Epoch [5/20], Step [580/704], Loss: 0.5394\n",
      "Epoch [5/20], Step [581/704], Loss: 0.4985\n",
      "Epoch [5/20], Step [582/704], Loss: 0.3679\n",
      "Epoch [5/20], Step [583/704], Loss: 0.5398\n",
      "Epoch [5/20], Step [584/704], Loss: 0.4397\n",
      "Epoch [5/20], Step [585/704], Loss: 0.4537\n",
      "Epoch [5/20], Step [586/704], Loss: 0.3448\n",
      "Epoch [5/20], Step [587/704], Loss: 0.5519\n",
      "Epoch [5/20], Step [588/704], Loss: 0.5096\n",
      "Epoch [5/20], Step [589/704], Loss: 0.5801\n",
      "Epoch [5/20], Step [590/704], Loss: 0.1775\n",
      "Epoch [5/20], Step [591/704], Loss: 0.3225\n",
      "Epoch [5/20], Step [592/704], Loss: 0.2556\n",
      "Epoch [5/20], Step [593/704], Loss: 0.5180\n",
      "Epoch [5/20], Step [594/704], Loss: 0.3573\n",
      "Epoch [5/20], Step [595/704], Loss: 0.4272\n",
      "Epoch [5/20], Step [596/704], Loss: 0.4619\n",
      "Epoch [5/20], Step [597/704], Loss: 0.4977\n",
      "Epoch [5/20], Step [598/704], Loss: 0.6592\n",
      "Epoch [5/20], Step [599/704], Loss: 0.4430\n",
      "Epoch [5/20], Step [600/704], Loss: 0.2799\n",
      "Epoch [5/20], Step [601/704], Loss: 0.9750\n",
      "Epoch [5/20], Step [602/704], Loss: 0.2627\n",
      "Epoch [5/20], Step [603/704], Loss: 0.6251\n",
      "Epoch [5/20], Step [604/704], Loss: 0.5925\n",
      "Epoch [5/20], Step [605/704], Loss: 0.5765\n",
      "Epoch [5/20], Step [606/704], Loss: 0.4701\n",
      "Epoch [5/20], Step [607/704], Loss: 0.4355\n",
      "Epoch [5/20], Step [608/704], Loss: 0.5805\n",
      "Epoch [5/20], Step [609/704], Loss: 0.4032\n",
      "Epoch [5/20], Step [610/704], Loss: 0.4338\n",
      "Epoch [5/20], Step [611/704], Loss: 0.3888\n",
      "Epoch [5/20], Step [612/704], Loss: 0.3614\n",
      "Epoch [5/20], Step [613/704], Loss: 0.2676\n",
      "Epoch [5/20], Step [614/704], Loss: 0.3593\n",
      "Epoch [5/20], Step [615/704], Loss: 0.3111\n",
      "Epoch [5/20], Step [616/704], Loss: 0.3272\n",
      "Epoch [5/20], Step [617/704], Loss: 0.4853\n",
      "Epoch [5/20], Step [618/704], Loss: 0.4153\n",
      "Epoch [5/20], Step [619/704], Loss: 0.4889\n",
      "Epoch [5/20], Step [620/704], Loss: 0.6060\n",
      "Epoch [5/20], Step [621/704], Loss: 0.4185\n",
      "Epoch [5/20], Step [622/704], Loss: 0.5077\n",
      "Epoch [5/20], Step [623/704], Loss: 0.6832\n",
      "Epoch [5/20], Step [624/704], Loss: 0.2624\n",
      "Epoch [5/20], Step [625/704], Loss: 0.3941\n",
      "Epoch [5/20], Step [626/704], Loss: 0.4615\n",
      "Epoch [5/20], Step [627/704], Loss: 0.4753\n",
      "Epoch [5/20], Step [628/704], Loss: 0.2953\n",
      "Epoch [5/20], Step [629/704], Loss: 0.3086\n",
      "Epoch [5/20], Step [630/704], Loss: 0.5590\n",
      "Epoch [5/20], Step [631/704], Loss: 0.4075\n",
      "Epoch [5/20], Step [632/704], Loss: 0.4841\n",
      "Epoch [5/20], Step [633/704], Loss: 0.3748\n",
      "Epoch [5/20], Step [634/704], Loss: 0.4854\n",
      "Epoch [5/20], Step [635/704], Loss: 0.3577\n",
      "Epoch [5/20], Step [636/704], Loss: 0.3140\n",
      "Epoch [5/20], Step [637/704], Loss: 0.3519\n",
      "Epoch [5/20], Step [638/704], Loss: 0.3263\n",
      "Epoch [5/20], Step [639/704], Loss: 0.3843\n",
      "Epoch [5/20], Step [640/704], Loss: 0.5089\n",
      "Epoch [5/20], Step [641/704], Loss: 0.5186\n",
      "Epoch [5/20], Step [642/704], Loss: 0.4125\n",
      "Epoch [5/20], Step [643/704], Loss: 0.3626\n",
      "Epoch [5/20], Step [644/704], Loss: 0.2533\n",
      "Epoch [5/20], Step [645/704], Loss: 0.5620\n",
      "Epoch [5/20], Step [646/704], Loss: 0.6357\n",
      "Epoch [5/20], Step [647/704], Loss: 0.4786\n",
      "Epoch [5/20], Step [648/704], Loss: 0.4718\n",
      "Epoch [5/20], Step [649/704], Loss: 0.4806\n",
      "Epoch [5/20], Step [650/704], Loss: 0.3913\n",
      "Epoch [5/20], Step [651/704], Loss: 0.3461\n",
      "Epoch [5/20], Step [652/704], Loss: 0.4710\n",
      "Epoch [5/20], Step [653/704], Loss: 0.3320\n",
      "Epoch [5/20], Step [654/704], Loss: 0.4733\n",
      "Epoch [5/20], Step [655/704], Loss: 0.4496\n",
      "Epoch [5/20], Step [656/704], Loss: 0.6219\n",
      "Epoch [5/20], Step [657/704], Loss: 0.5773\n",
      "Epoch [5/20], Step [658/704], Loss: 0.3727\n",
      "Epoch [5/20], Step [659/704], Loss: 0.6550\n",
      "Epoch [5/20], Step [660/704], Loss: 0.3284\n",
      "Epoch [5/20], Step [661/704], Loss: 0.5613\n",
      "Epoch [5/20], Step [662/704], Loss: 0.3809\n",
      "Epoch [5/20], Step [663/704], Loss: 0.2980\n",
      "Epoch [5/20], Step [664/704], Loss: 0.4342\n",
      "Epoch [5/20], Step [665/704], Loss: 0.3767\n",
      "Epoch [5/20], Step [666/704], Loss: 0.4356\n",
      "Epoch [5/20], Step [667/704], Loss: 0.4685\n",
      "Epoch [5/20], Step [668/704], Loss: 0.4776\n",
      "Epoch [5/20], Step [669/704], Loss: 0.4758\n",
      "Epoch [5/20], Step [670/704], Loss: 0.5032\n",
      "Epoch [5/20], Step [671/704], Loss: 0.6090\n",
      "Epoch [5/20], Step [672/704], Loss: 0.4533\n",
      "Epoch [5/20], Step [673/704], Loss: 0.3159\n",
      "Epoch [5/20], Step [674/704], Loss: 0.5062\n",
      "Epoch [5/20], Step [675/704], Loss: 0.3354\n",
      "Epoch [5/20], Step [676/704], Loss: 0.4478\n",
      "Epoch [5/20], Step [677/704], Loss: 0.5069\n",
      "Epoch [5/20], Step [678/704], Loss: 0.4822\n",
      "Epoch [5/20], Step [679/704], Loss: 0.4088\n",
      "Epoch [5/20], Step [680/704], Loss: 0.4877\n",
      "Epoch [5/20], Step [681/704], Loss: 0.3779\n",
      "Epoch [5/20], Step [682/704], Loss: 0.4106\n",
      "Epoch [5/20], Step [683/704], Loss: 0.5494\n",
      "Epoch [5/20], Step [684/704], Loss: 0.5487\n",
      "Epoch [5/20], Step [685/704], Loss: 0.1805\n",
      "Epoch [5/20], Step [686/704], Loss: 0.4829\n",
      "Epoch [5/20], Step [687/704], Loss: 0.4352\n",
      "Epoch [5/20], Step [688/704], Loss: 0.3749\n",
      "Epoch [5/20], Step [689/704], Loss: 0.3853\n",
      "Epoch [5/20], Step [690/704], Loss: 0.3777\n",
      "Epoch [5/20], Step [691/704], Loss: 0.3437\n",
      "Epoch [5/20], Step [692/704], Loss: 0.5449\n",
      "Epoch [5/20], Step [693/704], Loss: 0.2233\n",
      "Epoch [5/20], Step [694/704], Loss: 0.3952\n",
      "Epoch [5/20], Step [695/704], Loss: 0.3399\n",
      "Epoch [5/20], Step [696/704], Loss: 0.4945\n",
      "Epoch [5/20], Step [697/704], Loss: 0.4050\n",
      "Epoch [5/20], Step [698/704], Loss: 0.4082\n",
      "Epoch [5/20], Step [699/704], Loss: 0.4829\n",
      "Epoch [5/20], Step [700/704], Loss: 0.3772\n",
      "Epoch [5/20], Step [701/704], Loss: 0.3377\n",
      "Epoch [5/20], Step [702/704], Loss: 0.3163\n",
      "Epoch [5/20], Step [703/704], Loss: 0.6078\n",
      "Epoch [5/20], Step [704/704], Loss: 0.8508\n",
      "Epoch [5/20], Loss: 0.8508\n",
      "Epoch [6/20], Step [1/704], Loss: 0.1968\n",
      "Epoch [6/20], Step [2/704], Loss: 0.4024\n",
      "Epoch [6/20], Step [3/704], Loss: 0.4112\n",
      "Epoch [6/20], Step [4/704], Loss: 0.2712\n",
      "Epoch [6/20], Step [5/704], Loss: 0.5490\n",
      "Epoch [6/20], Step [6/704], Loss: 0.5139\n",
      "Epoch [6/20], Step [7/704], Loss: 0.5820\n",
      "Epoch [6/20], Step [8/704], Loss: 0.4829\n",
      "Epoch [6/20], Step [9/704], Loss: 0.4989\n",
      "Epoch [6/20], Step [10/704], Loss: 0.5908\n",
      "Epoch [6/20], Step [11/704], Loss: 0.5511\n",
      "Epoch [6/20], Step [12/704], Loss: 0.5696\n",
      "Epoch [6/20], Step [13/704], Loss: 0.3691\n",
      "Epoch [6/20], Step [14/704], Loss: 0.4452\n",
      "Epoch [6/20], Step [15/704], Loss: 0.4257\n",
      "Epoch [6/20], Step [16/704], Loss: 0.2127\n",
      "Epoch [6/20], Step [17/704], Loss: 0.3285\n",
      "Epoch [6/20], Step [18/704], Loss: 0.3576\n",
      "Epoch [6/20], Step [19/704], Loss: 0.6182\n",
      "Epoch [6/20], Step [20/704], Loss: 0.5347\n",
      "Epoch [6/20], Step [21/704], Loss: 0.3336\n",
      "Epoch [6/20], Step [22/704], Loss: 0.3102\n",
      "Epoch [6/20], Step [23/704], Loss: 0.4606\n",
      "Epoch [6/20], Step [24/704], Loss: 0.5093\n",
      "Epoch [6/20], Step [25/704], Loss: 0.3270\n",
      "Epoch [6/20], Step [26/704], Loss: 0.5916\n",
      "Epoch [6/20], Step [27/704], Loss: 0.4494\n",
      "Epoch [6/20], Step [28/704], Loss: 0.5041\n",
      "Epoch [6/20], Step [29/704], Loss: 0.3614\n",
      "Epoch [6/20], Step [30/704], Loss: 0.3647\n",
      "Epoch [6/20], Step [31/704], Loss: 0.5121\n",
      "Epoch [6/20], Step [32/704], Loss: 0.3066\n",
      "Epoch [6/20], Step [33/704], Loss: 0.5529\n",
      "Epoch [6/20], Step [34/704], Loss: 0.5588\n",
      "Epoch [6/20], Step [35/704], Loss: 0.4708\n",
      "Epoch [6/20], Step [36/704], Loss: 0.5399\n",
      "Epoch [6/20], Step [37/704], Loss: 0.4386\n",
      "Epoch [6/20], Step [38/704], Loss: 0.3862\n",
      "Epoch [6/20], Step [39/704], Loss: 0.3321\n",
      "Epoch [6/20], Step [40/704], Loss: 0.4410\n",
      "Epoch [6/20], Step [41/704], Loss: 0.4566\n",
      "Epoch [6/20], Step [42/704], Loss: 0.5910\n",
      "Epoch [6/20], Step [43/704], Loss: 0.4045\n",
      "Epoch [6/20], Step [44/704], Loss: 0.4215\n",
      "Epoch [6/20], Step [45/704], Loss: 0.3003\n",
      "Epoch [6/20], Step [46/704], Loss: 0.3197\n",
      "Epoch [6/20], Step [47/704], Loss: 0.3904\n",
      "Epoch [6/20], Step [48/704], Loss: 0.4175\n",
      "Epoch [6/20], Step [49/704], Loss: 0.4195\n",
      "Epoch [6/20], Step [50/704], Loss: 0.5055\n",
      "Epoch [6/20], Step [51/704], Loss: 0.4048\n",
      "Epoch [6/20], Step [52/704], Loss: 0.3301\n",
      "Epoch [6/20], Step [53/704], Loss: 0.4360\n",
      "Epoch [6/20], Step [54/704], Loss: 0.3595\n",
      "Epoch [6/20], Step [55/704], Loss: 0.4328\n",
      "Epoch [6/20], Step [56/704], Loss: 0.1733\n",
      "Epoch [6/20], Step [57/704], Loss: 0.3386\n",
      "Epoch [6/20], Step [58/704], Loss: 0.3471\n",
      "Epoch [6/20], Step [59/704], Loss: 0.3167\n",
      "Epoch [6/20], Step [60/704], Loss: 0.2990\n",
      "Epoch [6/20], Step [61/704], Loss: 0.3502\n",
      "Epoch [6/20], Step [62/704], Loss: 0.2931\n",
      "Epoch [6/20], Step [63/704], Loss: 0.5385\n",
      "Epoch [6/20], Step [64/704], Loss: 0.2831\n",
      "Epoch [6/20], Step [65/704], Loss: 0.3576\n",
      "Epoch [6/20], Step [66/704], Loss: 0.2413\n",
      "Epoch [6/20], Step [67/704], Loss: 0.3715\n",
      "Epoch [6/20], Step [68/704], Loss: 0.3639\n",
      "Epoch [6/20], Step [69/704], Loss: 0.4250\n",
      "Epoch [6/20], Step [70/704], Loss: 0.5557\n",
      "Epoch [6/20], Step [71/704], Loss: 0.4712\n",
      "Epoch [6/20], Step [72/704], Loss: 0.2666\n",
      "Epoch [6/20], Step [73/704], Loss: 0.2894\n",
      "Epoch [6/20], Step [74/704], Loss: 0.2759\n",
      "Epoch [6/20], Step [75/704], Loss: 0.3640\n",
      "Epoch [6/20], Step [76/704], Loss: 0.2992\n",
      "Epoch [6/20], Step [77/704], Loss: 0.2658\n",
      "Epoch [6/20], Step [78/704], Loss: 0.2085\n",
      "Epoch [6/20], Step [79/704], Loss: 0.2913\n",
      "Epoch [6/20], Step [80/704], Loss: 0.3947\n",
      "Epoch [6/20], Step [81/704], Loss: 0.5269\n",
      "Epoch [6/20], Step [82/704], Loss: 0.2711\n",
      "Epoch [6/20], Step [83/704], Loss: 0.4457\n",
      "Epoch [6/20], Step [84/704], Loss: 0.5158\n",
      "Epoch [6/20], Step [85/704], Loss: 0.5038\n",
      "Epoch [6/20], Step [86/704], Loss: 0.4026\n",
      "Epoch [6/20], Step [87/704], Loss: 0.0975\n",
      "Epoch [6/20], Step [88/704], Loss: 0.5300\n",
      "Epoch [6/20], Step [89/704], Loss: 0.2131\n",
      "Epoch [6/20], Step [90/704], Loss: 0.4468\n",
      "Epoch [6/20], Step [91/704], Loss: 0.2307\n",
      "Epoch [6/20], Step [92/704], Loss: 0.2722\n",
      "Epoch [6/20], Step [93/704], Loss: 0.1619\n",
      "Epoch [6/20], Step [94/704], Loss: 0.2793\n",
      "Epoch [6/20], Step [95/704], Loss: 0.4340\n",
      "Epoch [6/20], Step [96/704], Loss: 0.2153\n",
      "Epoch [6/20], Step [97/704], Loss: 0.3407\n",
      "Epoch [6/20], Step [98/704], Loss: 0.4031\n",
      "Epoch [6/20], Step [99/704], Loss: 0.3116\n",
      "Epoch [6/20], Step [100/704], Loss: 0.4251\n",
      "Epoch [6/20], Step [101/704], Loss: 0.2427\n",
      "Epoch [6/20], Step [102/704], Loss: 0.3254\n",
      "Epoch [6/20], Step [103/704], Loss: 0.1797\n",
      "Epoch [6/20], Step [104/704], Loss: 0.5188\n",
      "Epoch [6/20], Step [105/704], Loss: 0.6014\n",
      "Epoch [6/20], Step [106/704], Loss: 0.3662\n",
      "Epoch [6/20], Step [107/704], Loss: 0.4894\n",
      "Epoch [6/20], Step [108/704], Loss: 0.4520\n",
      "Epoch [6/20], Step [109/704], Loss: 0.3659\n",
      "Epoch [6/20], Step [110/704], Loss: 0.4717\n",
      "Epoch [6/20], Step [111/704], Loss: 0.4607\n",
      "Epoch [6/20], Step [112/704], Loss: 0.3666\n",
      "Epoch [6/20], Step [113/704], Loss: 0.3155\n",
      "Epoch [6/20], Step [114/704], Loss: 0.3816\n",
      "Epoch [6/20], Step [115/704], Loss: 0.4728\n",
      "Epoch [6/20], Step [116/704], Loss: 0.3950\n",
      "Epoch [6/20], Step [117/704], Loss: 0.4609\n",
      "Epoch [6/20], Step [118/704], Loss: 0.3474\n",
      "Epoch [6/20], Step [119/704], Loss: 0.3112\n",
      "Epoch [6/20], Step [120/704], Loss: 0.5613\n",
      "Epoch [6/20], Step [121/704], Loss: 0.4269\n",
      "Epoch [6/20], Step [122/704], Loss: 0.3876\n",
      "Epoch [6/20], Step [123/704], Loss: 0.4256\n",
      "Epoch [6/20], Step [124/704], Loss: 0.4834\n",
      "Epoch [6/20], Step [125/704], Loss: 0.4288\n",
      "Epoch [6/20], Step [126/704], Loss: 0.1940\n",
      "Epoch [6/20], Step [127/704], Loss: 0.4745\n",
      "Epoch [6/20], Step [128/704], Loss: 0.4363\n",
      "Epoch [6/20], Step [129/704], Loss: 0.4963\n",
      "Epoch [6/20], Step [130/704], Loss: 0.5528\n",
      "Epoch [6/20], Step [131/704], Loss: 0.4437\n",
      "Epoch [6/20], Step [132/704], Loss: 0.3173\n",
      "Epoch [6/20], Step [133/704], Loss: 0.3735\n",
      "Epoch [6/20], Step [134/704], Loss: 0.3555\n",
      "Epoch [6/20], Step [135/704], Loss: 0.3833\n",
      "Epoch [6/20], Step [136/704], Loss: 0.4658\n",
      "Epoch [6/20], Step [137/704], Loss: 0.3025\n",
      "Epoch [6/20], Step [138/704], Loss: 0.3077\n",
      "Epoch [6/20], Step [139/704], Loss: 0.3368\n",
      "Epoch [6/20], Step [140/704], Loss: 0.4305\n",
      "Epoch [6/20], Step [141/704], Loss: 0.4360\n",
      "Epoch [6/20], Step [142/704], Loss: 0.3112\n",
      "Epoch [6/20], Step [143/704], Loss: 0.3264\n",
      "Epoch [6/20], Step [144/704], Loss: 0.3424\n",
      "Epoch [6/20], Step [145/704], Loss: 0.4001\n",
      "Epoch [6/20], Step [146/704], Loss: 0.3761\n",
      "Epoch [6/20], Step [147/704], Loss: 0.2438\n",
      "Epoch [6/20], Step [148/704], Loss: 0.5046\n",
      "Epoch [6/20], Step [149/704], Loss: 0.3678\n",
      "Epoch [6/20], Step [150/704], Loss: 0.3597\n",
      "Epoch [6/20], Step [151/704], Loss: 0.2833\n",
      "Epoch [6/20], Step [152/704], Loss: 0.3374\n",
      "Epoch [6/20], Step [153/704], Loss: 0.1361\n",
      "Epoch [6/20], Step [154/704], Loss: 0.4463\n",
      "Epoch [6/20], Step [155/704], Loss: 0.4378\n",
      "Epoch [6/20], Step [156/704], Loss: 0.4061\n",
      "Epoch [6/20], Step [157/704], Loss: 0.2028\n",
      "Epoch [6/20], Step [158/704], Loss: 0.3289\n",
      "Epoch [6/20], Step [159/704], Loss: 0.6222\n",
      "Epoch [6/20], Step [160/704], Loss: 0.3789\n",
      "Epoch [6/20], Step [161/704], Loss: 0.2247\n",
      "Epoch [6/20], Step [162/704], Loss: 0.2996\n",
      "Epoch [6/20], Step [163/704], Loss: 0.4915\n",
      "Epoch [6/20], Step [164/704], Loss: 0.4034\n",
      "Epoch [6/20], Step [165/704], Loss: 0.5373\n",
      "Epoch [6/20], Step [166/704], Loss: 0.3286\n",
      "Epoch [6/20], Step [167/704], Loss: 0.1890\n",
      "Epoch [6/20], Step [168/704], Loss: 0.3348\n",
      "Epoch [6/20], Step [169/704], Loss: 0.3302\n",
      "Epoch [6/20], Step [170/704], Loss: 0.3485\n",
      "Epoch [6/20], Step [171/704], Loss: 0.3444\n",
      "Epoch [6/20], Step [172/704], Loss: 0.2937\n",
      "Epoch [6/20], Step [173/704], Loss: 0.3596\n",
      "Epoch [6/20], Step [174/704], Loss: 0.2842\n",
      "Epoch [6/20], Step [175/704], Loss: 0.3226\n",
      "Epoch [6/20], Step [176/704], Loss: 0.4076\n",
      "Epoch [6/20], Step [177/704], Loss: 0.2091\n",
      "Epoch [6/20], Step [178/704], Loss: 0.3312\n",
      "Epoch [6/20], Step [179/704], Loss: 0.6935\n",
      "Epoch [6/20], Step [180/704], Loss: 0.4993\n",
      "Epoch [6/20], Step [181/704], Loss: 0.2207\n",
      "Epoch [6/20], Step [182/704], Loss: 0.4895\n",
      "Epoch [6/20], Step [183/704], Loss: 0.4018\n",
      "Epoch [6/20], Step [184/704], Loss: 0.3519\n",
      "Epoch [6/20], Step [185/704], Loss: 0.4074\n",
      "Epoch [6/20], Step [186/704], Loss: 0.2793\n",
      "Epoch [6/20], Step [187/704], Loss: 0.3649\n",
      "Epoch [6/20], Step [188/704], Loss: 0.4452\n",
      "Epoch [6/20], Step [189/704], Loss: 0.6337\n",
      "Epoch [6/20], Step [190/704], Loss: 0.2442\n",
      "Epoch [6/20], Step [191/704], Loss: 0.2627\n",
      "Epoch [6/20], Step [192/704], Loss: 0.2251\n",
      "Epoch [6/20], Step [193/704], Loss: 0.2388\n",
      "Epoch [6/20], Step [194/704], Loss: 0.5402\n",
      "Epoch [6/20], Step [195/704], Loss: 0.3135\n",
      "Epoch [6/20], Step [196/704], Loss: 0.2229\n",
      "Epoch [6/20], Step [197/704], Loss: 0.2216\n",
      "Epoch [6/20], Step [198/704], Loss: 0.3649\n",
      "Epoch [6/20], Step [199/704], Loss: 0.2953\n",
      "Epoch [6/20], Step [200/704], Loss: 0.4342\n",
      "Epoch [6/20], Step [201/704], Loss: 0.2670\n",
      "Epoch [6/20], Step [202/704], Loss: 0.3667\n",
      "Epoch [6/20], Step [203/704], Loss: 0.3231\n",
      "Epoch [6/20], Step [204/704], Loss: 0.4199\n",
      "Epoch [6/20], Step [205/704], Loss: 0.2930\n",
      "Epoch [6/20], Step [206/704], Loss: 0.3060\n",
      "Epoch [6/20], Step [207/704], Loss: 0.3201\n",
      "Epoch [6/20], Step [208/704], Loss: 0.4611\n",
      "Epoch [6/20], Step [209/704], Loss: 0.3868\n",
      "Epoch [6/20], Step [210/704], Loss: 0.3812\n",
      "Epoch [6/20], Step [211/704], Loss: 0.3476\n",
      "Epoch [6/20], Step [212/704], Loss: 0.2783\n",
      "Epoch [6/20], Step [213/704], Loss: 0.4872\n",
      "Epoch [6/20], Step [214/704], Loss: 0.3358\n",
      "Epoch [6/20], Step [215/704], Loss: 0.5040\n",
      "Epoch [6/20], Step [216/704], Loss: 0.3410\n",
      "Epoch [6/20], Step [217/704], Loss: 0.3788\n",
      "Epoch [6/20], Step [218/704], Loss: 0.5059\n",
      "Epoch [6/20], Step [219/704], Loss: 0.3870\n",
      "Epoch [6/20], Step [220/704], Loss: 0.2576\n",
      "Epoch [6/20], Step [221/704], Loss: 0.4265\n",
      "Epoch [6/20], Step [222/704], Loss: 0.2954\n",
      "Epoch [6/20], Step [223/704], Loss: 0.1775\n",
      "Epoch [6/20], Step [224/704], Loss: 0.4752\n",
      "Epoch [6/20], Step [225/704], Loss: 0.3172\n",
      "Epoch [6/20], Step [226/704], Loss: 0.3312\n",
      "Epoch [6/20], Step [227/704], Loss: 0.1958\n",
      "Epoch [6/20], Step [228/704], Loss: 0.5422\n",
      "Epoch [6/20], Step [229/704], Loss: 0.3229\n",
      "Epoch [6/20], Step [230/704], Loss: 0.4391\n",
      "Epoch [6/20], Step [231/704], Loss: 0.3768\n",
      "Epoch [6/20], Step [232/704], Loss: 0.3551\n",
      "Epoch [6/20], Step [233/704], Loss: 0.4314\n",
      "Epoch [6/20], Step [234/704], Loss: 0.4229\n",
      "Epoch [6/20], Step [235/704], Loss: 0.4430\n",
      "Epoch [6/20], Step [236/704], Loss: 0.3909\n",
      "Epoch [6/20], Step [237/704], Loss: 0.3129\n",
      "Epoch [6/20], Step [238/704], Loss: 0.4121\n",
      "Epoch [6/20], Step [239/704], Loss: 0.3641\n",
      "Epoch [6/20], Step [240/704], Loss: 0.2707\n",
      "Epoch [6/20], Step [241/704], Loss: 0.3094\n",
      "Epoch [6/20], Step [242/704], Loss: 0.5487\n",
      "Epoch [6/20], Step [243/704], Loss: 0.3695\n",
      "Epoch [6/20], Step [244/704], Loss: 0.2788\n",
      "Epoch [6/20], Step [245/704], Loss: 0.2896\n",
      "Epoch [6/20], Step [246/704], Loss: 0.3803\n",
      "Epoch [6/20], Step [247/704], Loss: 0.5139\n",
      "Epoch [6/20], Step [248/704], Loss: 0.4264\n",
      "Epoch [6/20], Step [249/704], Loss: 0.2062\n",
      "Epoch [6/20], Step [250/704], Loss: 0.2591\n",
      "Epoch [6/20], Step [251/704], Loss: 0.3307\n",
      "Epoch [6/20], Step [252/704], Loss: 0.4009\n",
      "Epoch [6/20], Step [253/704], Loss: 0.2578\n",
      "Epoch [6/20], Step [254/704], Loss: 0.4119\n",
      "Epoch [6/20], Step [255/704], Loss: 0.5743\n",
      "Epoch [6/20], Step [256/704], Loss: 0.5394\n",
      "Epoch [6/20], Step [257/704], Loss: 0.4535\n",
      "Epoch [6/20], Step [258/704], Loss: 0.2862\n",
      "Epoch [6/20], Step [259/704], Loss: 0.3536\n",
      "Epoch [6/20], Step [260/704], Loss: 0.3840\n",
      "Epoch [6/20], Step [261/704], Loss: 0.2747\n",
      "Epoch [6/20], Step [262/704], Loss: 0.4074\n",
      "Epoch [6/20], Step [263/704], Loss: 0.3679\n",
      "Epoch [6/20], Step [264/704], Loss: 0.2842\n",
      "Epoch [6/20], Step [265/704], Loss: 0.5649\n",
      "Epoch [6/20], Step [266/704], Loss: 0.2432\n",
      "Epoch [6/20], Step [267/704], Loss: 0.3082\n",
      "Epoch [6/20], Step [268/704], Loss: 0.2682\n",
      "Epoch [6/20], Step [269/704], Loss: 0.5227\n",
      "Epoch [6/20], Step [270/704], Loss: 0.2171\n",
      "Epoch [6/20], Step [271/704], Loss: 0.2698\n",
      "Epoch [6/20], Step [272/704], Loss: 0.2952\n",
      "Epoch [6/20], Step [273/704], Loss: 0.5196\n",
      "Epoch [6/20], Step [274/704], Loss: 0.2360\n",
      "Epoch [6/20], Step [275/704], Loss: 0.5138\n",
      "Epoch [6/20], Step [276/704], Loss: 0.3634\n",
      "Epoch [6/20], Step [277/704], Loss: 0.3220\n",
      "Epoch [6/20], Step [278/704], Loss: 0.4646\n",
      "Epoch [6/20], Step [279/704], Loss: 0.2951\n",
      "Epoch [6/20], Step [280/704], Loss: 0.2761\n",
      "Epoch [6/20], Step [281/704], Loss: 0.5429\n",
      "Epoch [6/20], Step [282/704], Loss: 0.3681\n",
      "Epoch [6/20], Step [283/704], Loss: 0.4197\n",
      "Epoch [6/20], Step [284/704], Loss: 0.5807\n",
      "Epoch [6/20], Step [285/704], Loss: 0.2307\n",
      "Epoch [6/20], Step [286/704], Loss: 0.5102\n",
      "Epoch [6/20], Step [287/704], Loss: 0.2767\n",
      "Epoch [6/20], Step [288/704], Loss: 0.4211\n",
      "Epoch [6/20], Step [289/704], Loss: 0.4270\n",
      "Epoch [6/20], Step [290/704], Loss: 0.3826\n",
      "Epoch [6/20], Step [291/704], Loss: 0.5000\n",
      "Epoch [6/20], Step [292/704], Loss: 0.3620\n",
      "Epoch [6/20], Step [293/704], Loss: 0.4666\n",
      "Epoch [6/20], Step [294/704], Loss: 0.2824\n",
      "Epoch [6/20], Step [295/704], Loss: 0.2248\n",
      "Epoch [6/20], Step [296/704], Loss: 0.3115\n",
      "Epoch [6/20], Step [297/704], Loss: 0.3774\n",
      "Epoch [6/20], Step [298/704], Loss: 0.2649\n",
      "Epoch [6/20], Step [299/704], Loss: 0.5085\n",
      "Epoch [6/20], Step [300/704], Loss: 0.5405\n",
      "Epoch [6/20], Step [301/704], Loss: 0.3092\n",
      "Epoch [6/20], Step [302/704], Loss: 0.2578\n",
      "Epoch [6/20], Step [303/704], Loss: 0.4366\n",
      "Epoch [6/20], Step [304/704], Loss: 0.3549\n",
      "Epoch [6/20], Step [305/704], Loss: 0.3115\n",
      "Epoch [6/20], Step [306/704], Loss: 0.2980\n",
      "Epoch [6/20], Step [307/704], Loss: 0.3286\n",
      "Epoch [6/20], Step [308/704], Loss: 0.3312\n",
      "Epoch [6/20], Step [309/704], Loss: 0.5516\n",
      "Epoch [6/20], Step [310/704], Loss: 0.3064\n",
      "Epoch [6/20], Step [311/704], Loss: 0.3582\n",
      "Epoch [6/20], Step [312/704], Loss: 0.1908\n",
      "Epoch [6/20], Step [313/704], Loss: 0.2546\n",
      "Epoch [6/20], Step [314/704], Loss: 0.4003\n",
      "Epoch [6/20], Step [315/704], Loss: 0.2952\n",
      "Epoch [6/20], Step [316/704], Loss: 0.4012\n",
      "Epoch [6/20], Step [317/704], Loss: 0.2949\n",
      "Epoch [6/20], Step [318/704], Loss: 0.5794\n",
      "Epoch [6/20], Step [319/704], Loss: 0.3183\n",
      "Epoch [6/20], Step [320/704], Loss: 0.6110\n",
      "Epoch [6/20], Step [321/704], Loss: 0.3053\n",
      "Epoch [6/20], Step [322/704], Loss: 0.4591\n",
      "Epoch [6/20], Step [323/704], Loss: 0.3398\n",
      "Epoch [6/20], Step [324/704], Loss: 0.4154\n",
      "Epoch [6/20], Step [325/704], Loss: 0.3861\n",
      "Epoch [6/20], Step [326/704], Loss: 0.3077\n",
      "Epoch [6/20], Step [327/704], Loss: 0.5136\n",
      "Epoch [6/20], Step [328/704], Loss: 0.2493\n",
      "Epoch [6/20], Step [329/704], Loss: 0.4524\n",
      "Epoch [6/20], Step [330/704], Loss: 0.5984\n",
      "Epoch [6/20], Step [331/704], Loss: 0.4529\n",
      "Epoch [6/20], Step [332/704], Loss: 0.4594\n",
      "Epoch [6/20], Step [333/704], Loss: 0.3653\n",
      "Epoch [6/20], Step [334/704], Loss: 0.5060\n",
      "Epoch [6/20], Step [335/704], Loss: 0.3119\n",
      "Epoch [6/20], Step [336/704], Loss: 0.4073\n",
      "Epoch [6/20], Step [337/704], Loss: 0.2571\n",
      "Epoch [6/20], Step [338/704], Loss: 0.4777\n",
      "Epoch [6/20], Step [339/704], Loss: 0.2868\n",
      "Epoch [6/20], Step [340/704], Loss: 0.4953\n",
      "Epoch [6/20], Step [341/704], Loss: 0.3580\n",
      "Epoch [6/20], Step [342/704], Loss: 0.2420\n",
      "Epoch [6/20], Step [343/704], Loss: 0.4723\n",
      "Epoch [6/20], Step [344/704], Loss: 0.4049\n",
      "Epoch [6/20], Step [345/704], Loss: 0.3561\n",
      "Epoch [6/20], Step [346/704], Loss: 0.3635\n",
      "Epoch [6/20], Step [347/704], Loss: 0.6082\n",
      "Epoch [6/20], Step [348/704], Loss: 0.2929\n",
      "Epoch [6/20], Step [349/704], Loss: 0.1801\n",
      "Epoch [6/20], Step [350/704], Loss: 0.4400\n",
      "Epoch [6/20], Step [351/704], Loss: 0.3042\n",
      "Epoch [6/20], Step [352/704], Loss: 0.5419\n",
      "Epoch [6/20], Step [353/704], Loss: 0.5341\n",
      "Epoch [6/20], Step [354/704], Loss: 0.4489\n",
      "Epoch [6/20], Step [355/704], Loss: 0.1746\n",
      "Epoch [6/20], Step [356/704], Loss: 0.2772\n",
      "Epoch [6/20], Step [357/704], Loss: 0.2608\n",
      "Epoch [6/20], Step [358/704], Loss: 0.2124\n",
      "Epoch [6/20], Step [359/704], Loss: 0.2789\n",
      "Epoch [6/20], Step [360/704], Loss: 0.5648\n",
      "Epoch [6/20], Step [361/704], Loss: 0.2993\n",
      "Epoch [6/20], Step [362/704], Loss: 0.3276\n",
      "Epoch [6/20], Step [363/704], Loss: 0.4437\n",
      "Epoch [6/20], Step [364/704], Loss: 0.3477\n",
      "Epoch [6/20], Step [365/704], Loss: 0.5037\n",
      "Epoch [6/20], Step [366/704], Loss: 0.3613\n",
      "Epoch [6/20], Step [367/704], Loss: 0.4929\n",
      "Epoch [6/20], Step [368/704], Loss: 0.3399\n",
      "Epoch [6/20], Step [369/704], Loss: 0.3338\n",
      "Epoch [6/20], Step [370/704], Loss: 0.3586\n",
      "Epoch [6/20], Step [371/704], Loss: 0.3944\n",
      "Epoch [6/20], Step [372/704], Loss: 0.3709\n",
      "Epoch [6/20], Step [373/704], Loss: 0.4457\n",
      "Epoch [6/20], Step [374/704], Loss: 0.2096\n",
      "Epoch [6/20], Step [375/704], Loss: 0.2543\n",
      "Epoch [6/20], Step [376/704], Loss: 0.3174\n",
      "Epoch [6/20], Step [377/704], Loss: 0.4276\n",
      "Epoch [6/20], Step [378/704], Loss: 0.3760\n",
      "Epoch [6/20], Step [379/704], Loss: 0.3838\n",
      "Epoch [6/20], Step [380/704], Loss: 0.4246\n",
      "Epoch [6/20], Step [381/704], Loss: 0.3890\n",
      "Epoch [6/20], Step [382/704], Loss: 0.5675\n",
      "Epoch [6/20], Step [383/704], Loss: 0.6782\n",
      "Epoch [6/20], Step [384/704], Loss: 0.4048\n",
      "Epoch [6/20], Step [385/704], Loss: 0.3273\n",
      "Epoch [6/20], Step [386/704], Loss: 0.2542\n",
      "Epoch [6/20], Step [387/704], Loss: 0.3681\n",
      "Epoch [6/20], Step [388/704], Loss: 0.2950\n",
      "Epoch [6/20], Step [389/704], Loss: 0.4431\n",
      "Epoch [6/20], Step [390/704], Loss: 0.4545\n",
      "Epoch [6/20], Step [391/704], Loss: 0.3390\n",
      "Epoch [6/20], Step [392/704], Loss: 0.2664\n",
      "Epoch [6/20], Step [393/704], Loss: 0.3838\n",
      "Epoch [6/20], Step [394/704], Loss: 0.4112\n",
      "Epoch [6/20], Step [395/704], Loss: 0.3823\n",
      "Epoch [6/20], Step [396/704], Loss: 0.3641\n",
      "Epoch [6/20], Step [397/704], Loss: 0.3637\n",
      "Epoch [6/20], Step [398/704], Loss: 0.4749\n",
      "Epoch [6/20], Step [399/704], Loss: 0.2897\n",
      "Epoch [6/20], Step [400/704], Loss: 0.4080\n",
      "Epoch [6/20], Step [401/704], Loss: 0.4883\n",
      "Epoch [6/20], Step [402/704], Loss: 0.4388\n",
      "Epoch [6/20], Step [403/704], Loss: 0.3944\n",
      "Epoch [6/20], Step [404/704], Loss: 0.3705\n",
      "Epoch [6/20], Step [405/704], Loss: 0.3341\n",
      "Epoch [6/20], Step [406/704], Loss: 0.3096\n",
      "Epoch [6/20], Step [407/704], Loss: 0.4896\n",
      "Epoch [6/20], Step [408/704], Loss: 0.4590\n",
      "Epoch [6/20], Step [409/704], Loss: 0.3775\n",
      "Epoch [6/20], Step [410/704], Loss: 0.4647\n",
      "Epoch [6/20], Step [411/704], Loss: 0.2935\n",
      "Epoch [6/20], Step [412/704], Loss: 0.7056\n",
      "Epoch [6/20], Step [413/704], Loss: 0.3274\n",
      "Epoch [6/20], Step [414/704], Loss: 0.3100\n",
      "Epoch [6/20], Step [415/704], Loss: 0.3665\n",
      "Epoch [6/20], Step [416/704], Loss: 0.4019\n",
      "Epoch [6/20], Step [417/704], Loss: 0.4136\n",
      "Epoch [6/20], Step [418/704], Loss: 0.3021\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     29\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 30\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m images, labels, outputs\n\u001b[1;32m     32\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "File \u001b[0;32m~/Workspace/ML/deep_learning_research/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:485\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    482\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    483\u001b[0m             )\n\u001b[0;32m--> 485\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Workspace/ML/deep_learning_research/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:79\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 79\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/Workspace/ML/deep_learning_research/.venv/lib/python3.10/site-packages/torch/optim/sgd.py:125\u001b[0m, in \u001b[0;36mSGD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    119\u001b[0m momentum_buffer_list: \u001b[38;5;28mlist\u001b[39m[Optional[Tensor]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    121\u001b[0m has_sparse_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    122\u001b[0m     group, params, grads, momentum_buffer_list\n\u001b[1;32m    123\u001b[0m )\n\u001b[0;32m--> 125\u001b[0m \u001b[43msgd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmomentum\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdampening\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnesterov\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmomentum\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# update momentum_buffers in state\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p, momentum_buffer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params, momentum_buffer_list):\n",
      "File \u001b[0;32m~/Workspace/ML/deep_learning_research/.venv/lib/python3.10/site-packages/torch/optim/sgd.py:300\u001b[0m, in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, fused, grad_scale, found_inf, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_sgd\n\u001b[0;32m--> 300\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdampening\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnesterov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/ML/deep_learning_research/.venv/lib/python3.10/site-packages/torch/optim/sgd.py:368\u001b[0m, in \u001b[0;36m_single_tensor_sgd\u001b[0;34m(params, grads, momentum_buffer_list, grad_scale, found_inf, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[1;32m    366\u001b[0m         param\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mlr)\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 368\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "num_classes = 10\n",
    "num_epochs = 20\n",
    "batch_size = 16\n",
    "learning_rate = 0.01\n",
    "path = \"models/torch-resnet\"\n",
    "\n",
    "model = ResNet(ResidualBlock, [3, 4, 6, 3]).to(device)\n",
    "\n",
    "#Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.001, momentum = 0.9)  \n",
    "\n",
    "#Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        #Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        print(\"Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}\".format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "        #Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        del images, labels, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    print (\"Epoch [{}/{}], Loss: {:.4f}\".format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a39f7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in valid_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        del images, labels, outputs\n",
    "\n",
    "    print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
