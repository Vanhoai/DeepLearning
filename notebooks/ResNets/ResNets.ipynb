{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Implementation of ResNet architecture using PyTorch\n",
    "\n",
    "A Residual Network, also known as ResNet, is a deep neural network architecture that introduced the concept of residual blocks. The key idea behind ResNet is the use of residual blocks, which enable the training of extremely deep networks. Traditional deep neural networks suffer from the problem of vanishing gradients, where the gradients become increasingly small as they propagate backward through multiple layers. This makes it difficult to train very deep networks because the earlier layers receive weak gradient and have trouble learning meaningful representation.\n",
    "\n",
    "![Residual Connections](./residual_connections.png)\n",
    "\n",
    "To address this issue, ResNet introduces residual connections, also called skip connections, that allow for the direct flow of information from earlier layers to later layers. Instead of trying to learn the direct mapping from input to the output, ResNet learns the residual mapping - The difference between the input and output - making it easier for the network to optimize and learn the underlying features."
   ],
   "id": "505bff58875a1114"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Residual Block\n",
    "\n",
    "Residual ResNet block is composed of two layers of 3x3 convolutional layer/batch normalization/relu. In the picture below, the lines represent the residual operation.\n",
    "\n",
    "![Basic ResNet Block](residual_resnet_block.png)"
   ],
   "id": "9cfcd926dc174fc4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T09:28:40.333164Z",
     "start_time": "2025-06-18T09:28:40.328839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional"
   ],
   "id": "ba312f89399c8182",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T09:28:44.678567Z",
     "start_time": "2025-06-18T09:28:44.670275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    self.expansion * out_channels,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False\n",
    "                ),\n",
    "                nn.BatchNorm2d(self.expansion * out_channels)\n",
    "            )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += self.shortcut(x)\n",
    "        return self.relu(out)"
   ],
   "id": "f202531691cb2e6f",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Bottleneck Version for Deeper Networks\n",
    "\n",
    "To increase the network depth while keeping the parameters size as low as possible, the authors defined a BottleNeck block that “The three layers are 1x1, 3x3, and 1x1 convolutions, where the 1×1 layers are responsible for reducing and then increasing (restoring) dimensions, leaving the 3×3 layer a bottleneck with smaller input/output dimensions.”\n",
    "\n",
    "![BottleNeck Block](./bottleneck_resnet_block.png)"
   ],
   "id": "deb7639dae091029"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T09:28:48.398233Z",
     "start_time": "2025-06-18T09:28:48.389680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BottleneckBlock(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):\n",
    "        super(BottleneckBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            out_channels,\n",
    "            self.expansion * out_channels,\n",
    "            kernel_size=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion * out_channels)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != self.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    self.expansion * out_channels,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False\n",
    "                ),\n",
    "                nn.BatchNorm2d(self.expansion * out_channels)\n",
    "            )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out += self.shortcut(x)\n",
    "        return self.relu(out)\n"
   ],
   "id": "932bc5556aeac8c",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### ResNet Layer\n",
    "A ResNet’s layer is composed of the same blocks stacked one after the other."
   ],
   "id": "43799fe22376f687"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T09:28:57.373688Z",
     "start_time": "2025-06-18T09:28:57.364060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, in_channel=3, zero_init_residual=False):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        self.in_planes = 64\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channel,\n",
    "            64,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, BottleneckBlock):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, ResidualBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for i in range(num_blocks):\n",
    "            stride = strides[i]\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = functional.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        return out"
   ],
   "id": "90362e4747d3fc88",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Model Architecture\n",
    "ResNet architectures can vary in depth, with deeper versions having more residual blocks. Common variants include ResNet-18, ResNet-34, ResNet-50, ResNet-101, and ResNet-152, where the numbers represent the total number of layers in the network.\n",
    "\n",
    "![ResNet 18](./resnet_18_architecture.png)\n",
    "![ResNet 50](./resnet_50_architecture.png)"
   ],
   "id": "d3d8b0b0ad1d3b2a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T09:28:59.981225Z",
     "start_time": "2025-06-18T09:28:59.974461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def resnet18():\n",
    "    return ResNet(ResidualBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def resnet50():\n",
    "    return ResNet(BottleneckBlock, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def resnet101():\n",
    "    return ResNet(BottleneckBlock, [3, 4, 23, 3])\n",
    "\n",
    "\n",
    "def resnet152():\n",
    "    return ResNet(BottleneckBlock, [3, 8, 36, 3])"
   ],
   "id": "6181ec7f708c5b9a",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T09:48:43.029579Z",
     "start_time": "2025-06-18T09:48:41.400644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "model = resnet50()\n",
    "summary(model, (3, 224, 224))"
   ],
   "id": "1f3e7704edf4701f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,728\n",
      "       BatchNorm2d-2         [-1, 64, 224, 224]             128\n",
      "            Conv2d-3         [-1, 64, 224, 224]           4,096\n",
      "       BatchNorm2d-4         [-1, 64, 224, 224]             128\n",
      "              ReLU-5         [-1, 64, 224, 224]               0\n",
      "            Conv2d-6         [-1, 64, 224, 224]          36,864\n",
      "       BatchNorm2d-7         [-1, 64, 224, 224]             128\n",
      "              ReLU-8         [-1, 64, 224, 224]               0\n",
      "            Conv2d-9        [-1, 256, 224, 224]          16,384\n",
      "      BatchNorm2d-10        [-1, 256, 224, 224]             512\n",
      "           Conv2d-11        [-1, 256, 224, 224]          16,384\n",
      "      BatchNorm2d-12        [-1, 256, 224, 224]             512\n",
      "             ReLU-13        [-1, 256, 224, 224]               0\n",
      "  BottleneckBlock-14        [-1, 256, 224, 224]               0\n",
      "           Conv2d-15         [-1, 64, 224, 224]          16,384\n",
      "      BatchNorm2d-16         [-1, 64, 224, 224]             128\n",
      "             ReLU-17         [-1, 64, 224, 224]               0\n",
      "           Conv2d-18         [-1, 64, 224, 224]          36,864\n",
      "      BatchNorm2d-19         [-1, 64, 224, 224]             128\n",
      "             ReLU-20         [-1, 64, 224, 224]               0\n",
      "           Conv2d-21        [-1, 256, 224, 224]          16,384\n",
      "      BatchNorm2d-22        [-1, 256, 224, 224]             512\n",
      "             ReLU-23        [-1, 256, 224, 224]               0\n",
      "  BottleneckBlock-24        [-1, 256, 224, 224]               0\n",
      "           Conv2d-25         [-1, 64, 224, 224]          16,384\n",
      "      BatchNorm2d-26         [-1, 64, 224, 224]             128\n",
      "             ReLU-27         [-1, 64, 224, 224]               0\n",
      "           Conv2d-28         [-1, 64, 224, 224]          36,864\n",
      "      BatchNorm2d-29         [-1, 64, 224, 224]             128\n",
      "             ReLU-30         [-1, 64, 224, 224]               0\n",
      "           Conv2d-31        [-1, 256, 224, 224]          16,384\n",
      "      BatchNorm2d-32        [-1, 256, 224, 224]             512\n",
      "             ReLU-33        [-1, 256, 224, 224]               0\n",
      "  BottleneckBlock-34        [-1, 256, 224, 224]               0\n",
      "           Conv2d-35        [-1, 128, 224, 224]          32,768\n",
      "      BatchNorm2d-36        [-1, 128, 224, 224]             256\n",
      "             ReLU-37        [-1, 128, 224, 224]               0\n",
      "           Conv2d-38        [-1, 128, 112, 112]         147,456\n",
      "      BatchNorm2d-39        [-1, 128, 112, 112]             256\n",
      "             ReLU-40        [-1, 128, 112, 112]               0\n",
      "           Conv2d-41        [-1, 512, 112, 112]          65,536\n",
      "      BatchNorm2d-42        [-1, 512, 112, 112]           1,024\n",
      "           Conv2d-43        [-1, 512, 112, 112]         131,072\n",
      "      BatchNorm2d-44        [-1, 512, 112, 112]           1,024\n",
      "             ReLU-45        [-1, 512, 112, 112]               0\n",
      "  BottleneckBlock-46        [-1, 512, 112, 112]               0\n",
      "           Conv2d-47        [-1, 128, 112, 112]          65,536\n",
      "      BatchNorm2d-48        [-1, 128, 112, 112]             256\n",
      "             ReLU-49        [-1, 128, 112, 112]               0\n",
      "           Conv2d-50        [-1, 128, 112, 112]         147,456\n",
      "      BatchNorm2d-51        [-1, 128, 112, 112]             256\n",
      "             ReLU-52        [-1, 128, 112, 112]               0\n",
      "           Conv2d-53        [-1, 512, 112, 112]          65,536\n",
      "      BatchNorm2d-54        [-1, 512, 112, 112]           1,024\n",
      "             ReLU-55        [-1, 512, 112, 112]               0\n",
      "  BottleneckBlock-56        [-1, 512, 112, 112]               0\n",
      "           Conv2d-57        [-1, 128, 112, 112]          65,536\n",
      "      BatchNorm2d-58        [-1, 128, 112, 112]             256\n",
      "             ReLU-59        [-1, 128, 112, 112]               0\n",
      "           Conv2d-60        [-1, 128, 112, 112]         147,456\n",
      "      BatchNorm2d-61        [-1, 128, 112, 112]             256\n",
      "             ReLU-62        [-1, 128, 112, 112]               0\n",
      "           Conv2d-63        [-1, 512, 112, 112]          65,536\n",
      "      BatchNorm2d-64        [-1, 512, 112, 112]           1,024\n",
      "             ReLU-65        [-1, 512, 112, 112]               0\n",
      "  BottleneckBlock-66        [-1, 512, 112, 112]               0\n",
      "           Conv2d-67        [-1, 128, 112, 112]          65,536\n",
      "      BatchNorm2d-68        [-1, 128, 112, 112]             256\n",
      "             ReLU-69        [-1, 128, 112, 112]               0\n",
      "           Conv2d-70        [-1, 128, 112, 112]         147,456\n",
      "      BatchNorm2d-71        [-1, 128, 112, 112]             256\n",
      "             ReLU-72        [-1, 128, 112, 112]               0\n",
      "           Conv2d-73        [-1, 512, 112, 112]          65,536\n",
      "      BatchNorm2d-74        [-1, 512, 112, 112]           1,024\n",
      "             ReLU-75        [-1, 512, 112, 112]               0\n",
      "  BottleneckBlock-76        [-1, 512, 112, 112]               0\n",
      "           Conv2d-77        [-1, 256, 112, 112]         131,072\n",
      "      BatchNorm2d-78        [-1, 256, 112, 112]             512\n",
      "             ReLU-79        [-1, 256, 112, 112]               0\n",
      "           Conv2d-80          [-1, 256, 56, 56]         589,824\n",
      "      BatchNorm2d-81          [-1, 256, 56, 56]             512\n",
      "             ReLU-82          [-1, 256, 56, 56]               0\n",
      "           Conv2d-83         [-1, 1024, 56, 56]         262,144\n",
      "      BatchNorm2d-84         [-1, 1024, 56, 56]           2,048\n",
      "           Conv2d-85         [-1, 1024, 56, 56]         524,288\n",
      "      BatchNorm2d-86         [-1, 1024, 56, 56]           2,048\n",
      "             ReLU-87         [-1, 1024, 56, 56]               0\n",
      "  BottleneckBlock-88         [-1, 1024, 56, 56]               0\n",
      "           Conv2d-89          [-1, 256, 56, 56]         262,144\n",
      "      BatchNorm2d-90          [-1, 256, 56, 56]             512\n",
      "             ReLU-91          [-1, 256, 56, 56]               0\n",
      "           Conv2d-92          [-1, 256, 56, 56]         589,824\n",
      "      BatchNorm2d-93          [-1, 256, 56, 56]             512\n",
      "             ReLU-94          [-1, 256, 56, 56]               0\n",
      "           Conv2d-95         [-1, 1024, 56, 56]         262,144\n",
      "      BatchNorm2d-96         [-1, 1024, 56, 56]           2,048\n",
      "             ReLU-97         [-1, 1024, 56, 56]               0\n",
      "  BottleneckBlock-98         [-1, 1024, 56, 56]               0\n",
      "           Conv2d-99          [-1, 256, 56, 56]         262,144\n",
      "     BatchNorm2d-100          [-1, 256, 56, 56]             512\n",
      "            ReLU-101          [-1, 256, 56, 56]               0\n",
      "          Conv2d-102          [-1, 256, 56, 56]         589,824\n",
      "     BatchNorm2d-103          [-1, 256, 56, 56]             512\n",
      "            ReLU-104          [-1, 256, 56, 56]               0\n",
      "          Conv2d-105         [-1, 1024, 56, 56]         262,144\n",
      "     BatchNorm2d-106         [-1, 1024, 56, 56]           2,048\n",
      "            ReLU-107         [-1, 1024, 56, 56]               0\n",
      " BottleneckBlock-108         [-1, 1024, 56, 56]               0\n",
      "          Conv2d-109          [-1, 256, 56, 56]         262,144\n",
      "     BatchNorm2d-110          [-1, 256, 56, 56]             512\n",
      "            ReLU-111          [-1, 256, 56, 56]               0\n",
      "          Conv2d-112          [-1, 256, 56, 56]         589,824\n",
      "     BatchNorm2d-113          [-1, 256, 56, 56]             512\n",
      "            ReLU-114          [-1, 256, 56, 56]               0\n",
      "          Conv2d-115         [-1, 1024, 56, 56]         262,144\n",
      "     BatchNorm2d-116         [-1, 1024, 56, 56]           2,048\n",
      "            ReLU-117         [-1, 1024, 56, 56]               0\n",
      " BottleneckBlock-118         [-1, 1024, 56, 56]               0\n",
      "          Conv2d-119          [-1, 256, 56, 56]         262,144\n",
      "     BatchNorm2d-120          [-1, 256, 56, 56]             512\n",
      "            ReLU-121          [-1, 256, 56, 56]               0\n",
      "          Conv2d-122          [-1, 256, 56, 56]         589,824\n",
      "     BatchNorm2d-123          [-1, 256, 56, 56]             512\n",
      "            ReLU-124          [-1, 256, 56, 56]               0\n",
      "          Conv2d-125         [-1, 1024, 56, 56]         262,144\n",
      "     BatchNorm2d-126         [-1, 1024, 56, 56]           2,048\n",
      "            ReLU-127         [-1, 1024, 56, 56]               0\n",
      " BottleneckBlock-128         [-1, 1024, 56, 56]               0\n",
      "          Conv2d-129          [-1, 256, 56, 56]         262,144\n",
      "     BatchNorm2d-130          [-1, 256, 56, 56]             512\n",
      "            ReLU-131          [-1, 256, 56, 56]               0\n",
      "          Conv2d-132          [-1, 256, 56, 56]         589,824\n",
      "     BatchNorm2d-133          [-1, 256, 56, 56]             512\n",
      "            ReLU-134          [-1, 256, 56, 56]               0\n",
      "          Conv2d-135         [-1, 1024, 56, 56]         262,144\n",
      "     BatchNorm2d-136         [-1, 1024, 56, 56]           2,048\n",
      "            ReLU-137         [-1, 1024, 56, 56]               0\n",
      " BottleneckBlock-138         [-1, 1024, 56, 56]               0\n",
      "          Conv2d-139          [-1, 512, 56, 56]         524,288\n",
      "     BatchNorm2d-140          [-1, 512, 56, 56]           1,024\n",
      "            ReLU-141          [-1, 512, 56, 56]               0\n",
      "          Conv2d-142          [-1, 512, 28, 28]       2,359,296\n",
      "     BatchNorm2d-143          [-1, 512, 28, 28]           1,024\n",
      "            ReLU-144          [-1, 512, 28, 28]               0\n",
      "          Conv2d-145         [-1, 2048, 28, 28]       1,048,576\n",
      "     BatchNorm2d-146         [-1, 2048, 28, 28]           4,096\n",
      "          Conv2d-147         [-1, 2048, 28, 28]       2,097,152\n",
      "     BatchNorm2d-148         [-1, 2048, 28, 28]           4,096\n",
      "            ReLU-149         [-1, 2048, 28, 28]               0\n",
      " BottleneckBlock-150         [-1, 2048, 28, 28]               0\n",
      "          Conv2d-151          [-1, 512, 28, 28]       1,048,576\n",
      "     BatchNorm2d-152          [-1, 512, 28, 28]           1,024\n",
      "            ReLU-153          [-1, 512, 28, 28]               0\n",
      "          Conv2d-154          [-1, 512, 28, 28]       2,359,296\n",
      "     BatchNorm2d-155          [-1, 512, 28, 28]           1,024\n",
      "            ReLU-156          [-1, 512, 28, 28]               0\n",
      "          Conv2d-157         [-1, 2048, 28, 28]       1,048,576\n",
      "     BatchNorm2d-158         [-1, 2048, 28, 28]           4,096\n",
      "            ReLU-159         [-1, 2048, 28, 28]               0\n",
      " BottleneckBlock-160         [-1, 2048, 28, 28]               0\n",
      "          Conv2d-161          [-1, 512, 28, 28]       1,048,576\n",
      "     BatchNorm2d-162          [-1, 512, 28, 28]           1,024\n",
      "            ReLU-163          [-1, 512, 28, 28]               0\n",
      "          Conv2d-164          [-1, 512, 28, 28]       2,359,296\n",
      "     BatchNorm2d-165          [-1, 512, 28, 28]           1,024\n",
      "            ReLU-166          [-1, 512, 28, 28]               0\n",
      "          Conv2d-167         [-1, 2048, 28, 28]       1,048,576\n",
      "     BatchNorm2d-168         [-1, 2048, 28, 28]           4,096\n",
      "            ReLU-169         [-1, 2048, 28, 28]               0\n",
      " BottleneckBlock-170         [-1, 2048, 28, 28]               0\n",
      "AdaptiveAvgPool2d-171           [-1, 2048, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 23,500,352\n",
      "Trainable params: 23,500,352\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 4315.08\n",
      "Params size (MB): 89.65\n",
      "Estimated Total Size (MB): 4405.30\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T09:35:24.777418Z",
     "start_time": "2025-06-18T09:35:24.768651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, weight_decay=0.001, momentum=0.9)"
   ],
   "id": "2767c6b117e7f7a3",
   "outputs": [],
   "execution_count": 74
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
