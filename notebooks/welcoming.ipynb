{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0039d022",
   "metadata": {},
   "source": [
    "##### Getting Started with Torch & TorchVision\n",
    "\n",
    "This is notebook is designed to help you get started with PyTorch and TorchVision, two powerful libraries for deep learning and computer vision tasks.\n",
    "In this notebook, we will cover the following topics:\n",
    "1. Installation: How to install PyTorch and TorchVision.\n",
    "2. Building Model: How to build a simple neural network model using PyTorch.\n",
    "3. Loading Data: How to load and preprocess datasets using TorchVision.\n",
    "4. Training: How to train the model on a dataset.\n",
    "5. Techniques: How to apply various techniques to improve model performance.\n",
    "6. Evaluation: How to evaluate the model's performance.\n",
    "\n",
    "Notice: In the end of notebook you will build model MobileNetV2 and train it in CIFAR10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a928271e",
   "metadata": {},
   "source": [
    "##### Install required dependencies\n",
    "\n",
    "```bash\n",
    "pip install torch torchvision\n",
    "pip install torchinfo # for summarizing models\n",
    "```\n",
    "\n",
    "You can install in cell by running the following command:\n",
    "\n",
    "```python\n",
    "# !pip install torch torchvision\n",
    "# !pip install torchinfo\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577f62d4",
   "metadata": {},
   "source": [
    "##### Building Simple Model\n",
    "\n",
    "In the section, we will build a simple neural network model using PyTorch and training for MNIST dataset.\n",
    "\n",
    "So, let's start by importing the necessary libraries and modules.\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import torchinfo\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "id": "4916ea5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T08:08:31.208608Z",
     "start_time": "2025-06-20T08:08:31.202870Z"
    }
   },
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchinfo import summary\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "id": "27ba3ec8",
   "metadata": {},
   "source": [
    "##### Define the Simple Model\n",
    "\n",
    "With MNIST dataset, we will create a simple model with architecture like this:\n",
    "1. Input Layer: 28x28 pixels (MNIST images)\n",
    "2. Flatten Layer: Converts the 2D image into a 1D vector -> 784 features\n",
    "3. We will use two fully connected layers:\n",
    "    - First Layer: 512 neurons with ReLU activation\n",
    "    - Second Layer: 128 neurons with ReLU activation\n",
    "4. Output Layer: 10 neurons (one for each digit 0-9) with Softmax activation\n",
    "\n",
    "Okay, so let's define the model class with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "id": "15e82073",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T08:08:31.345209Z",
     "start_time": "2025-06-20T08:08:31.342650Z"
    }
   },
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, num_classes) -> None:\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.flatten(x)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Model Summary",
   "id": "d5ac04e6246897e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T08:08:31.499646Z",
     "start_time": "2025-06-20T08:08:31.491801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "simple_model = SimpleNN(num_classes=10)\n",
    "summary(simple_model, input_size=(1, 28, 28))"
   ],
   "id": "708a34b523268eff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "SimpleNN                                 [1, 10]                   --\n",
       "├─Flatten: 1-1                           [1, 784]                  --\n",
       "├─Linear: 1-2                            [1, 512]                  401,920\n",
       "├─ReLU: 1-3                              [1, 512]                  --\n",
       "├─Linear: 1-4                            [1, 128]                  65,664\n",
       "├─ReLU: 1-5                              [1, 128]                  --\n",
       "├─Linear: 1-6                            [1, 10]                   1,290\n",
       "==========================================================================================\n",
       "Total params: 468,874\n",
       "Trainable params: 468,874\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.47\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.01\n",
       "Params size (MB): 1.88\n",
       "Estimated Total Size (MB): 1.88\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Loading MNIST Dataset",
   "id": "ea307c34b690e69d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T08:08:31.663511Z",
     "start_time": "2025-06-20T08:08:31.625594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Mean and std deviation for MNIST\n",
    "])\n",
    "\n",
    "dataset = \"/Users/hinsun/Workspace/ComputerScience/DeepLearning/data\"\n",
    "\n",
    "train_batch_size = 64\n",
    "test_batch_size = 128\n",
    "\n",
    "train_dataset = MNIST(root=dataset, train=True, download=True, transform=transform)\n",
    "test_dataset = MNIST(root=dataset, train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train X shape: {train_dataset.data.shape} y shape: {train_dataset.targets.shape}\")\n",
    "print(f\"Test X shape: {test_dataset.data.shape} y shape: {test_dataset.targets.shape}\")"
   ],
   "id": "c309bbef130f6684",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X shape: torch.Size([60000, 28, 28]) y shape: torch.Size([60000])\n",
      "Test X shape: torch.Size([10000, 28, 28]) y shape: torch.Size([10000])\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Training the Model\n",
    "\n",
    "Before training the model, we will cover about device in PyTorch.\n",
    "In PyTorch, you can specify the device (CPU or GPU) on which you want to run your computations. This is done using the `torch.device` function. If a GPU is available, you can use it to speed up training and inference.\n",
    "\n",
    "Notice: In MacOS, you can use MPS (Metal Performance Shaders) for GPU acceleration."
   ],
   "id": "4790ef7262442bb1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T08:08:31.797296Z",
     "start_time": "2025-06-20T08:08:31.793274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sys import platform\n",
    "\n",
    "# You can check if a GPU is available and set the device accordingly\n",
    "# Because we are using macOS, we will use MPS (Metal Performance Shaders) for GPU acceleration\n",
    "\n",
    "if platform == \"darwin\":\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Device: {device}\")"
   ],
   "id": "ae6e0ee17a3810f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Before training the model, we need to define the loss function and optimizer.",
   "id": "b35d26a69e947d37"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T08:08:31.947411Z",
     "start_time": "2025-06-20T08:08:31.944057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loss function for multi-class classification\n",
    "# Notice: You can see in last layer of model, we not use Softmax activation, because CrossEntropyLoss already applies Softmax internally. If you remember, when we use Softmax and CrossEntropyLoss together, so when gradient backpropagation, it will be calculated simply with formula: A - Y, where A is the output of the model and Y is the target label.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Learning rate for the optimizer\n",
    "lr = 0.001\n",
    "\n",
    "# Momentum technique to accelerate SGD in the relevant direction and dampen oscillations\n",
    "momentum = 0.9\n",
    "\n",
    "# Nesterov momentum, which is a variant of momentum that looks ahead to the next position\n",
    "nesterov = True\n",
    "\n",
    "# Optimizer for updating model weights\n",
    "optimizer = optim.SGD(simple_model.parameters(), lr=lr, momentum=momentum, nesterov=nesterov)"
   ],
   "id": "922682d9ef655371",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Also, I will define a function to calculate the accuracy when training and testing the model.",
   "id": "f64f1fa97c9235cd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T08:08:32.078742Z",
     "start_time": "2025-06-20T08:08:32.075849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_accuracy(model, loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            X, y = X.to(device), y.to(device)  # Move data to the device\n",
    "            outputs = model(X)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Get the index of the max log-probability\n",
    "            total += y.size(0)  # Total number of samples\n",
    "            correct += (predicted == y).sum().item()  # Count correct predictions\n",
    "\n",
    "    accuracy = 100 * correct / total  # Calculate accuracy\n",
    "    model.train()  # Set the model back to training mode\n",
    "    return accuracy"
   ],
   "id": "a85f0f30721d6b32",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's train the model for 10 epochs. In each epoch, we will iterate over the training dataset, compute the loss, and update the model weights using the optimizer.",
   "id": "5259db3ee1c7e567"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T08:09:46.961826Z",
     "start_time": "2025-06-20T08:08:32.209125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "simple_model = simple_model.to(device)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # Remember convert inputs and labels to the device (GPU or CPU)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = simple_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if (i + 1) % 100 == 0:  # Print every 100 mini-batches\n",
    "            print(f\"Epoch [{epoch + 1}/{epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Calculate accuracy after each epoch\n",
    "    train_accuracy = calculate_accuracy(simple_model, train_loader, device)\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Train Accuracy: {train_accuracy:.2f}%\")"
   ],
   "id": "e9b81075a94cf49f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/938], Loss: 2.1597\n",
      "Epoch [1/10], Step [200/938], Loss: 1.5584\n",
      "Epoch [1/10], Step [300/938], Loss: 0.9138\n",
      "Epoch [1/10], Step [400/938], Loss: 0.6326\n",
      "Epoch [1/10], Step [500/938], Loss: 0.5182\n",
      "Epoch [1/10], Step [600/938], Loss: 0.4623\n",
      "Epoch [1/10], Step [700/938], Loss: 0.4132\n",
      "Epoch [1/10], Step [800/938], Loss: 0.3770\n",
      "Epoch [1/10], Step [900/938], Loss: 0.3716\n",
      "Epoch [1/10], Train Accuracy: 90.17%\n",
      "Epoch [2/10], Step [100/938], Loss: 0.3558\n",
      "Epoch [2/10], Step [200/938], Loss: 0.3242\n",
      "Epoch [2/10], Step [300/938], Loss: 0.3319\n",
      "Epoch [2/10], Step [400/938], Loss: 0.3091\n",
      "Epoch [2/10], Step [500/938], Loss: 0.3065\n",
      "Epoch [2/10], Step [600/938], Loss: 0.2945\n",
      "Epoch [2/10], Step [700/938], Loss: 0.2825\n",
      "Epoch [2/10], Step [800/938], Loss: 0.2857\n",
      "Epoch [2/10], Step [900/938], Loss: 0.2552\n",
      "Epoch [2/10], Train Accuracy: 92.16%\n",
      "Epoch [3/10], Step [100/938], Loss: 0.2491\n",
      "Epoch [3/10], Step [200/938], Loss: 0.2599\n",
      "Epoch [3/10], Step [300/938], Loss: 0.2598\n",
      "Epoch [3/10], Step [400/938], Loss: 0.2580\n",
      "Epoch [3/10], Step [500/938], Loss: 0.2449\n",
      "Epoch [3/10], Step [600/938], Loss: 0.2580\n",
      "Epoch [3/10], Step [700/938], Loss: 0.2346\n",
      "Epoch [3/10], Step [800/938], Loss: 0.2323\n",
      "Epoch [3/10], Step [900/938], Loss: 0.2311\n",
      "Epoch [3/10], Train Accuracy: 93.53%\n",
      "Epoch [4/10], Step [100/938], Loss: 0.2326\n",
      "Epoch [4/10], Step [200/938], Loss: 0.2160\n",
      "Epoch [4/10], Step [300/938], Loss: 0.2061\n",
      "Epoch [4/10], Step [400/938], Loss: 0.2152\n",
      "Epoch [4/10], Step [500/938], Loss: 0.2158\n",
      "Epoch [4/10], Step [600/938], Loss: 0.1903\n",
      "Epoch [4/10], Step [700/938], Loss: 0.2022\n",
      "Epoch [4/10], Step [800/938], Loss: 0.1993\n",
      "Epoch [4/10], Step [900/938], Loss: 0.1918\n",
      "Epoch [4/10], Train Accuracy: 94.58%\n",
      "Epoch [5/10], Step [100/938], Loss: 0.1785\n",
      "Epoch [5/10], Step [200/938], Loss: 0.1883\n",
      "Epoch [5/10], Step [300/938], Loss: 0.1707\n",
      "Epoch [5/10], Step [400/938], Loss: 0.1853\n",
      "Epoch [5/10], Step [500/938], Loss: 0.1882\n",
      "Epoch [5/10], Step [600/938], Loss: 0.1759\n",
      "Epoch [5/10], Step [700/938], Loss: 0.1873\n",
      "Epoch [5/10], Step [800/938], Loss: 0.1745\n",
      "Epoch [5/10], Step [900/938], Loss: 0.1648\n",
      "Epoch [5/10], Train Accuracy: 95.35%\n",
      "Epoch [6/10], Step [100/938], Loss: 0.1649\n",
      "Epoch [6/10], Step [200/938], Loss: 0.1655\n",
      "Epoch [6/10], Step [300/938], Loss: 0.1703\n",
      "Epoch [6/10], Step [400/938], Loss: 0.1545\n",
      "Epoch [6/10], Step [500/938], Loss: 0.1613\n",
      "Epoch [6/10], Step [600/938], Loss: 0.1501\n",
      "Epoch [6/10], Step [700/938], Loss: 0.1556\n",
      "Epoch [6/10], Step [800/938], Loss: 0.1487\n",
      "Epoch [6/10], Step [900/938], Loss: 0.1435\n",
      "Epoch [6/10], Train Accuracy: 96.02%\n",
      "Epoch [7/10], Step [100/938], Loss: 0.1455\n",
      "Epoch [7/10], Step [200/938], Loss: 0.1427\n",
      "Epoch [7/10], Step [300/938], Loss: 0.1442\n",
      "Epoch [7/10], Step [400/938], Loss: 0.1358\n",
      "Epoch [7/10], Step [500/938], Loss: 0.1351\n",
      "Epoch [7/10], Step [600/938], Loss: 0.1245\n",
      "Epoch [7/10], Step [700/938], Loss: 0.1363\n",
      "Epoch [7/10], Step [800/938], Loss: 0.1274\n",
      "Epoch [7/10], Step [900/938], Loss: 0.1523\n",
      "Epoch [7/10], Train Accuracy: 96.39%\n",
      "Epoch [8/10], Step [100/938], Loss: 0.1208\n",
      "Epoch [8/10], Step [200/938], Loss: 0.1368\n",
      "Epoch [8/10], Step [300/938], Loss: 0.1199\n",
      "Epoch [8/10], Step [400/938], Loss: 0.1283\n",
      "Epoch [8/10], Step [500/938], Loss: 0.1257\n",
      "Epoch [8/10], Step [600/938], Loss: 0.1309\n",
      "Epoch [8/10], Step [700/938], Loss: 0.1229\n",
      "Epoch [8/10], Step [800/938], Loss: 0.1124\n",
      "Epoch [8/10], Step [900/938], Loss: 0.1043\n",
      "Epoch [8/10], Train Accuracy: 96.87%\n",
      "Epoch [9/10], Step [100/938], Loss: 0.1125\n",
      "Epoch [9/10], Step [200/938], Loss: 0.1189\n",
      "Epoch [9/10], Step [300/938], Loss: 0.1050\n",
      "Epoch [9/10], Step [400/938], Loss: 0.1153\n",
      "Epoch [9/10], Step [500/938], Loss: 0.1068\n",
      "Epoch [9/10], Step [600/938], Loss: 0.1102\n",
      "Epoch [9/10], Step [700/938], Loss: 0.1008\n",
      "Epoch [9/10], Step [800/938], Loss: 0.1146\n",
      "Epoch [9/10], Step [900/938], Loss: 0.1085\n",
      "Epoch [9/10], Train Accuracy: 97.20%\n",
      "Epoch [10/10], Step [100/938], Loss: 0.0944\n",
      "Epoch [10/10], Step [200/938], Loss: 0.0978\n",
      "Epoch [10/10], Step [300/938], Loss: 0.1031\n",
      "Epoch [10/10], Step [400/938], Loss: 0.0918\n",
      "Epoch [10/10], Step [500/938], Loss: 0.0981\n",
      "Epoch [10/10], Step [600/938], Loss: 0.0936\n",
      "Epoch [10/10], Step [700/938], Loss: 0.1048\n",
      "Epoch [10/10], Step [800/938], Loss: 0.1096\n",
      "Epoch [10/10], Step [900/938], Loss: 0.1023\n",
      "Epoch [10/10], Train Accuracy: 97.39%\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Evaluating the Model",
   "id": "43e40b9fce1b14ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T08:10:24.080976Z",
     "start_time": "2025-06-20T08:10:23.555347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# After training the model, we can evaluate its performance on the test dataset.\n",
    "test_accuracy = calculate_accuracy(simple_model, test_loader, device)\n",
    "print(f\"Accuracy: {test_accuracy:.2f}%\")"
   ],
   "id": "7923e7c26febb5bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.79%\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Good job! Now you have successfully built and trained a simple neural network model using PyTorch on the MNIST dataset. And look at that, accuracy >= 95% is achieved on the test dataset.",
   "id": "bfbf98862e33511b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### MobileNetV2 with CIFAR10 Dataset\n",
    "Paper: [MobileNetV2: Inverted Residuals and Linear Bottlenecks for Efficient Mobile Network Design](https://arxiv.org/abs/1801.04381)\n",
    "\n",
    "Now, let's build a more complex model with MobileNetV2 architecture and train it on CIFAR10 dataset.\n",
    "First of all, we dive into MobileNetV2 architecture, which is a lightweight deep learning model designed for mobile and embedded vision applications. It is based on depthwise separable convolutions, which significantly reduce the number of parameters and computational cost compared to traditional convolutional neural networks (CNNs).\n",
    "\n",
    "In this architecture, we will use the following components:\n",
    "1. Inverted Residual Block: A residual block where the number of channels is expanded first, then reduced (inverted from traditional ResNet).\n",
    "2. Linear Bottleneck: The final layer of the block uses a linear activation function instead of ReLU to avoid non-linearity in the bottleneck layer.\n",
    "3. Depthwise Separable Convolution: Similar to MobileNetV1, it uses depthwise separable convolutions to reduce the number of parameters and computations.\n",
    "4. Skip Connections: Similar to ResNet, it uses skip connections to allow gradients to flow through the network more easily."
   ],
   "id": "9bf3b69b608593ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Inverted Residual Block\n",
    "Inverted Residual Block is a key component of MobileNetV2 architecture. It consists of three main parts:\n",
    "1. Expansion: The input is first expanded to a higher number of channels using a 1x1 convolution.\n",
    "2. Depthwise Convolution: A depthwise convolution is applied to the expanded channels, which applies a single filter to each input channel.\n",
    "3. Projection: The output of the depthwise convolution is then projected back to a lower number of channels using another 1x1 convolution.\n",
    "4. Skip Connection: If the input and output channels are the same, a skip connection is added to allow gradients to flow through the network more easily.\n",
    "\n",
    "The Inverted Residual Block can be implemented in PyTorch as follows:"
   ],
   "id": "9dffb4426d3ace20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T09:07:14.548281Z",
     "start_time": "2025-06-20T09:07:14.544069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class InvertedResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Inverted Residual Block\n",
    "    This block is a key component of MobileNetV2 architecture.\n",
    "    It consists of three main parts:\n",
    "    1. Expansion: The input is first expanded to a higher number of channels using a 1x1 convolution.\n",
    "    2. Depthwise Convolution: A depthwise convolution is applied to the expanded channels.\n",
    "    3. Projection: The output of the depthwise convolution is then projected back to a lower\n",
    "    number of channels using another 1x1 convolution.\n",
    "    4. Skip Connection: If the input and output channels are the same, a skip connection is added.\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_channels (int): Number of output channels.\n",
    "        stride (int): Stride for the depthwise convolution.\n",
    "        expand_ratio (float): Expansion ratio for the number of channels.\n",
    "    Returns:\n",
    "        torch.Tensor: Output tensor after applying the Inverted Residual Block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride, expand_ratio):\n",
    "        super(InvertedResidualBlock, self).__init__()\n",
    "        self.stride = stride\n",
    "\n",
    "        \"\"\"\n",
    "        The distinction of Inverted Residual Block and Residual Block is that the Inverted\n",
    "        Residual Block first expands the number of channels, then reduces it whereas the\n",
    "        Residual Block reduces the number of channels first.\n",
    "        So, hidden dimension is calculated by multiplying the input channels with the expand ratio.\n",
    "        \"\"\"\n",
    "        hidden_dim = int(in_channels * expand_ratio)\n",
    "\n",
    "        # Residual Connection same with ResNet\n",
    "        self.use_residual_connection = self.stride == 1 and in_channels == out_channels\n",
    "\n",
    "        # init layers\n",
    "        layers = []\n",
    "        if expand_ratio != 1:\n",
    "            # 1. Expansion (Conv 1x1) -> only change channels to hidden dimension\n",
    "            layers.append(nn.Conv2d(in_channels, hidden_dim, kernel_size=1, bias=False))\n",
    "            layers.append(nn.BatchNorm2d(hidden_dim))\n",
    "            layers.append(nn.ReLU6(inplace=True))\n",
    "\n",
    "        # 2. Depthwise Convolution\n",
    "        layers.append(nn.Conv2d(\n",
    "            in_channels=hidden_dim,\n",
    "            out_channels=hidden_dim,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            groups=hidden_dim,  # Depthwise convolution\n",
    "            padding=1,\n",
    "        ))\n",
    "        layers.append(nn.BatchNorm2d(hidden_dim))\n",
    "        layers.append(nn.ReLU6(inplace=True))\n",
    "\n",
    "        # 3. Projection (Conv 1x1, no activation)\n",
    "        layers.append(nn.Conv2d(hidden_dim, out_channels, kernel_size=1, bias=False))\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        # Combine all layers into a sequential block\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the Inverted Residual Block.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after applying the block.\n",
    "        \"\"\"\n",
    "        if self.use_residual_connection:\n",
    "            return x + self.block(x)  # Add skip connection\n",
    "        else:\n",
    "            return self.block(x)  # No skip connection"
   ],
   "id": "e13059d8313c9d9c",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### MobileNetV2 Model",
   "id": "fc69b1e3a48b4c12"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T09:07:21.920492Z",
     "start_time": "2025-06-20T09:07:21.910235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# (1, 16, 1, 1),\n",
    "# (6, 24, 2, 2),\n",
    "# (6, 32, 3, 2),\n",
    "# (6, 64, 4, 2),\n",
    "# (6, 96, 3, 1),\n",
    "# (6, 160, 3, 2),\n",
    "# (6, 320, 1, 1),\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    \"\"\"\n",
    "    MobileNetV2\n",
    "    This class implements the MobileNetV2 architecture.\n",
    "    It consists of an initial convolution layer followed by a series of Inverted\n",
    "    Residual Blocks. Follow Architecture:\n",
    "    1. First layer: A 3x3 convolution with stride 2 and 32 output channels.\n",
    "    2. Inverted Residual Blocks: A series of blocks with different configurations\n",
    "    following the pattern:\n",
    "        - t, c, n, s: (expand_ratio, out_channels, num_blocks, stride)\n",
    "        - expand_ratio: The ratio by which the number of channels is expanded in the block.\n",
    "    3. Last layer: A 1x1 convolution with 1280 output channels.\n",
    "    4. Final layer: A 1x1 convolution with num_classes output channels.\n",
    "    Remember BatchNorm and ReLU6 will be applied in this architecture.\n",
    "    Args:\n",
    "        num_classes (int): Number of output classes for classification.\n",
    "    Returns:\n",
    "        torch.Tensor: Output tensor after applying the MobileNetV2 model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=10):  # use for CIFAR10\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        self.configuration = [\n",
    "            (1, 16, 1, 1),\n",
    "            (6, 24, 2, 2),\n",
    "            (6, 32, 3, 2),\n",
    "            (6, 64, 4, 2),\n",
    "            (6, 96, 3, 1),\n",
    "            (6, 160, 3, 2),\n",
    "            (6, 320, 1, 1),\n",
    "        ]\n",
    "\n",
    "        # Initial First Layer\n",
    "        input_channels = 32\n",
    "        layers = [\n",
    "            nn.Conv2d(3, input_channels, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(input_channels),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        ]\n",
    "\n",
    "        # Inverted Residual Blocks\n",
    "        for t, c, n, s in self.configuration:\n",
    "            outputs_channels = c\n",
    "            for i in range(n):  # n is number of blocks\n",
    "                # First block uses stride, others use 1\n",
    "                # Example:\n",
    "                #     (1, 16, 1, 1) -> 1 block with stride 1\n",
    "                #     (6, 24, 2, 2) -> 2 blocks with stride 2 for first block and 1 for second block\n",
    "                stride = s if i == 0 else 1\n",
    "                layers.append(InvertedResidualBlock(input_channels, outputs_channels, stride, t))\n",
    "                input_channels = outputs_channels  # Update input channels for next block\n",
    "\n",
    "        # Final Conv 1x1\n",
    "        final_channel = 1280\n",
    "        # Use kernel_size=1 to reduce the number of channels to final_channel\n",
    "        layers.append(nn.Conv2d(input_channels, final_channel, kernel_size=1, bias=False))\n",
    "        layers.append(nn.BatchNorm2d(final_channel))\n",
    "        layers.append(nn.ReLU6(inplace=True))\n",
    "        self.features = nn.Sequential(*layers)\n",
    "\n",
    "        # Classifier\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifier = nn.Conv2d(final_channel, num_classes, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the MobileNetV2 model.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after applying the MobileNetV2 model.\n",
    "        \"\"\"\n",
    "        x = self.features(x)  # Apply feature extraction layers\n",
    "        x = self.avgpool(x)  # Adaptive average pooling to 1x1\n",
    "        x = self.classifier(x)  # Final classification layer\n",
    "        x = torch.flatten(x, 1)  # Flatten the output to [B, num_classes]\n",
    "        return x"
   ],
   "id": "f8bd9f88bb6a163a",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Model Summary",
   "id": "21b83c977302ecc7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T09:07:23.801514Z",
     "start_time": "2025-06-20T09:07:23.744259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "mobilenet_v2 = MobileNetV2(num_classes=10)  # CIFAR10 has 10 classes\n",
    "summary(mobilenet_v2, (1, 3, 32, 32))"
   ],
   "id": "dc9a0b88bd89bb69",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MobileNetV2                              [1, 10]                   --\n",
       "├─Sequential: 1-1                        [1, 1280, 1, 1]           --\n",
       "│    └─Conv2d: 2-1                       [1, 32, 16, 16]           864\n",
       "│    └─BatchNorm2d: 2-2                  [1, 32, 16, 16]           64\n",
       "│    └─ReLU6: 2-3                        [1, 32, 16, 16]           --\n",
       "│    └─InvertedResidualBlock: 2-4        [1, 16, 16, 16]           --\n",
       "│    │    └─Sequential: 3-1              [1, 16, 16, 16]           928\n",
       "│    └─InvertedResidualBlock: 2-5        [1, 24, 8, 8]             --\n",
       "│    │    └─Sequential: 3-2              [1, 24, 8, 8]             5,232\n",
       "│    └─InvertedResidualBlock: 2-6        [1, 24, 8, 8]             --\n",
       "│    │    └─Sequential: 3-3              [1, 24, 8, 8]             8,976\n",
       "│    └─InvertedResidualBlock: 2-7        [1, 32, 4, 4]             --\n",
       "│    │    └─Sequential: 3-4              [1, 32, 4, 4]             10,144\n",
       "│    └─InvertedResidualBlock: 2-8        [1, 32, 4, 4]             --\n",
       "│    │    └─Sequential: 3-5              [1, 32, 4, 4]             15,040\n",
       "│    └─InvertedResidualBlock: 2-9        [1, 32, 4, 4]             --\n",
       "│    │    └─Sequential: 3-6              [1, 32, 4, 4]             15,040\n",
       "│    └─InvertedResidualBlock: 2-10       [1, 64, 2, 2]             --\n",
       "│    │    └─Sequential: 3-7              [1, 64, 2, 2]             21,248\n",
       "│    └─InvertedResidualBlock: 2-11       [1, 64, 2, 2]             --\n",
       "│    │    └─Sequential: 3-8              [1, 64, 2, 2]             54,656\n",
       "│    └─InvertedResidualBlock: 2-12       [1, 64, 2, 2]             --\n",
       "│    │    └─Sequential: 3-9              [1, 64, 2, 2]             54,656\n",
       "│    └─InvertedResidualBlock: 2-13       [1, 64, 2, 2]             --\n",
       "│    │    └─Sequential: 3-10             [1, 64, 2, 2]             54,656\n",
       "│    └─InvertedResidualBlock: 2-14       [1, 96, 2, 2]             --\n",
       "│    │    └─Sequential: 3-11             [1, 96, 2, 2]             67,008\n",
       "│    └─InvertedResidualBlock: 2-15       [1, 96, 2, 2]             --\n",
       "│    │    └─Sequential: 3-12             [1, 96, 2, 2]             118,848\n",
       "│    └─InvertedResidualBlock: 2-16       [1, 96, 2, 2]             --\n",
       "│    │    └─Sequential: 3-13             [1, 96, 2, 2]             118,848\n",
       "│    └─InvertedResidualBlock: 2-17       [1, 160, 1, 1]            --\n",
       "│    │    └─Sequential: 3-14             [1, 160, 1, 1]            155,840\n",
       "│    └─InvertedResidualBlock: 2-18       [1, 160, 1, 1]            --\n",
       "│    │    └─Sequential: 3-15             [1, 160, 1, 1]            320,960\n",
       "│    └─InvertedResidualBlock: 2-19       [1, 160, 1, 1]            --\n",
       "│    │    └─Sequential: 3-16             [1, 160, 1, 1]            320,960\n",
       "│    └─InvertedResidualBlock: 2-20       [1, 320, 1, 1]            --\n",
       "│    │    └─Sequential: 3-17             [1, 320, 1, 1]            474,880\n",
       "│    └─Conv2d: 2-21                      [1, 1280, 1, 1]           409,600\n",
       "│    └─BatchNorm2d: 2-22                 [1, 1280, 1, 1]           2,560\n",
       "│    └─ReLU6: 2-23                       [1, 1280, 1, 1]           --\n",
       "├─AdaptiveAvgPool2d: 1-2                 [1, 1280, 1, 1]           --\n",
       "├─Conv2d: 1-3                            [1, 10, 1, 1]             12,800\n",
       "==========================================================================================\n",
       "Total params: 2,243,808\n",
       "Trainable params: 2,243,808\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 6.21\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 2.18\n",
       "Params size (MB): 8.98\n",
       "Estimated Total Size (MB): 11.17\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Loading CIFAR10 Dataset",
   "id": "cd81db925f5f51b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T09:07:31.047889Z",
     "start_time": "2025-06-20T09:07:29.736558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# CIFAR10 dataset\n",
    "dataset = \"/Users/hinsun/Workspace/ComputerScience/DeepLearning/data\"\n",
    "train_batch_size = 64\n",
    "test_batch_size = 64\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "train_dataset = CIFAR10(root=dataset, train=True, download=True, transform=transform)\n",
    "test_dataset = CIFAR10(root=dataset, train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)"
   ],
   "id": "288cbbe24a4d9e46",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Training the Model",
   "id": "f6bf6c513bfa9a7a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T09:37:04.230819Z",
     "start_time": "2025-06-20T09:30:48.873338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def training(model_train, epochs_train, current_train_loader, train_criterion, train_optimizer):\n",
    "    for epoch in range(epochs_train):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(current_train_loader):\n",
    "            # Remember convert inputs and labels to the device (GPU or CPU)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model_train(inputs)\n",
    "            loss = train_criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if (i + 1) % 100 == 0:  # Print every 100 mini-batches\n",
    "                print(\n",
    "                    f\"Epoch [{epoch + 1}/{epochs_train}], Step [{i + 1}/{len(current_train_loader)}], Loss: {running_loss / 100:.4f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Calculate accuracy after each epoch\n",
    "        accuracy = calculate_accuracy(model_train, current_train_loader, device)\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs_train}], Train Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "nesterov = True\n",
    "device = torch.accelerator.current_accelerator()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(mobilenet_v2.parameters(), lr=lr, momentum=momentum, nesterov=nesterov)\n",
    "mobilenet_v2 = mobilenet_v2.to(device)\n",
    "epochs = 5\n",
    "\n",
    "training(mobilenet_v2, epochs, train_loader, criterion, optimizer)"
   ],
   "id": "d88b8cdf049d2d77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/782], Loss: 0.6705\n",
      "Epoch [1/5], Step [200/782], Loss: 0.6662\n",
      "Epoch [1/5], Step [300/782], Loss: 0.7205\n",
      "Epoch [1/5], Step [400/782], Loss: 0.6996\n",
      "Epoch [1/5], Step [500/782], Loss: 0.7426\n",
      "Epoch [1/5], Step [600/782], Loss: 0.7244\n",
      "Epoch [1/5], Step [700/782], Loss: 0.7259\n",
      "Epoch [1/5], Train Accuracy: 80.41%\n",
      "Epoch [2/5], Step [100/782], Loss: 0.6233\n",
      "Epoch [2/5], Step [200/782], Loss: 0.6277\n",
      "Epoch [2/5], Step [300/782], Loss: 0.6823\n",
      "Epoch [2/5], Step [400/782], Loss: 0.7120\n",
      "Epoch [2/5], Step [500/782], Loss: 0.6873\n",
      "Epoch [2/5], Step [600/782], Loss: 0.6544\n",
      "Epoch [2/5], Step [700/782], Loss: 0.6903\n",
      "Epoch [2/5], Train Accuracy: 78.66%\n",
      "Epoch [3/5], Step [100/782], Loss: 0.5827\n",
      "Epoch [3/5], Step [200/782], Loss: 0.5830\n",
      "Epoch [3/5], Step [300/782], Loss: 0.6160\n",
      "Epoch [3/5], Step [400/782], Loss: 0.6279\n",
      "Epoch [3/5], Step [500/782], Loss: 0.6384\n",
      "Epoch [3/5], Step [600/782], Loss: 0.6559\n",
      "Epoch [3/5], Step [700/782], Loss: 0.6594\n",
      "Epoch [3/5], Train Accuracy: 83.00%\n",
      "Epoch [4/5], Step [100/782], Loss: 0.5727\n",
      "Epoch [4/5], Step [200/782], Loss: 0.5918\n",
      "Epoch [4/5], Step [300/782], Loss: 0.6521\n",
      "Epoch [4/5], Step [400/782], Loss: 0.6517\n",
      "Epoch [4/5], Step [500/782], Loss: 0.6491\n",
      "Epoch [4/5], Step [600/782], Loss: 0.6621\n",
      "Epoch [4/5], Step [700/782], Loss: 0.6347\n",
      "Epoch [4/5], Train Accuracy: 80.55%\n",
      "Epoch [5/5], Step [100/782], Loss: 0.6105\n",
      "Epoch [5/5], Step [200/782], Loss: 0.5411\n",
      "Epoch [5/5], Step [300/782], Loss: 0.5751\n",
      "Epoch [5/5], Step [400/782], Loss: 0.6292\n",
      "Epoch [5/5], Step [500/782], Loss: 0.5808\n",
      "Epoch [5/5], Step [600/782], Loss: 0.5780\n",
      "Epoch [5/5], Step [700/782], Loss: 0.5823\n",
      "Epoch [5/5], Train Accuracy: 85.83%\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Applying Techniques to Improve Model Performance\n",
    "\n",
    "Look at that, we have trained the MobileNetV2 model on CIFAR10 dataset for 10 epochs. Time training so long, but we can apply some techniques to improve, such as:\n",
    "1. Save and Load Model: Save the trained model to disk and load it later for inference or further training.\n",
    "2. Try Adam Optimizer: Use the Adam optimizer instead of SGD for better convergence."
   ],
   "id": "247fe368850c844f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T10:00:38.492934Z",
     "start_time": "2025-06-20T09:54:39.416658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "is_new_training = False\n",
    "mobilenet_v2 = None\n",
    "path = \"/Users/hinsun/Workspace/ComputerScience/DeepLearning/saved/mobilenet_v2_cifar10.pth\"\n",
    "device = torch.accelerator.current_accelerator()\n",
    "\n",
    "if is_new_training:\n",
    "    print(\"Training new model...\")\n",
    "    mobilenet_v2 = MobileNetV2(num_classes=10).to(device)\n",
    "else:\n",
    "    print(\"Loading pre-trained model...\")\n",
    "    mobilenet_v2 = MobileNetV2(num_classes=10).to(device)\n",
    "    mobilenet_v2.load_state_dict(torch.load(path, map_location=device))\n",
    "\n",
    "lr = 0.1\n",
    "momentum = 0.9\n",
    "nesterov = True\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(mobilenet_v2.parameters(), lr=lr, momentum=momentum, nesterov=nesterov)\n",
    "\n",
    "epochs = 5\n",
    "training(mobilenet_v2, epochs, train_loader, criterion, optimizer)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(mobilenet_v2.state_dict(), path)"
   ],
   "id": "b0244a7877676b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained model...\n",
      "Epoch [1/5], Step [100/782], Loss: 0.9204\n",
      "Epoch [1/5], Step [200/782], Loss: 0.9401\n",
      "Epoch [1/5], Step [300/782], Loss: 0.9090\n",
      "Epoch [1/5], Step [400/782], Loss: 0.9043\n",
      "Epoch [1/5], Step [500/782], Loss: 0.8932\n",
      "Epoch [1/5], Step [600/782], Loss: 0.9091\n",
      "Epoch [1/5], Step [700/782], Loss: 0.8704\n",
      "Epoch [1/5], Train Accuracy: 67.74%\n",
      "Epoch [2/5], Step [100/782], Loss: 0.8474\n",
      "Epoch [2/5], Step [200/782], Loss: 0.8352\n",
      "Epoch [2/5], Step [300/782], Loss: 0.8395\n",
      "Epoch [2/5], Step [400/782], Loss: 0.8287\n",
      "Epoch [2/5], Step [500/782], Loss: 0.8298\n",
      "Epoch [2/5], Step [600/782], Loss: 0.8603\n",
      "Epoch [2/5], Step [700/782], Loss: 0.7941\n",
      "Epoch [2/5], Train Accuracy: 68.28%\n",
      "Epoch [3/5], Step [100/782], Loss: 0.7773\n",
      "Epoch [3/5], Step [200/782], Loss: 0.8027\n",
      "Epoch [3/5], Step [300/782], Loss: 0.7815\n",
      "Epoch [3/5], Step [400/782], Loss: 0.7825\n",
      "Epoch [3/5], Step [500/782], Loss: 0.7805\n",
      "Epoch [3/5], Step [600/782], Loss: 0.7660\n",
      "Epoch [3/5], Step [700/782], Loss: 0.7930\n",
      "Epoch [3/5], Train Accuracy: 77.09%\n",
      "Epoch [4/5], Step [100/782], Loss: 0.7066\n",
      "Epoch [4/5], Step [200/782], Loss: 0.7486\n",
      "Epoch [4/5], Step [300/782], Loss: 0.7328\n",
      "Epoch [4/5], Step [400/782], Loss: 0.7441\n",
      "Epoch [4/5], Step [500/782], Loss: 0.7376\n",
      "Epoch [4/5], Step [600/782], Loss: 0.7494\n",
      "Epoch [4/5], Step [700/782], Loss: 0.7572\n",
      "Epoch [4/5], Train Accuracy: 74.23%\n",
      "Epoch [5/5], Step [100/782], Loss: 0.6861\n",
      "Epoch [5/5], Step [200/782], Loss: 0.7055\n",
      "Epoch [5/5], Step [300/782], Loss: 0.6882\n",
      "Epoch [5/5], Step [400/782], Loss: 0.7252\n",
      "Epoch [5/5], Step [500/782], Loss: 0.7065\n",
      "Epoch [5/5], Step [600/782], Loss: 0.7041\n",
      "Epoch [5/5], Step [700/782], Loss: 0.7159\n",
      "Epoch [5/5], Train Accuracy: 78.55%\n"
     ]
    }
   ],
   "execution_count": 85
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
